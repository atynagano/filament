<meta charset="utf-8">

<style>img { max-width: 100%; }</style>

**Filamentにおける物理ベースレンダリング**

![](images/filament_logo.png)

# このドキュメントについて

このドキュメントは[Physically Based Rendering in Filament](https://google.github.io/filament/Filament.html)の一個人による日本語訳です。オリジナルのプロジェクトについては[Filament project](https://github.com/google/filament)を参照してください。

## 著者

- [Romain Guy](https://github.com/romainguy), [@romainguy](https://twitter.com/romainguy)
- [Mathias Agopian](https://github.com/pixelflinger), [@darthmoosious](https://twitter.com/darthmoosious)

# 概要

Filamentは、Android向けの物理ベースレンダリング（PBR）エンジンです。Filamentの目標は、Android開発者が簡単に高品質な2Dおよび3Dレンダリングを作成できるようにする一連のツールとAPIを提供することです。

この文書の目的は、Filamentで使用されるマテリアルとライティングモデルの背後にある方程式と理論を説明することです。この文書は、Filamentの貢献者やエンジンの内部動作に興味のある開発者を対象としています。理論と実践の関係を可能な限り明確にするために、必要に応じてコードスニペットを提供します。

この文書は設計文書を目指しているわけではありません。それはアルゴリズムにのみ焦点を当てており、その内容は任意のエンジンでPBRを実装するために使用できます。ただし、この文書では、なぜ特定のアルゴリズム/モデルを他のものよりも選んだのかを説明します。

特に注記がない限り、この文書に存在するすべての3Dレンダリングはエンジン内で生成されています（プロトタイプまたは製品）。これらの3Dレンダリングの多くは、Filamentの開発初期段階でキャプチャされ、最終的な品質を反映していません。

## 原則

リアルタイムレンダリングは研究の活発な分野であり、実装する必要があるすべての機能に対して選択できる方程式、アルゴリズム、実装が大量にあります（例えば、*Rendering real-time shadows*という本は、数十のシャドウレンダリング技術の400ページの要約です）。したがって、私たちは情報に基づいた決定を下す前に、まず目標（またはBrent Burleyの画期的な論文「Physically-based shading at Disney」[#Burley12]に従った原則）を定義しなければなりません。

リアルタイムのモバイルパフォーマンス
:   私たちの主な目標は、モバイルプラットフォームで効率的に動作するレンダリングシステムを設計および実装することです。主なターゲットはOpenGL ES 3.xクラスのGPUになります。

品質
:   私たちのレンダリングシステムは全体的な画質を重視します。ただし、低性能および中性能のGPUをサポートするために品質の妥協を受け入れます。

使いやすさ
:   アーティストは自分のアセットを頻繁に、そして迅速に反復する必要があり、私たちのレンダリングシステムは彼らが直感的にそうすることを可能にしなければなりません。したがって、私たちは理解しやすいパラメータを提供しなければなりません（例えば、鏡面パワーはありません）。

    また、すべての開発者がアーティストと一緒に働く余裕があるわけではないことを理解しています。私たちのシステムの物理ベースのアプローチにより、開発者は実装の理論を理解することなく視覚的に妥当なマテリアルを作成することができます。

    アーティストと開発者の両方に対して、私たちのシステムは試行錯誤を減らし、ユーザーが素早くマテリアルモデルをマスターできるように、可能な限り少ないパラメータに依存します。

    さらに、パラメータ値の任意の組み合わせは物理的に妥当な結果につながるべきです。物理的に不可能なマテリアルは作成が難しくなければなりません。

親しみやすさ
:   私たちのシステムは、可能な限り物理的な単位を使用するべきです: 距離はメートルまたはセンチメートル、色温度はケルビン、光の単位はルーメンまたはカンデラなど。

柔軟性
:   物理ベースのアプローチは非現実的なレンダリングを排除してはなりません。ユーザーインターフェースは例えば、非照明マテリアルが必要になります。

デプロイサイズ
:   この文書の内容とは直接関係ありませんが、レンダリングライブラリを可能な限り小さく保つという私たちの願望を強調することは重要です。これにより、任意のアプリケーションがバイナリを望ましくないサイズに増やすことなくそれをバンドルできます。

## 物理ベースレンダリング

私たちは、芸術的および生産効率の観点からの利点、およびそれが私たちの目標と互換性があるため、PBRを採用することを選びました。

物理ベースのレンダリングは、伝統的なリアルタイムモデルと比較して、マテリアルとそれらが光とどのように相互作用するかのより正確な表現を提供するレンダリング方法です。PBR方法の中心にあるマテリアルとライティングの分離により、すべての照明条件で正確に見えるリアルなアセットを作成することが容易になります。

# 注釈

$$
\newcommand{NoL}{n \cdot l}
\newcommand{NoV}{n \cdot v}
\newcommand{NoH}{n \cdot h}
\newcommand{VoH}{v \cdot h}
\newcommand{LoH}{l \cdot h}
\newcommand{fNormal}{f_{0}}
\newcommand{fDiffuse}{f_d}
\newcommand{fSpecular}{f_r}
\newcommand{fX}{f_x}
\newcommand{aa}{\alpha^2}
\newcommand{fGrazing}{f_{90}}
\newcommand{schlick}{F_{Schlick}}
\newcommand{nior}{n_{ior}}
\newcommand{Ed}{E_d}
\newcommand{Lt}{L_{\bot}}
\newcommand{Lout}{L_{out}}
\newcommand{cosTheta}{\left< \cos \theta \right> }
$$

この文書全体で見つかる方程式は、table [symbols]で説明されている記号を使用します。

          Symbol             |           Definition
:---------------------------:|:---------------------------|
$v$                          | 視線方向の単位ベクトル
$l$                          | 入射光の単位ベクトル
$n$                          | 表面法線の単位ベクトル
$h$                          | $l$ and $v$ の間の半分の単位ベクトル
$f$                          | BRDF
$\fDiffuse$                  | BRDFの拡散成分
$\fSpecular$                 | BRDFの拡散成分
$\alpha$                     | 入力`perceptualRoughness`を使用して再マップされた粗さ
$\sigma$                     | 拡散反射率
$\Omega$                     | 球面ドメイン
$\fNormal$                   | 法線方向入射時の反射率
$\fGrazing$                  | かすめ角での反射率
$\chi^+(a)$                  | ヘヴィサイド関数 ($a > 0$ ならば1、それ以外は0)
$n_{ior}$                    | 境界面の屈折率 (IOR)
$\left< \NoL \right>$        | [0..1]にクランプされたドット積
$\left< a \right>$           | [0..1]にクランプされた値
[Table [symbols]: Symbols definitions]

# マテリアルシステム

以下のセクションでは、異方性やクリアコート層などのさまざまな表面特性の説明を簡素化するために、複数のマテリアルモデルを説明します。ただし、実際にはこれらのモデルの一部は単一のものに凝縮されます。例えば、標準モデル、クリアコートモデル、異方性モデルは、単一の、より柔軟で強力なモデルを形成するために組み合わせることができます。Filamentで実装されているマテリアルモデルの説明については、[マテリアルドキュメンテーション](./Materials.md.html)を参照してください。

## 標準モデル

私たちのモデルの目標は、標準的なマテリアルの外観を表現することです。マテリアルモデルは、BSDF（双方向散乱分布関数）という数学的な形で記述されます。これ自体は他の2つの関数から構成されています: BRDF（双方向反射分布関数）とBTDF（双方向透過関数）。

私たちは一般的に遭遇する表面をモデル化することを目指しているので、私たちの標準マテリアルモデルはBRDFに焦点を当て、BTDFを無視するか、大幅に近似します。したがって、私たちの標準モデルは、短い平均自由路を持つ、反射的で、等方性のある、誘電体または導体の表面を正確に模倣することができます。

BRDFは、標準マテリアルの表面の反射特性を2つの項の関数として記述します: 
- 拡散成分、または $f_d$
- 鏡面成分、または $f_r$

表面、表面法線、入射光、これらの項との関係は、figure [frFd]に示されています（現時点では、表面下散乱は無視します）:

![Figure [frfd]: 拡散項$f_d$と鏡面項$f_r$によるBRDFモデルを使用した光と表面の相互作用](images/diagram_fr_fd.png)

完全な表面反射特性は次のように表現できます:

$$\begin{equation}\label{brdf}
f(v,l)=f_d(v,l)+f_r(v,l)
\end{equation}$$

この式は、単一の方向からの入射光に対する表面反射特性を特徴付けています。完全なレンダリング方程式では、 $ l $を半球全体で積分する必要があります。

一般的に遭遇する表面は、通常は平らな境界面でないため、不規則な界面で光がどのように相互作用するかを特徴付けるモデルが必要です。

微小面BRDFは、その目的に適した物理的に妥当なBRDFです。このようなBRDFでは、表面は微視的には滑らかではなく、ランダムに配置された多数の平面状の表面断片である微小面(microfacet, マイクロファセット)で構成されていると述べています。figure [microfacetVsFlat]は、ミクロレベルでの平界面と不規則な界面の違いを示しています: 

![Figure [microfacetVsFlat]: 微小面モデル（左）と平界面モデル（右）による不規則な境界面のモデリング](images/diagram_microfacet.png)

figure [microfacets]に示すように、法線が光線方向と視線方向の中間を向いている微小面だけが可視光を反射する。

![Figure [microfacets]: 微小面](images/diagram_macrosurface.png)

ただし、適切な方向を持つ法線を持つすべての微小面が反射光に寄与するわけではありません。BRDFはマスキングと影の考慮も行っています。これはfigure [microfacetshadowing]で示されています。

![Figure [microfacetShadowing]: 微小面のマスキングと影](images/diagram_shadowing_masking.png)

微小面BRDFは、表面がミクロレベルでどれだけ滑らか（低粗度）または粗い（高粗度）かを表す _粗度_ パラメータに大きく影響を受けます。表面がより滑らかであれば、小面(facet)はより整列し、反射光はより顕著になります。表面がより粗いと、カメラの方向を向いている小面が少なくなり、入射光が反射後に散乱し、鏡面ハイライトにぼやけた外観を与えることがあります。

figure [roughness]は、異なる粗度の表面と光の相互作用を示しています。

![Figure [roughness]: 異なる粗度（左から右に向かって、粗い→滑らか）とその結果のBRDF鏡面成分ローブ](images/diagram_roughness.png)

!!! 注: 粗度（roughness）について
    ユーザーが設定する粗度パラメータは、このドキュメント全体のシェーダースニペットでは`perceptualRoughness`と呼ばれます。`roughness`と呼ばれる変数は、セクション[Parameterization]で説明される再マッピングを使用した`perceptualRoughness`です。

微小面モデルは、次の式で説明されます（ここで、xは鏡面または拡散成分を表します）: 

$$\begin{equation}
\fX(v,l) = \frac{1}{| \NoV | | \NoL |}
\int_\Omega D(m,\alpha) G(v,l,m) f_m(v,l,m) (v \cdot m) (l \cdot m) dm
\end{equation}$$

項 $D$ は、微小面の分布をモデリングします（この項はNDFまたは法線分布関数とも呼ばれます）。この項は、figure [roughness]で示されているように、表面の外観に重要な役割を果たします。

項 $G$ は、微小面の可視性（またはocclusion、あるいはshadow-masking）をモデル化します。

この式は、鏡面および拡散成分の両方に対して有効ですが、微小面BRDF $f_m$ の違いがあります。

この方程式は _ミクロレベル_ の半球面上で積分するために使用されることに注意することが重要です: 

![Figure [microLevel]: 表面応答をモデリングするためには、単一の点でミクロレベルでの積分が必要です](images/diagram_micro_vs_macro.png)

上の図は、マクロレベルでは表面が平らであると考えられていることを示しています。これは、単一方向から照らされた陰影片が表面の単一点に対応すると仮定することで、方程式を単純化するのに役立ちます。

しかし、ミクロレベルでは、表面は平坦ではないため、一本の光線を仮定することはできなくなります（ただし、入射光線は平行であると仮定することはできます）。平行な入射光線の束がある場合、微小面は異なる方向に光を散乱させるので、上図ではmと記されている半球上の表面応答を積分しなければならない。

各陰影片について、微小面の半球上での完全な積分を計算することは、明らかに実用的ではありません。したがって、鏡面成分と拡散成分の両方の積分の近似値に頼ることになります。

## 誘電体と導体

以下に示すいくつかの方程式や振る舞いをより理解するためには、まず金属（導体）と非金属（誘電体）の表面の違いを明確に理解する必要があります。

以前にも述べたように、BRDFによって制御される表面に入射した光は、拡散反射と鏡面反射の2つの別々の成分として反射します。この振る舞いのモデリングは、figure [bsdfBrdf]に示すように直感的です。

![Figure [bsdfBrdf]: BSDFのBRDF部分のモデリング](images/diagram_fr_fd.png)

このモデリングは、光が実際に表面と相互作用する方法の単純化です。実際には、入射光の一部は表面を通過し、内部で散乱し、再び拡散反射として表面から出てきます。この現象は、figure [diffuseScattering]に示されています。

![Figure [diffuseScattering]: 拡散光の散乱](images/diagram_scattering.png)

ここに、導体と誘電体の違いがあります。純粋な金属材料では、表面下散乱は発生しないため、拡散成分は存在しません（そして後で見るように、これは鏡面成分の知覚される色に影響を与えます）。一方、誘電体では散乱が起こるため、鏡面成分と拡散成分の両方が存在します。

BRDFを適切にモデリングするためには、誘電体と導体を区別する必要があります（わかりやすさのために散乱は表示されていません）、figure [dielectricConductor]に示すように。

![Figure [dielectricConductor]: 誘電体(dielectric)と導体(conductor)表面のBRDFモデリング](images/diagram_brdf_dielectric_conductor.png)

## エネルギーの保存

エネルギーの保存は物理ベースレンダリングにおいて良いBRDF（双方向反射分布関数）の重要な要素の一つです。エネルギー保存型のBRDFでは、反射される光の全エネルギー（鏡面反射と拡散反射）は入射エネルギーの総量よりも少なくなります。エネルギー保存型のBRDFがない場合、アーティストは表面から反射される光が入射光よりも強くならないように手動で調整する必要があります。

## 鏡面BRDF

鏡面項については、$f_r$はフレネル則でモデル化できる鏡面BRDFであり、微小面モデル積分のCook-Torrance近似で$F$と記される:

$$\begin{equation}
f_r(v,l) = \frac{D(h, \alpha) G(v, l, \alpha) F(v, h, f0)}{4(\NoV)(\NoL)}
\end{equation}$$

リアルタイム制約がある場合、$D$、$G$、$F$の3つの項について近似を使用する必要があります。[#Karis13a]では、Cook-Torranceの鏡面BRDFに使用できるこれらの3つの項の数式がまとめられています。以下のセクションでは、これらの項に選んだ数式について説明します。

### 正規分布関数（鏡面D）

[#Burley12]は、長いテールを持つ正規分布関数（NDF）が実世界の表面に適していると述べています。[#Walter07]で説明されているGGX分布は、ハイライト部分に長いテールと短いピークを持つ分布であり、リアルタイムの実装に適した簡単な数式です。これはまた、現代の物理ベースレンダラーでは、Trowbridge-Reitz分布に相当する、よく使われるモデルでもあります。

$$\begin{equation}
D_{GGX}(h,\alpha) = \frac{\aa}{\pi ( (\NoH)^2 (\aa - 1) + 1)^2}
\end{equation}$$

NDFのGLSL実装は、listing [specularD]に示されているように、シンプルで効率的です。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float D_GGX(float NoH, float roughness) {
    float a = NoH * roughness;
    float k = roughness / (1.0 - NoH * NoH + a * a);
    return k * k * (1.0 / PI);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [specularD]: GLSLによる鏡面D項の実装]

半精度浮動小数点数を使用することで、この実装を改善することができます。この最適化では、元の方程式に変更が必要です。なぜなら、半精度浮動小数点数で$1 - (\NoH)^2$を計算する際に2つの問題が発生するからです。まず、この計算は、$(\NoH)^2$が1に近い場合（ハイライト）に浮動小数点のキャンセル問題によって影響を受けます。さらに、$\NoH$は1付近では十分な精度がありません。

解決策はラグランジュの恒等式を使用することです。

$$\begin{equation}
| a \times b |^2 = |a|^2 |b|^2 - (a \cdot b)^2
\end{equation}$$

$n$と$h$が単位ベクトルであるため、$|n \times h|^2 = 1 - (\NoH)^2$です。これにより、クロス積を使用して半精度浮動小数点数で直接$1 - (\NoH)^2$を計算することができます。最適化された最終的な実装は、listing [specularDfp16]に示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#define MEDIUMP_FLT_MAX    65504.0
#define saturateMediump(x) min(x, MEDIUMP_FLT_MAX)

float D_GGX(float roughness, float NoH, const vec3 n, const vec3 h) {
    vec3 NxH = cross(n, h);
    float a = NoH * roughness;
    float k = roughness / (dot(NxH, NxH) + a * a);
    float d = k * k * (1.0 / PI);
    return saturateMediump(d);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing （specularDfp16）: fp16に最適化されたGLSLの鏡面D項の実装]

### 幾何学的なシェーディング（鏡面G）

Eric Heitzは、[#Heitz14]でSmithの幾何学的な影関数が正確な $G$ の項であることを示しました。スミスの定式化は次のようです: 

$$
\begin{equation}
G(v,l,\alpha) = G_1(l,\alpha) G_1(v,\alpha)
\end{equation}
$$

$G_1$はさまざまなモデルに従うことができ、一般的にはGGXの定式化に設定されます: 

$$\begin{equation}
G_1(v,\alpha) = G_{GGX}(v,\alpha) = \frac{2 (\NoV)}{\NoV + \sqrt{\aa + (1 - \aa) (\NoV)^2}}
\end{equation}$$

したがって、完全なSmith-GGXの定式化は次のようになります: 

$$\begin{equation}
G(v,l,\alpha) = \frac{2 (\NoL)}{\NoL + \sqrt{\aa + (1 - \aa) (\NoL)^2}} \frac{2 (\NoV)}{\NoV + \sqrt{\aa + (1 - \aa) (\NoV)^2}}
\end{equation}$$

被除数の $2 (\NoL)$ と $2 (n \cdot v)$ によって、可視性関数 $v$ を導入することで元の関数 $f_r$ を簡略化することができることが分かります:

$$\begin{equation}
f_r(v,l) = D(h, \alpha) V(v, l, \alpha) F(v, h, f_0)
\end{equation}$$

ここで: 

$$\begin{equation}
V(v,l,\alpha) = \frac{G(v, l, \alpha)}{4 (\NoV) (\NoL)} = V_1(l,\alpha) V_1(v,\alpha)
\end{equation}$$

および: 

$$\begin{equation}
V_1(v,\alpha) = \frac{1}{\NoV + \sqrt{\aa + (1 - \aa) (\NoV)^2}}
\end{equation}$$

ただし、Heitzは、微小面の高さを考慮してマスキングとシャドウィングを関連付けることにより、より正確な結果が得られると指摘しています。彼は高さに相関するSmithの関数を次のように定義しています: 

$$\begin{equation}
G(v,l,h,\alpha) = \frac{\chi^+(\VoH) \chi^+(\LoH)}{1 + \Lambda(v) + \Lambda(l)}
\end{equation}$$

$$\begin{equation}
\Lambda(m) = \frac{-1 + \sqrt{1 + \aa tan^2(\theta_m)}}{2} = \frac{-1 + \sqrt{1 + \aa \frac{(1 - cos^2(\theta_m))}{cos^2(\theta_m)}}}{2}
\end{equation}$$

$cos(\theta_m)$ を $\NoV$ に置き換えると、次の式が得られます: 

$$\begin{equation}
\Lambda(v) = \frac{1}{2} \left( \frac{\sqrt{\aa + (1 - \aa)(\NoV)^2}}{\NoV} - 1 \right)
\end{equation}$$

これから、以下の可視性関数を導くことができます:

$$\begin{equation}
V(v,l,\alpha) = \frac{0.5}{\NoL \sqrt{(\NoV)^2 (1 - \aa) + \aa} + \NoV \sqrt{(\NoL)^2 (1 - \aa) + \aa}}
\end{equation}$$

GLSLの可視性項の実装は、listing [specularV]に示すように、2つの `sqrt` 演算を必要とするため、望ましくないほど高価です。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float V_SmithGGXCorrelated(float NoV, float NoL, float roughness) {
    float a2 = roughness * roughness;
    float GGXV = NoL * sqrt(NoV * NoV * (1.0 - a2) + a2);
    float GGXL = NoV * sqrt(NoL * NoL * (1.0 - a2) + a2);
    return 0.5 / (GGXV + GGXL);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [specularV]: GLSLでの鏡面 V 項の実装]

平方根以下の項がすべて2乗であること、すべての項が$[0..1]$の範囲にあることに注目し、近似を用いてこの可視性関数を最適化することができる:

$$\begin{equation}
V(v,l,\alpha) = \frac{0.5}{\NoL (\NoV (1 - \alpha) + \alpha) + \NoV (\NoL (1 - \alpha) + \alpha)}
\end{equation}$$

この近似は数学的には正しくありませんが、2つの平方根演算を節約し、listing [approximatedSpecularV]に示すように、リアルタイムのモバイルアプリケーションには十分です。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float V_SmithGGXCorrelatedFast(float NoV, float NoL, float roughness) {
    float a = roughness;
    float GGXV = NoL * (NoV * (1.0 - a) + a);
    float GGXL = NoV * (NoL * (1.0 - a) + a);
    return 0.5 / (GGXV + GGXL);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [approximatedSpecularV]: GLSLでの近似された鏡面 V 項の実装]

[#Hammon17]は、同様の観察に基づいて近似を提案しており、平方根を取り除くために式を _lerp_ として書き直しています: 

$$\begin{equation}
V(v,l,\alpha) = \frac{0.5}{lerp(2 (\NoL) (\NoV), \NoL + \NoV, \alpha)}
\end{equation}$$

<!-- ### Fresnel (specular F) -->
### フレネル（鏡面F）

フレネル効果は、物理ベースの材料の外観に重要な役割を果たします。この効果は、観察者が表面から反射される光の量が視野角に依存するという事実をモデル化しています。大きな水面は、この現象を体験する完璧な方法です。figure [fresnelLake]に示されているように、水を真上から観察すると（法線入射）、水を通して見ることができます。ただし、遠くを見ると（浅い角度で、知覚される光線が表面に平行になる場合）、水面の鏡面反射がより強くなります。

反射する光の量は、視野角だけでなく、素材の屈折率（IOR）にも依存します。法線入射（垂直方向、または0度の角度）では、反射される光の量は$\fNormal$で示され、セクション[Reflectance remapping]で説明するように、IORから導かれます。浅い角度で反射される光の量は$\fGrazing$で示され、滑らかな材料では100％に近づきます。

![Figure [fresnelLake]: フレネル効果は、大きな水面で特に顕著に表れます](images/photo_fresnel_lake.jpg)

より正式には、フレネル項は、2つの異なる媒質の境界面で光が反射および屈折する方法、または反射と透過のエネルギーの比率を定義しています。[#Schlick94]では、Cook-Torranceの鏡面BRDFに対するフレネル項の安価な近似を説明しています。

$$\begin{equation}
F_{Schlick}(v,h,\fNormal,\fGrazing) = \fNormal + (\fGrazing - \fNormal)(1 - \VoH)^5
\end{equation}$$

定数$\fNormal$は、法線入射時の鏡面反射率を表し、誘電体の場合は非色乗り、金属の場合は色乗りになります。実際の値は、界面の屈折率に依存します。この項のGLSL実装では、`pow`の使用が必要であり、listing [specularF]に示されているように、いくつかの乗算で置き換えることができます。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 F_Schlick(float u, vec3 f0, float f90) {
  return f0 + (vec3(f90) - f0) * pow(1.0 - u, 5.0);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [specularF]: GLSLでの鏡面F項の実装]

このフレネル関数は、入射鏡面反射と浅い角度での反射率（ここでは$\fGrazing$で表される）を補間するものと見なすことができます。実際の材料の観察では、誘電体と導体の両方が浅い角度での非色乗りの鏡面反射率を示し、フレネル反射率が90度で1.0となることがわかっています。より正確な$\fGrazing$については、セクション[Specular occlusion]で説明されています。

$\fGrazing$を1に設定すると、フレネル項のSchlickの近似は、コードを若干再構成することでスカラ演算に最適化することができます。その結果は、listing [scalarspecularf]に示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 F_Schlick(float u, vec3 f0) {
    float f = pow(1.0 - u, 5.0);
    return f + f0 * (1.0 - f);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [scalarSpecularF]: GLSLでの鏡面F項のスカラー最適化]

<!-- ## Diffuse BRDF -->
## 拡散BRDF

拡散項では、$f_m$はLambertian関数であり、BRDFの拡散項は次のようになります。

$$\begin{equation}
\fDiffuse(v,l) = \frac{\sigma}{\pi} \frac{1}{| \NoV | | \NoL |}
\int_\Omega D(m,\alpha) G(v,l,m) (v \cdot m) (l \cdot m) dm
\end{equation}$$

私たちの実装では、代わりに微小面の半球上で均一な拡散反応を仮定する簡単なLambertian BRDFを使用します。

$$\begin{equation}
\fDiffuse(v,l) = \frac{\sigma}{\pi}
\end{equation}$$

実践的には、後で拡散反射率$\sigma$が乗算されます（listing [diffusebrdf]参照）。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float Fd_Lambert() {
    return 1.0 / PI;
}

vec3 Fd = diffuseColor * Fd_Lambert();
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [diffusebrdf]: glslでのLambertian拡散BRDFの実装]

Lambertian BRDFは明らかに非常に効率的であり、複雑なモデルとほぼ同等の結果を提供します。

ただし、拡散部分は理想的には鏡面項と連携し、表面の粗さを考慮する必要があります。ディズニーの拡散BRDF [#Burley12] と Oren-Nayarモデル [#Oren94] の両方は、粗さを考慮して接触角での逆反射を生成します。制約の上で、わずかな品質向上に見合うだけの追加の実行時間コストは正当化されないと判断しました。この洗練された拡散モデルは、画像ベースおよび球面調和数を表現および実装するのが難しくなります。

完全性のために、[#Burley12]で表現されたディズニーの拡散BRDFは以下のようになります。

$$\begin{equation}
\fDiffuse(v,l) = \frac{\sigma}{\pi} \schlick(n,l,1,\fGrazing) \cdot \schlick(n,v,1,\fGrazing)
\end{equation}$$

ここで、

$$\begin{equation}
\fGrazing=0.5 + 2 \cdot \alpha cos^2(\theta_d)
\end{equation}$$

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float F_Schlick(float u, float f0, float f90) {
    return f0 + (f90 - f0) * pow(1.0 - u, 5.0);
}

float Fd_Burley(float NoV, float NoL, float LoH, float roughness) {
    float f90 = 0.5 + 2.0 * roughness * LoH * LoH;
    float lightScatter = F_Schlick(NoL, 1.0, f90);
    float viewScatter = F_Schlick(NoV, 1.0, f90);
    return lightScatter * viewScatter * (1.0 / PI);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [diffusebrdf]: glslでのディズニー拡散BRDFの実装]

figure [lambert_vs_disney]は、完全な粗い非金属材料を使用した単純なLambertian拡散BRDFと高品質のディズニー拡散BRDFの比較を示しています。比較のために、右の球体はミラーリングされました。両方のBRDFで表面応答は非常に似ていますが、ディズニーの方が接触角でいくつかの素敵な逆反射を示します（球体の左縁をよく見てください）。

![Figure [lambert_vs_disney]: Lambertian拡散BRDF（左）とディズニー拡散BRDF（右）の比較](images/diagram_lambert_vs_disney.png)

アーティスト/開発者が必要な品質とターゲットデバイスのパフォーマンスに応じてディズニーの拡散BRDFを選択できるようにすることができます。ただし、ここで示されたように、ディズニーの拡散BRDFはエネルギー保存ではありません。

<!-- ## Standard model summary -->
## 標準モデルのまとめ

**鏡面反射項**: クック・トランス鏡面微小面モデルで、ggx法線分布関数、smith-ggx高さ関連の可視性関数、そしてシックフレネル関数を用いています。

**拡散反射項**: Lambert拡散モデルです。

スタンダードモデルの完全なGLSL実装は、[glslbrdf]のリストに示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float D_GGX(float NoH, float a) {
    float a2 = a * a;
    float f = (NoH * a2 - NoH) * NoH + 1.0;
    return a2 / (PI * f * f);
}

vec3 F_Schlick(float u, vec3 f0) {
    return f0 + (vec3(1.0) - f0) * pow(1.0 - u, 5.0);
}

float V_SmithGGXCorrelated(float NoV, float NoL, float a) {
    float a2 = a * a;
    float GGXL = NoV * sqrt((-NoL * a2 + NoL) * NoL + a2);
    float GGXV = NoL * sqrt((-NoV * a2 + NoV) * NoV + a2);
    return 0.5 / (GGXV + GGXL);
}

float Fd_Lambert() {
    return 1.0 / PI;
}

void BRDF(...) {
    vec3 h = normalize(v + l);

    float NoV = abs(dot(n, v)) + 1e-5;
    float NoL = clamp(dot(n, l), 0.0, 1.0);
    float NoH = clamp(dot(n, h), 0.0, 1.0);
    float LoH = clamp(dot(l, h), 0.0, 1.0);

    // perceptually linear roughness to roughness (see parameterization)
    float roughness = perceptualRoughness * perceptualRoughness;

    float D = D_GGX(NoH, roughness);
    vec3  F = F_Schlick(LoH, f0);
    float V = V_SmithGGXCorrelated(NoV, NoL, roughness);

    // specular BRDF
    vec3 Fr = (D * V) * F;

    // diffuse BRDF
    vec3 Fd = diffuseColor * Fd_Lambert();

    // apply lighting...
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[listing [glslbrdf]: GLSLによるBRDFの評価]

<!-- ## Improving the BRDFs -->
## BRDFの改善

前述の[エネルギー保存]セクションで、エネルギー保存が優れたBRDFの重要な要素の一つであることを述べました。残念ながら、以前に探索されたBRDFには、以下で検討する2つの問題があります。

<!-- ### Energy gain in diffuse reflectance -->
### 拡散反射におけるエネルギーの増加

Lambertian拡散BRDFは、表面で反射し、したがって拡散散乱イベントに参加できない光を考慮に入れていません。

[TODO: talk about the issue with fr+fd]

### 鏡面反射におけるエネルギー損失

先に紹介したクック-トランスBRDFは、微小面レベルでの複数のイベントをモデル化しようと試みていますが、光の単一の反射を考慮しています。この近似は、高い粗さにおいてエネルギーの損失を引き起こすことがあります。つまり、表面はエネルギーを保存していません。figure [singlevsmultibounce]は、このエネルギーの損失が発生する理由を示しています。単一の反射（または単一の散乱）モデルでは、表面に当たった光の光線は別の微小面に反射され、マスキングとシャドウング項によって破棄される場合があります。しかし、複数の反射（マルチスキャタリング）を考慮すると、同じ光の光線は微小面の領域から逃れて視点に向かって反射される可能性があります。

![Figure [singlevsmultibounce]: 単一の散乱（左）対マルチスキャタリング](images/diagram_single_vs_multi_scatter.png)

この簡単な説明に基づいて、表面が粗いほど、複数の散乱イベントを考慮しないためにエネルギーが失われる可能性が高くなることが直感的に推測できます。このエネルギーの損失は、粗い材料を暗くするように見えます。金属表面は特に影響を受けます。なぜなら、その反射はすべて鏡面的だからです。この暗くなる効果は、figure [metallicroughenergyloss]で示されています。マルチスキャタリングによってエネルギーの保存が実現されることが、figure [metallicroughenergypreservation]で示されています。

![Figure [metallicroughenergyloss]: 単一散乱による粗さに伴う暗くなる効果](images/material_metallic_energy_loss.png)

![Figure [metallicroughenergypreservation]: マルチスキャタリングによるエネルギー保存](images/material_metallic_energy_preservation.png)

BRDFのエネルギー保存特性が確認できるように、純粋に反射的な金属表面（$\fNormal = 1$）が背景と区別できないような純白の炉を使用できます。figure [whitefurnaceloss]は、前のセクションで紹介した鏡面BRDFを使用したこのような表面の見た目を示しています。粗さが増すにつれて、エネルギーの損失が明らかです。それに対して、figure [whitefurnacepreservation]は、マルチスキャタリングイベントを考慮することでエネルギーの損失が解消される様子を示しています。

![Figure [whitefurnaceloss]: 単一散乱による粗さに伴う暗くなる効果](images/material_furnace_energy_loss.png)

![Figure [whitefurnacepreservation]: マルチスキャタリングによるエネルギー保存](images/material_furnace_energy_preservation.png)

マルチスキャタリングの微小面BRDFについては、[#Heitz16]で詳しく説明されています。残念ながら、この論文ではマルチスキャタリングBRDFの確率的な評価のみが示されています。そのため、この解決策はリアルタイムレンダリングには適していません。[#Kulla17]では、クラとコンティが異なるアプローチを提案しています。彼らのアイデアは、追加のBRDFローブとしてエネルギー補償項を追加することです。式$\ref{energycompensationlobe}$に示されています。

$$\begin{equation}\label{energyCompensationLobe}
f_{ms}(l,v) = \frac{(1 - E(l)) (1 - E(v)) F_{avg}^2 E_{avg}}{\pi (1 - E_{avg}) (1 - F_{avg}(1 - E_{avg}))}
\end{equation}$$

ここで、$E$は鏡面BRDF $f_r$ の方向性アルベドであり、$\fNormal$は1に設定されています。

$$\begin{equation}
E(l) = \int_{\Omega} f(l,v) (\NoV) dv
\end{equation}$$

項$E_{avg}$は、$E$の余弦加重平均です。

$$\begin{equation}
E_{avg} = 2 \int_0^1 E(\mu) \mu d\mu
\end{equation}$$

同様に、$f_{avg}$はフレネル項の余弦加重平均です。

$$\begin{equation}
F_{avg} = 2 \int_0^1 F(\mu) \mu d\mu
\end{equation}$$

両方の項$E$と$E_{avg}$は、ルックアップテーブルに事前に計算して保存することができます。また、Schlickの近似を使用すると、$f_{avg}$は大幅に簡略化することができます。

$$\begin{equation}\label{averagefresnel}
f_{avg} = \frac{1 + 20 \fNormal}{21}
\end{equation}$$

この新しいローブは、以前に$f_r$として注目されていた元の単一の散乱ローブと組み合わせられます。

$$\begin{equation}
f_{r}(l,v) = f_{ss}(l,v) + f_{ms}(l,v)
\end{equation}$$

[#Lagarde18]では、エマニュエル・トゥルキンの貢献をもとに、式$\ref{averagefresnel}$を$\fNormal$に簡略化できることが指摘されています。彼らはまた、スケールされたGGX鏡面ローブを追加することでエネルギー補償を適用することを提案しています。

$$\begin{equation}\label{energycompensation}
f_{ms}(l,v) = \fNormal \frac{1 - e(l)}{e(l)} f_{ss}(l,v)
\end{equation}$$

鍵となるインサイトは、$E(l)$が事前に計算およびイメージベースのライティング前方積分と共有できることです。したがって、マルチスキャタリングエネルギー補償式は次のようになります。

$$\begin{equation}\label{scaledEnergyCompensationLobe}
f_r(l,v) = f_{ss}(l,v) + \fNormal \left( \frac{1}{r} - 1 \right) f_{ss}(l,v)
\end{equation}$$

Where $r$ is defined as:

$$\begin{equation}
r = \int_{\Omega} D(l,v) V(l,v) \left< \NoL \right> dl
\end{equation}$$

We can implement specular energy compensation at a negligible cost if we store $r$ in the DFG lookup table presented in section [Image based lights]. Listing [energyCompensationImpl] shows that the implementation is a direct conversion of equation $\ref{scaledEnergyCompensationLobe}$.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 energyCompensation = 1.0 + f0 * (1.0 / dfg.y - 1.0);
// Scale the specular lobe to account for multiscattering
Fr *= pixel.energyCompensation;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [energyCompensationImpl]: Implementation of the energy compensation specular lobe]

Please refer to section [Image based lights] and section [Pre-integration for multiscattering] to learn how the DFG lookup table is derived and computed.

## パラメータ化

ディズニーのマテリアルモデルは[#Burley12]で説明されており、良い出発点ですが、その多くのパラメータはリアルタイム実装には実用的でないです。さらに、私たちはアーティストや開発者の両方にとって理解しやすく使いやすい標準的なマテリアルモデルを求めています。

### 標準パラメータ

[Table [standardparameters]: 満たす制約条件のパラメータの一覧を示す]

次の表は、パラメータの一覧であり、制約条件を満たしています。

       パラメータ      |      定義
---------------------:|:---------------------
**basecolor**         | 非金属の表面の拡散反射率、および金属の表面の鏡面反射色
**metallic**          | 表面が非誘電体（0.0）または導体（1.0）と見なされるかを表す。しばしばバイナリ値（0または1）として使用される
**roughness**         | 表面の滑らかさ（0.0）または粗さ（1.0）の知覚。滑らかな表面は鮮明な反射を示す
**reflectance**       | 誘電体表面における法線入射によるフレネル反射。これは明示的な屈折率の代わりになる
**emissive**          | エミッシブ表面（ネオンなど）をシミュレートするための追加の拡散反射率。このパラメータは、ブルーム処理を伴うHDRパイプラインで主に有用です
**ambient occlusion** | 表面点に対してどれだけ環境光がアクセス可能かを定義します。これは、0.0から1.0の範囲のピクセルごとの陰影係数です。このパラメータについては、照明セクションで詳しく説明されます

figure [material_parameters]は、金属、粗さ、反射率のパラメータが表面の外観にどのように影響するかを示しています。

![Figure [material_parameters]: 上から下へ: 金属の変動、誘電体粗さの変動、金属粗さの変動、反射率の変動](images/material_parameters.png)

### タイプと範囲

私たちのマテリアルモデルの異なるパラメータのタイプと範囲を理解することは重要です。[standardparameterstypes]というテーブルで説明されています。

       パラメータ      |    タイプと範囲
---------------------:|:---------------------
**basecolor**         | 線形RGB [0..1]
**metallic**          | スカラー [0..1]
**roughness**         | スカラー [0..1]
**reflectance**       | スカラー [0..1]
**emissive**          | 線形RGB [0..1] + 輝度補正
**ambient occlusion** | スカラー [0..1]
[Table [standardparameterstypes]: 標準モデルのパラメータの範囲とタイプ]

ここで説明されているタイプと範囲は、シェーダーが期待するものです。APIやツールのUIでは、より直感的なものである場合、他のタイプや範囲でパラメータを指定できるようにすることが望ましいことに注意してください。

例えば、ベースカラーはsRGB空間で表現され、シェーダーに送信される前に線形空間に変換されることがあります。また、アーティストがメタリック、ラフネス、反射率パラメータを0から255のグレースケール値（黒から白）で表すことも役立ちます。

もう1つの例として、発光パラメータは黒体によって放射される光をシミュレートするため、色温度と強度として表現されることがあります。

### リマッピング

アーティストが標準のマテリアルモデルをより簡単かつ直感的に使用できるようにするために、パラメータ _ベースカラー_、_粗さ_、_反射率_ をリマップする必要があります。

#### 基本色のリマッピング

物質の基本色は、その物質の「金属性」によって影響を受けます。非導体は無色の鏡面反射率を持ちますが、拡散色として基本色を保持します。一方、導体は基本色を鏡面色として使用し、拡散成分を持ちません。

したがって、照明方程式では基本色ではなく、拡散色と$\fNormal$を使用する必要があります。基本色から拡散色を簡単に計算することができます。[basecolortodiffuse]リストに示されているように。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 diffusecolor = (1.0 - metallic) * basecolor.rgb;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [basecolortodiffuse]: GLSLでの基本色から拡散色への変換]

#### 反射率のリマッピング

**誘電体**

フレネル項は、$\fNormal$（法線入射角での鏡面反射率）に依存しており、誘電体では色収差がありません。[#Lagarde14]で説明されている誘電体表面のリマッピングを使用します: 

$$\begin{equation}
\fNormal = 0.16 \cdot reflectance^2
\end{equation}$$

目標は、一般的な誘電体表面（4% 反射率）と宝石（8% から 16%）のフレネル値を表現できる範囲に$\fNormal$をマッピングすることです。figure [reflectance]は、これらの一般的な値とマッピング関数の関係を示しています。

![Figure [reflectance]: 一般的な反射率の値](images/diagram_reflectance.png)

屈折率が既知である場合（例えば、空気と水の界面の屈折率は1.33）、フレネル反射率は次のように計算できます: 

$$\begin{equation}\label{fresnelequation}
\fNormal(n_{ior}) = \frac{(\nior - 1)^2}{(\nior + 1)^2}
\end{equation}$$

また、反射率値が既知である場合は、対応する屈折率を計算できます: 

$$\begin{equation}
n_{ior} = \frac{2}{1 - \sqrt{\fNormal}} - 1 
\end{equation}$$

table [commonmatreflectance]には、さまざまな材料の許容されるフレネル反射率の値が記載されています（現実世界の材料の値は2%未満ではありません）。

          材料       |    反射率   |      屈折率     |   線形値
-----------------:|:------------|:---------------|:----------------
水                |  2%          | 1.33           | 0.35
布                |  4% から 5.6% | 1.5 から 1.62   | 0.5 から 0.59
一般的な液体      |  2% から 4%   | 1.33 から 1.5   | 0.35 から 0.5
一般的な宝石      |  5% から 16%  | 1.58 から 2.33  | 0.56 から 1.0
プラスチック、ガラス |  4% から 5%   | 1.5 から 1.58   | 0.5 から 0.56
その他の誘電体材料 |  2% から 5%   | 1.33 から 1.58  | 0.35 から 0.56
目                |  2.5%        | 1.38           | 0.39
肌                |  2.8%        | 1.4            | 0.42
髪の毛            |  4.6%        | 1.55           | 0.54
歯                |  5.8%        | 1.63           | 0.6
デフォルトの値    |  4%          | 1.5            | 0.5
[Table [commonmatreflectance]: 一般的な材料の反射率（参考文献: リアルタイムレンダリング第4版）]

table [fnormalmetals]には、いくつかの金属の$\fNormal$の値がリストされています。これらの値はsRGBで与えられ、マテリアルモデルではベースカラーとして使用する必要があります。測定データからこれらのsRGB色がどのように計算されるかの詳細については、付録のセクション[specular color]を参照してください。


    金属 | $\fNormal$ (sRGB) | 16進数 | 色
--------:|:-----------------:|:------:|------------------------------------
銀      | 0.97, 0.96, 0.91  | #f7f4e8 | <div style="background-color: #f7f4e8; width: 60px">&nbsp;</div>
アルミニウム | 0.91, 0.92, 0.92 | #e8eaea | <div style="background-color: #e8eaea; width: 60px">&nbsp;</div>
チタニウム | 0.76, 0.73, 0.69 | #c1baaf | <div style="background-color: #c1baaf; width: 60px">&nbsp;</div>
鉄      | 0.77, 0.78, 0.78  | #c4c6c6 | <div style="background-color: #c4c6c6; width: 60px">&nbsp;</div>
プラチナ | 0.83, 0.81, 0.78  | #d3cec6 | <div style="background-color: #d3cec6; width: 60px">&nbsp;</div>
金      | 1.00, 0.85, 0.57  | #ffd891 | <div style="background-color: #ffd891; width: 60px">&nbsp;</div>
真鍮    | 0.98, 0.90, 0.59  | #f9e596 | <div style="background-color: #f9e596; width: 60px">&nbsp;</div>
銅      | 0.97, 0.74, 0.62  | #f7bc9e | <div style="background-color: #f7bc9e; width: 60px">&nbsp;</div>
[Table [fnormalmetals]: 一般的な金属の$\fNormal$]

すべての材料は、傾斜角においてフレネル反射率が100%であるため、以下のように$\fSpecular$を評価する際には$\fGrazing$を設定します: 

$$\begin{equation}
\fGrazing = 1.0
\end{equation}$$

figure [grazing_reflectance]は、赤いプラスチックのボールを示しています。球体の端をじっくりと見ると、傾斜角での色収差のない鏡面反射率が見えるでしょう。

![Figure [grazing_reflectance]: 傾斜角での鏡面反射率が色収差のないものになる](images/material_grazing_reflectance.png)

**導体**

金属表面の鏡面反射率は色収差があります: 

$$\begin{equation}
\fNormal = baseColor \cdot metallic
\end{equation}$$

listing [fnorma]は、誘電体と金属の材料に対してどのように$\fNormal$が計算されるかを示しています。金属の場合、鏡面反射率の色は基本色から派生しています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 f0 = 0.16 * reflectance * reflectance * (1.0 - metallic) + basecolor * metallic;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [fnorma]: glslでの誘電体と金属の材料の$\fNormal$の計算方法]

#### ラフネスの再マッピングとクランプ

ユーザーによって設定されたラフネスは、ここでは`perceptualroughness`と呼ばれ、以下の数式を用いて知覚的に線形範囲に再マッピングされます。

$$\begin{equation}
\alpha = perceptualRoughness^2
\end{equation}$$

figure [roughness_remap]は、ラフネスの値(下部)と再マッピングされた値(上部)を使用して、ラフネスが増加するにつれて銀メタリック表面を示しています。

![Figure [roughness_remap]: 再マッピングの比較: 知覚的に線形なラフネス (上部) と元のラフネス (下部)](images/material_roughness_remap.png)

この比較により、再マッピングされたラフネスがアーティストや開発者にとって理解しやすいことが明らかになります。この再マッピングがない場合、輝くメタリック表面は0.0から0.05の非常に小さな範囲に制限される必要があります。

Brent Burleyは彼のプレゼンテーション[#Burley12]でも同様の観察をしました。他の再マッピング(例: 立方体や二次関数など)を試してみた後、私たちはこのシンプルな二乗再マッピングが視覚的に魅力的で直感的な結果をもたらすことがわかり、リアルタイムアプリケーションにも適しているという結論に達しました。

最後に、ランタイム時にラフネスパラメータがさまざまな計算に使用される際に、浮動小数点の精度が問題になることを注意しておくことが重要です。例えば、モバイルGPUでは、_mediump_精度の浮動小数点は半浮動小数点(fp16)として実装されることがよくあります。

これにより、私たちのライティング方程式(ラフネスの2乗であるggx計算)において$\frac{1}{perceptualroughness^4}$のような小さな値を計算する際に問題が発生します。半浮動小数点で表現できる最小の値は$2^{-14}$または$6.1 \times 10^{-5}$です。デノーマルに対応していないデバイスではゼロでの除算を回避するために、$\frac{1}{roughness^4}$の結果は$6.1 \times 10^{-5}$より小さくならないようにする必要があります。そのため、ラフネスを0.089にクランプする必要があり、$6.274 \times 10^{-5}$の値が得られます。

性能の低下を防ぐためにデノーマルを避ける必要があります。また、明らかな0での除算を防ぐために、シェーダーでラフネスを安全範囲にクランプする必要もあります。このクランプにより、低ラフネスの値において現れる鮮明なハイライトのエイリアシング[^frostbiteroughnessclamp]も修正されます。

Since we also want specular highlights to have a minimum size (a roughness close to 0 creates almost invisible highlights), we should clamp the roughness to a safe range in the shader. This clamping has the added benefit of correcting specular aliasing[^frostbiteRoughnessClamp] that can appear for low roughness values.

[^frostbiteroughnessclamp]: Frostbiteエンジンでは解析光のラフネスを0.045にクランプして、ハイライトのエイリアシングを低減しています。これは、単精度浮動小数点数(fp32)を使用している場合に可能です。

### ブレンディングとレイヤリング

[#Burley12]と[#Neubelt13]で指摘されているように、このモデルでは、異なるマテリアル間の頑丈なブレンディングが、異なるパラメータを単純に補完することで可能になります。特に、これにより、簡単なマスクを使用して異なるマテリアルをレイヤリングすることができます。

例えば、figure [materialblending]は、_the order: 1886_でready at dawn studiosがマテリアルブレンディングとレイヤリングを使用して、シンプルな素材（金、銅、木材、さびなど）のライブラリから複雑な外観を作成した方法を示しています。

![Figure [materialblending]: マテリアルブレンディングとレイヤリング。出典: ready at dawn studios](images/material_blending.png)

マテリアルのブレンディングとレイヤリングは、実質的にはマテリアルモデルのさまざまなパラメータの補間です。figure [material_interpolation]は、光沢のあるメタリックなクロムと粗い赤いプラスチックの間の補間を示しています。中間のブレンデッドマテリアルは物理的には理にかなっていませんが、見た目はありえるものです。

![Figure [material_interpolation]: 光沢のあるクロム（左）から粗い赤いプラスチック（右）への補間](images/material_interpolation.png)

### 物理ベースの素材の作成

物理ベースの素材のデザインは、基本色、金属、粗さ、反射率という4つの主要なパラメーターの性質を理解すれば比較的簡単です。

芸術家や開発者が自分自身で物理ベースの素材を作成するための[便利なチャート/参考ガイド](./material%20properties.pdf)を提供しています。

![物理ベースの素材の作成](images/material_chart.jpg)

さらに、当社の素材モデルの使用方法の簡単な要約を以下に示します:

全ての素材について
:   **基本色**は、マイクロオクルージョン以外の照明情報を含んではいけません。

    **金属**はほぼバイナリ値です。純粋な導体は金属値が1であり、純粋な非導体は金属値が0です。0や1に近い値を使用するようにしてください。中間値は表面タイプの遷移(例: 金属からさびへ)に使用されます。

非金属性の素材について
:   **基本色**は反射する色を表し、srgb値が50-240の範囲(厳密範囲)または30-240の範囲(許容範囲)内にある必要があります。

    **金属**は0または0に近い値であるべきです。

    **反射率**は適切な値が見つからない場合には、srgbで127(0.5 linear, 4%反射率)に設定してください。90未満の値は使用しないでください(0.35 linear, 2%反射率)。

金属の素材について
:   **基本色**は、鏡面反射色と反射率の両方を表します。輝度が67%から100%(170-255 srgb)の値を使用してください。酸化または汚れた金属は、非金属成分を考慮に入れるために、清潔な金属よりも低い輝度を使用する必要があります。

    **金属**は1または1に近い値であるべきです。

    **反射率**は無視されます(基本色から計算されます)。

## クリアコートモデル

先ほど説明した通常のマテリアルモデルは、単層で構成される等方性の表面に適しています。しかし、薄い透明な層が通常の層の上にある多層の材料は、残念ながらかなり一般的です。このような材料の実世界の例には、車の塗装、ソーダ缶、漆塗りの木材、アクリルなどがあります。

![Figure [materialclearcoat]: 標準マテリアルモデル（左）とクリアコートモデル（右）の下にある青い金属表面の比較](images/material_clear_coat.png)

クリアコート層は、第二の鏡面ローブを追加することで、標準マテリアルモデルの拡張としてシミュレートできます。これには、第二の鏡面BRDFの評価が含まれます。実装とパラメータ化を簡素化するために、クリアコート層は常に等方性であり、誘電体です。ベース層は標準モデル（誘電体または導体）で許可されるものであれば何でもかまいません。

クリアコート層を透過する光が入射するため、figure [clearcoatmodel]に示すようにエネルギーの損失も考慮する必要があります。ただし、当社のモデルでは相互反射や屈折の振る舞いはシミュレートしません。

![Figure [clearcoatmodel]: クリアコート表面モデル](images/diagram_clear_coat.png)

### クリアコートの鏡面BRDF

クリアコート層は、標準モデルで使用されているクック・トランス・微小面BRDFと同様の方法でモデル化されます。クリアコート層は常に等方的であり、低い粗さ値を持つ誘電体であるため（[クリアコートのパラメータ設定]セクションを参照）、視覚的品質を著しく低下させることなく、より安価なDFG項を選択することができます。

[#Karis13a]と[#Burley12]のリストに掲載されている項目の調査から、標準モデルで既に使用しているフレネル項とNDF項は、他の項目よりも計算コストが高くないことが分かります。[#Kelemen01]では、私たちのSmith-GGX可視性項を置き換えることができる、はるかに簡単な項目が説明されています。

$$\begin{equation}
V(l,h) = \frac{1}{4(\LoH)^2}
\end{equation}$$

このマスキング・シャドウ関数は、[#Heitz14]で示されているように物理的な根拠はありませんが、その単純さからリアルタイムレンダリングには好ましいです。

要約すると、私たちのクリアコートBRDFは、クック・トランスの鏡面微小面モデルであり、GGX法線分布関数、ケレメン可視性関数、およびSchlickフレネル関数を使用しています。listing [ケレメン]は、GLSLの実装がどれほど簡単かを示しています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float v_kelemen(float loh) {
    return 0.25 / (loh * loh);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [ケレメン]: GLSLでのケレメン可視性項の実装]

**フレネル項に関する注意**

鏡面BRDFのフレネル項には、法線入射角における鏡面反射率 $\fNormal$ が必要です。このパラメータは、界面の屈折率から計算することができます。私たちは、クリアコート層がポリウレタンで作られていると仮定します。これは、コーティングやニスに使用される一般的な化合物です（[ポリウレタンの応用の一覧](https://en.wikipedia.org/wiki/list_of_polyurethane_applications#varnish)）。空気とポリウレタンの界面の屈折率は、[1.5](http://www.clearpur.com/transparent-polyurethanes/)であり、$\fNormal$ を以下のように推定できます: 

$$\begin{equation}
\fNormal(1.5) = \frac{(1.5 - 1)^2}{(1.5 + 1)^2} = 0.04
\end{equation}$$

これは、一般的な誘電体材料に関連付けられる4%のフレネル反射率に対応しています。

### 表面反射における統合

クリアコート層の追加によるエネルギーの損失を考慮する必要があるため、式$\ref{brdf}$からBRDFを以下のように再定義することができます:

$$\begin{equation}
f(v,l)=\fDiffuse(v,l) (1 - F_c) + \fSpecular(v,l) (1 - F_c) + f_c(v,l)
\end{equation}$$

ここで、$F_c$はクリアコートBRDFのフレネル項であり、$f_c$はクリアコートBRDFです。

### クリアコートのパラメータ化

クリアコート素材モデルは、標準素材モードで以前に定義されたすべてのパラメータに加えて、[clearcoatparameters] テーブルで説明されている2つのパラメータを含みます。

        パラメータ      |      定義
----------------------:|:---------------------
**clearcoat**          | クリアコートの層の強度。0から1の間のスカラー
**clearcoatroughness** | クリアコートの層の滑らかさまたは粗さの知覚。0から1の間のスカラー
[Table [clearcoatparameters]: クリアコートモデルのパラメータ]

クリアコートの粗さパラメータは、標準素材の粗さパラメータと同様に再マッピングされ、クランプされます。

[clearcoat] 図と [clearcoatroughness] 図は、クリアコートのパラメータが表面の外見にどのように影響するかを示しています。

![figure [clearcoat]: メタリックを1.0、粗さを0.8にした場合、クリアコートが0.0（左）から1.0（右）に変化する](images/material_clear_coat1.png)

![figure [clearcoatroughness]: メタリックを1.0、粗さを0.8、クリアコートを1.0にした場合、クリアコートの粗さが0.0（左）から1.0（右）に変化する](images/material_clear_coat2.png)

[clearcoatbrdf] リストでは、クリアコート素材モデルのGLSL実装が、再マッピング、パラメータ化、および標準表面応答への統合後に示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
void brdf(...) {
    // 標準モデルからfdとfrを計算する

    // クリアコートの粗さのリマッピングと線形化
    clearcoatperceptualroughness = clamp(clearcoatperceptualroughness, 0.089, 1.0);
    clearcoatroughness = clearcoatperceptualroughness * clearcoatperceptualroughness;

    // クリアコートBRDF
    float  dc = d_ggx(clearcoatroughness, noh);
    float  vc = v_kelemen(clearcoatroughness, loh);
    float  fc = f_schlick(0.04, loh) * clearcoat; // クリアコートの強度
    float frc = (dc * vc) * fc;

    // 基本層でのエネルギーの損失を考慮する
    return color * ((fd + fr * (1.0 - fc)) * (1.0 - fc) + frc);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[listing [clearcoatbrdf]: GLSLでのクリアコートBRDFの実装]

### ベースレイヤーの変更

クリアコートレイヤーの存在により、通常は空気との境界面に基づいている $\fNormal$ を再計算する必要があります。したがって、ベースレイヤーでは $\fNormal$ をクリアコートとの境界面に基づいて計算する必要があります。

これは、マテリアルの屈折率(IOR)を $\fNormal$ から計算し、新たに計算された IOR とクリアコートレイヤーの IOR (1.5) に基づいて新しい $\fNormal$ を計算することで実現できます。

まず、ベースレイヤーの IOR を計算します:

$$
IOR_{base} = \frac{1 + \sqrt{\fNormal}}{1 - \sqrt{\fNormal}}
$$

Then we compute the new $\fNormal$ from this new index of refraction:

$$
f_{0_{base}} = \left( \frac{IOR_{base} - 1.5}{IOR_{base} + 1.5} \right) ^2
$$

クリアコートレイヤーの IOR が固定されているため、両方のステップを組み合わせて単純化することができます:

$$
f_{0_{base}} = \frac{\left( 1 - 5 \sqrt{\fNormal} \right) ^2}{\left( 5 - \sqrt{\fNormal} \right) ^2}
$$

また、クリアコートレイヤーの IOR に基づいてベースレイヤーの見かけの粗さも変更する必要がありますが、これについては現時点では省略することにしました。

## 異方性モデル

以前に説明した標準的な材料モデルは、すべての方向で特性が同じである、つまり等方性の表面のみを説明することができます。しかし、ブラシのかかった金属などの多くの実世界の材料は、異方性モデルを使用してのみ再現することができます。

![Figure [異方性]: 等方性材料（左）と異方性材料（右）の比較](images/material_anisotropic.png)

### 異方性鏡面BRDF

前述した等方性鏡面BRDFを、異方性材料に対応できるように修正することができます。Burleyは、異方性のGGX NDFを使用することでこれを実現しています。

$$\begin{equation}
D_{aniso}(h,\alpha) = \frac{1}{\pi \alpha_t \alpha_b} \frac{1}{((\frac{t \cdot h}{\alpha_t})^2 + (\frac{b \cdot h}{\alpha_b})^2 + (\NoH)^2)^2}
\end{equation}$$

このNDFは、悲しいことに、$\alpha_t$（接線方向の粗さ）と$\alpha_b$（垂直方向の粗さ）という2つの補助的な粗さの用語に依存しています。NeubeltとPettineo [#Neubelt13]は、素材の2つの粗さの関係を表す_anisotropy_パラメータを使用して、$\alpha_t$から$\alpha_b$を導出する方法を提案しています。

$$
\begin{align*}
  \alpha_t &= \alpha \\
  \alpha_b &= lerp(0, \alpha, 1 - anisotropy)
\end{align*}
$$

[#Burley12]で定義されている関係は異なり、より良い結果をもたらし、直感的ですが、やや高価です。

$$
\begin{align*}
  \alpha_t &= \frac{\alpha}{\sqrt{1 - 0.9 \times anisotropy}} \\
  \alpha_b &= \alpha \sqrt{1 - 0.9 \times anisotropy}
\end{align*}
$$

代わりに、私たちは[#Kulla17]で説明されている関係に従うことを選択しました。これにより、鮮明なハイライトを生成することができます。

$$
\begin{align*}
  \alpha_t &= \alpha \times (1 + anisotropy) \\
  \alpha_b &= \alpha \times (1 - anisotropy)
\end{align*}
$$

このNDFでは、法線方向に加えて接線方向と垂直方向のベクトルが必要です。これらのベクトルは既に法線マッピングに必要なので、提供することは問題ではありません。

実装例はlisting [anisotropicbrdf]に記載されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float at = max(roughness * (1.0 + anisotropy), 0.001);
float ab = max(roughness * (1.0 - anisotropy), 0.001);

float D_GGX_Anisotropic(float NoH, const vec3 h,
        const vec3 t, const vec3 b, float at, float ab) {
    float ToH = dot(t, h);
    float BoH = dot(b, h);
    float a2 = at * ab;
    highp vec3 v = vec3(ab * ToH, at * BoH, a2 * NoH);
    highp float v2 = dot(v, v);
    float w2 = a2 / v2;
    return a2 * w2 * w2 * (1.0 / PI);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [anisotropicbrdf]: GLSLでのBurleyの異方性NDFの実装]

さらに、[#Heitz14]では、高さ相関GGX分布に対応する異方性のマスキング・シャドウ関数が提案されています。マスキング・シャドウ関数は、可視性関数の代わりに非常に簡略化することができます。

$$\begin{equation}
G(v,l,h,\alpha) = \frac{\chi^+(\VoH) \chi^+(\LoH)}{1 + \Lambda(v) + \Lambda(l)}
\end{equation}$$

$$\begin{equation}
\Lambda(m) = \frac{-1 + \sqrt{1 + \alpha_0^2 tan^2(\theta_m)}}{2} = \frac{-1 + \sqrt{1 + \alpha_0^2 \frac{(1 - cos^2(\theta_m))}{cos^2(\theta_m)}}}{2}
\end{equation}$$

ここで、

$$\begin{equation}
\alpha_0 = \sqrt{cos^2(\phi_0)\alpha_x^2 + sin^2(\phi_0)\alpha_y^2}
\end{equation}$$

派生後、次の式が得られます。

$$\begin{equation}
V_{aniso}(\NoL,\NoV,\alpha) = \frac{1}{2((\NoL)\hat{\Lambda}_v+(\NoV)\hat{\Lambda}_l)} \\
\hat{\Lambda}_v = \sqrt{\alpha^2_t(t \cdot v)^2+\alpha^2_b(b \cdot v)^2+(\NoV)^2} \\
\hat{\Lambda}_l = \sqrt{\alpha^2_t(t \cdot l)^2+\alpha^2_b(b \cdot l)^2+(\NoL)^2}
\end{equation}$$

項 $\hat{\Lambda}_v$ はすべての光線に対して同じであり、必要に応じて1度だけ計算することができます。実装例はlisting [anisotropicv]に記載されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float at = max(roughness * (1.0 + anisotropy), 0.001);
float ab = max(roughness * (1.0 - anisotropy), 0.001);

float V_SmithGGXCorrelated_Anisotropic(float at, float ab, float ToV, float BoV,
        float ToL, float BoL, float NoV, float NoL) {
    float lambdaV = NoL * length(vec3(at * ToV, ab * BoV, NoV));
    float lambdaL = NoV * length(vec3(at * ToL, ab * BoL, NoL));
    float v = 0.5 / (lambdaV + lambdaL);
    return saturateMediump(v);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [anisotropicv]: GLSLでの異方性可視性関数の実装]

### 異方性パラメータ化

異方性材料モデルは、標準の材料モデルに先ほど定義されたすべてのパラメータに加え、table [異方性パラメータ]で説明されている追加のパラメータを含んでいます。

        パラメータ    |    定義
--------------------:|:---------------------
**異方性**             | 異方性の程度。-1から1のスカラーです。
[Table [異方性パラメータ]: 異方性モデルのパラメータ]

追加の変換は必要ありません。なお、負の値は、接線方向ではなく接線方向とは逆の方向に異方性を整列させます。figure [異方性パラメータ]は、粗い金属表面の外観に異方性パラメータがどのように影響するかを示しています。

![Figure [異方性パラメータ]: 異方性が0.0（左）から1.0（右）に変化する様子](images/materials/anisotropy.png)

## サブサーフェスモデル

[未完了]

### サブサーフェスの鏡面BRDF

[未完了]

### 地下パラメータ化

[未完成]

## 衣料品モデル

先に説明したすべての素材モデルは、マクロレベルとミクロレベルの両方で密な表面をシミュレートするように設計されています。しかし、衣類や布地はしばしば緩く接続された糸で作られており、入射光を吸収し散乱させます。先に紹介した微小面BRDFは、表面が完全な鏡として振る舞うランダムな溝から成るという基本的な仮定に基づいているため、布地の性質を再現するのには適していません。硬い表面と比較すると、布地は大きな減衰を持つ柔らかい鏡面ローブと、前方/後方散乱によって引き起こされるファズライティングの存在によって特徴付けられます。一部の布地はまた、二色の鏡面カラー（例えばベルベット）も示します。figure [materialCloth]は、伝統的な微小面BRDFがデニム生地の外観を捉えることができないことを示しています。表面は剛直（ほぼプラスチックのよう）であり、服ではなくタープに似ています。この図はまた、吸収と散乱によって引き起こされる柔らかい鏡面ローブが生地の忠実な再現にどれだけ重要であるかを示しています。

Figure [materialCloth] shows how a traditional microfacet BRDF fails to capture the appearance of a sample of denim fabric. The surface appears rigid (almost plastic-like), more similar to a tarp than a piece of clothing. This figure also shows how important the softer specular lobe caused by absorption and scattering is to the faithful recreation of the fabric.

![Figure [materialCloth]: Comparison of denim fabric rendered using a traditional microfacet BRDF (left) and our cloth BRDF (right)](images/screenshot_cloth.png)

Velvet is an interesting use case for a cloth material model. As shown in figure [materialVelvet] this type of fabric exhibits strong rim lighting due to forward and backward scattering. These scattering events are caused by fibers standing straight at the surface of the fabric. When the incident light comes from the direction opposite to the view direction, the fibers will forward-scatter the light. Similarly, when the incident light from the same direction as the view direction, the fibers will scatter the light backward.

![Figure [materialVelvet]: Velvet fabric showcasing forward and backward scattering](images/screenshot_cloth_velvet.png)

Since fibers are flexible, we should in theory model the ability to groom the surface. While our model does not replicate this characteristic, it does model a visible front facing specular contribution that can be attributed to the random variance in the direction of the fibers.

It is important to note that there are types of fabrics that are still best modeled by hard surface material models. For instance, leather, silk and satin can be recreated using the standard or anisotropic material models.


### クロスの鏡面BRDF

私たちが使用するクロスの鏡面BRDFは、AshikhminとPremozeによって[#Ashikhmin07]で説明された修正された微小面BRDFです。AshikhminとPremozeは、配布項がBRDFに最も寄与する要素であり、シャドウ/マスキング項は彼らのベルベット分布には必要ないと指摘しています。配布項自体は逆ガウス分布です。これにより、ファズライティング（前方散乱と後方散乱）を実現する一方、オフセットが追加されて正面の鏡面寄与をシミュレートすることができます。このベルベットNDFと呼ばれるものは、次のように定義されます: 

$$\begin{equation}
D_{velvet}(v,h,\alpha) = c_{norm}(1 + 4 exp\left(\frac{-{cot}^2\theta_{h}}{\alpha^2}\right))
\end{equation}$$

このNDFは、同じ著者が[#Ashikhmin00]で説明しているNDFの変形であり、オフセット（ここでは1に設定）と振幅（4）が追加されています。[#Neubelt13]では、NeubeltとPettineoがこのNDFの正規化バージョンを提案しています: 

$$\begin{equation}
D_{velvet}(v,h,\alpha) = \frac{1}{\pi(1 + 4\alpha^2)} (1 + 4 \frac{exp\left(\frac{-{cot}^2\theta_{h}}{\alpha^2}\right)}{{sin}^4\theta_{h}})
\end{equation}$$

完全な鏡面BRDFの場合、私たちはまた、[#Neubelt13]に従い、従来の分母をより滑らかなバリアントで置き換えます: 

$$\begin{equation}\label{clothSpecularBRDF}
f_{r}(v,h,\alpha) = \frac{D_{velvet}(v,h,\alpha)}{4(\NoL + \NoV - (\NoL)(\NoV))}
\end{equation}$$

ベルベットNDFの実装は、[clothbrdf]のリスト中に示されており、ハーフフロート形式に適切にフィットし、高コストなコタンジェントの計算を避けるために三角関数の恒等式に依存して最適化されています。このBRDFからフレネルコンポーネントを削除しました。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float D_Ashikhmin(float roughness, float NoH) {
    // Ashikhmin 2007, "Distribution-based BRDFs"
	float a2 = roughness * roughness;
	float cos2h = NoH * NoH;
	float sin2h = max(1.0 - cos2h, 0.0078125); // 2^(-14/2), so sin2h^2 > 0 in fp16
	float sin4h = sin2h * sin2h;
	float cot2 = -cos2h / (a2 * sin2h);
	return 1.0 / (PI * (4.0 * a2 + 1.0) * sin4h) * (4.0 * exp(cot2) + sin4h);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [clothbrdf]: glslでのashikhminのベルベットNDFの実装]

[#Estevez17]では、EstevezとKullaが逆ガウスの代わりに指数関数形の正弦波に基づく異なるNDF（「チャーリー」シーン）を提案しています。このNDFはいくつかの理由で魅力的です: パラメータ化がより自然で直感的に感じられ、より柔らかい外観を提供します。以下の式$\ref{charliendf}$で示すように、その実装も簡単です: 

$$\begin{equation}\label{charlieNDF}
D(m) = \frac{(2 + \frac{1}{\alpha}) sin(\theta)^{\frac{1}{\alpha}}}{2 \pi}
\end{equation}$$

[#Estevez17]では、コストがかかるため、ここでは省略しますが、新しいシャドウング項を提案しています。代わりに、[#Neubelt13]の可視性項に依存します（上記の式$\ref{clothspecularbrdf}$に示されています）。
このNDFの実装は、[clothcharliebrdf]のリスト中に示されており、ハーフフロート形式に適切にフィットするために最適化されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float D_Charlie(float roughness, float NoH) {
    // Estevez and Kulla 2017, "Production Friendly Microfacet Sheen BRDF"
    float invAlpha  = 1.0 / roughness;
    float cos2h = NoH * NoH;
    float sin2h = max(1.0 - cos2h, 0.0078125); // 2^(-14/2), so sin2h^2 > 0 in fp16
    return (2.0 + invAlpha) * pow(sin2h, invAlpha * 0.5) / (2.0 * PI);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [clothcharliebrdf]: glslでの「チャーリー」NDFの実装]

#### シーンの色

布の外観をより細かく制御し、ユーザーが二色の光沢素材を再現できるようにするために、私たちは光沢反射率を直接修正する機能を導入しました。figure [materialclothsheen]は、私たちが「シーンの色」と呼ぶパラメータを使用した例を示しています。

![Figure [materialclothsheen]: 光沢なしの青い生地（左）と光沢ありの青い生地（右）](images/screenshot_cloth_sheen.png)

### クロスの拡散反射BRDF

私たちのクロスの素材モデルは、まだランバーチアン拡散反射BRDFに頼っています。ただし、わずかに修正され、エネルギー保存性があり（クリアコート素材モデルと同様）、オプションのサブサーフェイス散乱項を提供します。この追加項は物理的に基づいていないため、特定のタイプの生地で光の散乱、部分吸収、再放射をシミュレートするために使用することができます。

まず、オプションのサブサーフェイス散乱なしの拡散項は次のようになります: 

$$\begin{equation}
f_{d}(v,h) = \frac{c_{diff}}{\pi}(1 - F(v,h))
\end{equation}$$

ここで、$f(v,h)$は式$\ref{clothspecularbrdf}$のクロスの鏡面BRDFのフレネル項です。実際のところ、拡散成分の$1 - f(v, h)$の項を省略することにしました。その効果はわずかですが、追加のコストをかける価値はないと判断しました。

サブサーフェイス散乱は、エネルギー保存形式のラップド拡散光技術を使用して実装されています: 

$$\begin{equation}
f_{d}(v,h) = \frac{c_{diff}}{\pi}(1 - F(v,h)) \left< \frac{\NoL + w}{(1 + w)^2} \right> \left< c_{subsurface} + \NoL \right>
\end{equation}$$

ここで、$w$は0から1の値で、拡散光がターミネーターを周りにどれだけ巻き込むかを定義します。別のパラメータを導入しないために、$w = 0.5$とします。ラップド拡散光を使用する場合、拡散項は$\NoL$で乗算してはなりません。この安価なサブサーフェイス散乱の近似の効果はfigure [materialclothsubsurface]に示されています。

![Figure [materialclothsubsurface]: 白い生地（左列）と茶色いサブサーフェイス散乱を持つ白い生地（右）](images/screenshot_cloth_subsurface.png)

シーンカラーやオプションのサブサーフェイス散乱を含む私たちのクロスのBRDFの完全な実装は、リスティング[clothfullbrdf]で見つけることができます。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// 鏡面BRDF
float D = distributionCloth(roughness, NoH);
float V = visibilityCloth(NoV, NoL);
vec3  F = sheenColor;
vec3 Fr = (D * V) * F;

// 拡散BRDF
float diffuse = diffuse(roughness, NoV, NoL, LoH);
#if defined(MATERIAL_HAS_SUBSURFACE_COLOR)
// energy conservative wrap diffuse
diffuse *= saturate((dot(n, light.l) + 0.5) / 2.25);
#endif
vec3 Fd = diffuse * pixel.diffuseColor;

#if defined(MATERIAL_HAS_SUBSURFACE_COLOR)
// 低コストな表面化散乱
Fd *= saturate(subsurfaceColor + NoL);
vec3 color = Fd + Fr * NoL;
color *= (lightIntensity * lightAttenuation) * lightColor;
#else
vec3 color = Fd + Fr;
color *= (lightIntensity * lightAttenuation * NoL) * lightColor;
#endif
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [clothFullBRDF]: Implementation of our cloth BRDF in GLSL]


### クロスパラメータ化

クロスの材料モデルには、_metallic_と_reflectance_以外のすべてのパラメーターが含まれます。[clothparameters]のテーブルで説明されている2つの追加パラメーターも利用できます。

       パラメーター      |      定義
---------------------:|:---------------------
**sheencolor**        | 二色の艶を作成するための鏡面色（標準反射率に合わせてデフォルトは0.04）
**subsurfacecolor**   | マテリアルを通じた散乱と吸収後の拡散色の色合い
[Table [clothparameters]: クロスモデルのパラメーター]

ベルベットのような材料を作成するには、ベースカラーを黒（または濃い色）に設定します。色度情報は代わりにsheen colorに設定する必要があります。デニム、コットンなどの一般的なファブリックを作成するには、ベースカラーに色度を使用し、デフォルトのsheen colorを使用するか、sheen colorをベースカラーの輝度に設定します。

# 照明

照明環境の正確さと一貫性は、信憑性のある視覚表現を実現するために非常に重要です。既存のレンダリングエンジン（UnityやUnreal Engine 4など）や従来のリアルタイムレンダリングの文献を調査した結果、一貫性はほとんど達成されていないことが明らかです。

たとえば、Unreal Engineでは、アーティストはルーメン（光束の単位）で点光源の「明るさ」を指定できます。一方、方向光の明るさは任意の無名単位で表現されます。5,000ルーメンの光束を持つ点光源と同じ明るさを持つ方向光を合わせるには、アーティストは明るさ10の方向光を使用する必要があります。このような不一致は、照明を追加、削除、または変更する際にアーティストがシーンの視覚的な一貫性を維持することを難しくします。
単に任意の単位を使用することは一貫した解決策ではありますが、ライティングリグの再利用を困難にします。たとえば、屋外のシーンでは、太陽として明るさ10の方向光を使用し、その値に対して他のすべてのライトを定義します。これらのライトを室内環境に移動すると、明るすぎることになります。

したがって、私たちの目標は、全ての照明をデフォルトで正確にする一方、アーティストが目的のルックを実現するために十分な自由を持つことです。私たちは、直接照明と間接照明の2つのカテゴリに分けられる複数のライトをサポートします。

**直接照明**: 点光源、測光ライト、面光源。

**間接照明**: 画像ベースのライト（IBL）（[^localprobesmobile]のローカルと遠隔のライトプローブを含む）。

[^localprobesmobile]: モバイルではローカルライトプローブは高コストになる可能性がありますので、最初は無限遠に設定された遠隔ライトプローブに焦点を当てます。

## 単位

以下のセクションでは、さまざまなタイプの照明を実装する方法と、提案された方程式がtable [光の単位]にまとめられた異なるシンボルと単位を使用する方法について説明します。



    光度学的項目      |     表記     |       単位
-----------------------:|:------------------:|:-----------------
光度                | $\Phi$             | ルーメン（$lm$）
光度強度            | $I$                | カンデラ（$cd$）または$\frac{lm}{sr}$
照度                | $E$                | ルクス（$lx$）または$\frac{lm}{m^2}$
輝度                | $L$                | ニット（$nt$）または$\frac{cd}{m^2}$
放射力              | $\Phi_e$           | ワット（$w$）
光度効率            | $\eta$             | ルーメン毎ワット（$\frac{lm}{w}$）
光効率              | $V$                | パーセンテージ（%）
[Table [光の単位]]



適切に一貫した照明を得るためには、実世界のシーンで見つかるさまざまな光の強度の比率を尊重する光の単位を使用する必要があります。これらの強度は、家庭用電球の約800ルーメンから、昼間の空と太陽の照明における約120,000ルクスまで大きく異なる場合があります。

照明の一貫性を実現するための最も簡単な方法は、物理的な光の単位を採用することです。これにより、照明リグの完全な再利用が可能になります。物理的な光の単位を使用することで、物理ベースのカメラも使用できます。

[Table [lightTypesUnits]]では、サポートする各タイプの光に関連する光の単位が示されています。


        光のタイプ        |        単位
------------------------:|:---------------------
平行光                 | 照度（$lx$または$\frac{lm}{m^2}$）
点光源                 | 光度（$lm$）
スポットライト            | 光度（$lm$）
光度計                 | 光度強度（$cd$）
マスクされた光度計         | 光度（$lm$）
面光源                 | 光度（$lm$）
画像ベースの光源         | 輝度（$\frac{cd}{m^2}$）
[Table [lightTypesUnits]: 各光のタイプの光強度の単位]

**放射力の単位に関する注意**

市販の電球では、光の明るさを表示する際にしばしばパッケージにルーメン単位を使用しますが、光の明るさをワットで必要としていることを一般的には参照することが一般的です。ワット数は、電球がどれだけのエネルギーを使用しているかを示すだけであり、光の明るさを示すものではありません。ハロゲン、LEDなどの省エネ電球がより利用可能になっている現在、この違いを理解することはさらに重要です。

ただし、アーティストが光の明るさを電力で判断することに慣れている可能性があるため、光の明るさを定義するために電力単位を使用することをユーザーに許可する必要があります。変換は式 $\ref{radiantpowertoluminouspower}$ で示されています。

$$\begin{equation}\label{radiantPowerToLuminousPower}
\Phi = \Phi_e \eta
\end{equation}$$

式 $\ref{radiantpowertoluminouspower}$ で、$\eta$ は光の光度効率であり、ルーメン毎ワットで表されます。[最大可能な光度効率](http://en.wikipedia.org/wiki/luminous_efficacy)は683 $\frac{lm}{w}$であることを知っているため、式 $\ref{radiantpowerluminousefficiency}$ でも光度効率 $v$（光度係数とも呼ばれる）を使用することができます。

$$\begin{equation}\label{radiantPowerLuminousEfficiency}
\Phi = \Phi_e 683 \times V
\end{equation}$$

[Table [lightTypesEfficacy]]は、さまざまな光源の効率や効率を使用してワットをルーメンに変換するための参考情報として使用できます。より詳細な値は、ウィキペディアの[光度効率](http://en.wikipedia.org/wiki/luminous_efficacy)ページで入手できます。


       光のタイプ       |  効率 $\eta$   |  効率 $v$
-----------------------:|:------------------:|:-----------------
白熱灯                  | 14-35              | 2-5%
LED                    | 28-100             | 4-15%
蛍光灯                  | 60-100             | 9-15%
[Table [lightTypesEfficacy]: さまざまな光源の効率と効率]

### 光量の検証

物理的な光量を使用する大きな利点の一つは、方程式を物理的に検証できることです。専用の装置を使用して、3つの光量を測定することができます。

#### 照度

表面に到達する照度は、入射光計を使用して測定することができます。私たちのテストでは、[Sekonic L-478D](http://www.sekonic.com/products/l-478d/overview.aspx)を使用しています。figure [sekonic]に示されています。

入射光計は、表面に到達する照度を捉えるために白色の拡散ドームを使用します。測定の目的に応じて、ドームを適切に配置することが重要です。例えば、明るい晴れた日に太陽に対してドームを垂直に配置すると、水平に配置する場合とは非常に異なる結果が得られます。

![Figure [sekonic]: Sekonic L-478D入射光計](images/photo_light_meter.jpg)

#### 輝度

表面の輝度、または入射光と表面の積は、輝度計とも呼ばれるスポットメーターを使用して測定することができます。入射光メーターは拡散半球を使用して全方向から光を捉えますが、スポットメーターはシールドを使用して単一の方向からの入射光を測定します。私たちのテストでは、[Sekonic 5度ビューファインダー](http://www.sekonic.com/products/l-478dr/accessories/np-finder-5-degree-for-l-478.aspx)を使用して、L-478Dの拡散板を置き換えて5度の円錐内の輝度を測定します。

![特別なビューファインダーを使用して輝度計として動作するSekonic L-478D](images/photo_incident_light_meter.jpg)

#### 光度

光源の光度は直接測定することはできませんが、測定された照度と測定装置と光源の距離がわかっていれば、それから導出することができます。式$\ref{derivedLuminousIntensity}$は、[点光源]のセクションで議論された逆二乗則の単純な応用です。

$$\begin{equation}\label{derivedLuminousIntensity}
I = E \cdot d^2
\end{equation}$$

## 直接照明

前のセクションで、レンダラーがサポートするすべての光源タイプに対して光の単位を定義しましたが、照明方程式の結果に対する光の単位は定義していません。物理的な光の単位を選択することは、シェーダー内で輝度値を計算することを意味し、したがってすべての照明評価関数は任意の点で出射輝度$L_{out}$（または出射放射輝度）を計算します。輝度は照度$E$とBSDF $f(v,l)$ に依存します。

$$\begin{equation}\label{luminanceEquation} L_{out} = f(v,l)E \end{equation}$$

### ディレクショナルライト

ディレクショナルライトの主な目的は、屋外環境で重要な光源である太陽や月を再現することです。ディレクショナルライトは物理世界では実際に存在しませんが、光受信機から十分に遠い光源はディレクショナルであると仮定できます（つまり、figure [directionallight]に示すように、すべての入射光線が平行である）。


![Figure [directionallight]: ディレクショナルライトと表面との相互作用。光源は方向で表せる仮想的な構造です](images/diagram_directional_light.png)

この近似は、表面の拡散反応に非常に適していますが、鏡面反応は不正確です。Frostbiteエンジンは、この問題を解決するために、「太陽」ディレクショナルライトを円盤面光源として扱っています。しかし、私たちのテストでは、品質の向上が追加される計算コストに見合っていないことがわかっています。

先ほど述べたように、ディレクショナルライトには照度単位（$lx$）を選びました。これは、空と太陽の照度値を簡単に見つけることができるため（オンラインまたは光量計で）、また$\ref{luminanceEquation}$で説明される輝度の方程式を簡略化するためです。

$$\begin{equation}\label{directionalLuminanceEquation}
L_{out} = f(v,l) E_{\bot} \left< \NoL \right>
\end{equation}$$

簡略化された輝度の方程式$\ref{directionalluminanceequation}$では、$e_{\bot}$は光源の照度であり、光源に垂直な面に対するものです。ディレクショナルライトの光源が太陽をシミュレートする場合、$e_{\bot}$は太陽の光源に垂直な面の照度です。

table [sunskyilluminance]は、カリフォルニア州の3月の晴れた日に計測された太陽と空の照度に関する有用な参考値を提供しています。

          光源          |   10時   |   12時   |  17時  
--------------------------:|---------:|---------:|---------:
$Sky_{\bot} + Sun_{\bot}$  | 120,000  | 130,000  | 90,000  
$Sky_{\bot}$               | 20,000   | 25,000   | 9,000   
$Sun_{\bot}$               | 100,000  | 105,000  | 81,000  
[Table [sunskyilluminance]: 照度値（満月の照度は1 lx）]

動的ディレクショナルライトは、ランタイムで評価するのに非常にコストがかかりません。listing [glslディレクショナルライト]に示すように。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 l = normalize(-lightDirection);
float NoL = clamp(dot(n, l), 0.0, 1.0);

// lightIntensity is the illuminance
// at perpendicular incidence in lux
float illuminance = lightIntensity * NoL;
vec3 luminance = BSDF(v, l) * illuminance;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [glslディレクショナルライト]: glslでのディレクショナルライトの実装]

figure [directionallighttest]は、ディレクショナルライトのセットアップで午後の太陽（照度が110,000 lxに設定されている）を近似したシンプルなシーンのライティングの効果を示しています。イラストのため、直接のライティングのみが表示されています。

![Figure [directionallighttest]: ディレクショナルライトの下でのさまざまな粗さの絶縁材料のシリーズ](images/screenshot_directional_light.png)

[^illuminancemeasures]: 測定は光量計（sekonic l-478d）で行われました

### 光源の種類

当社のエンジンは、一般的にほとんどのレンダリングエンジンで見られる、点光源とスポットライトの2種類の光源をサポートします。ただし、これらの光源は2つの理由から伝統的に物理的に正確ではありません。

1. これらの光源は本当に点であり、無限に小さいです。
2. これらの光源は[逆二乗則](http://en.wikipedia.org/wiki/inverse-square_law)に従っていません。

第一の問題はエリアライトを用いて対処することができますが、点光源の方がコストが安く、可能な限り点光源を使用することが実用的であると見なされています。

第二の問題は簡単に修正できます。ある点光源に対して、知覚される強度は距離の二乗に比例して減少します（より正確には、光受容器からの距離の二乗に比例して減少します）。

逆二乗則に従っている点光源に対して、方程式$\ref{luminanceequation}$の項$e$は、方程式$\ref{punctuallightequation}$で表されます。ここで、$d$は表面上の点から光源までの距離です。

$$\begin{equation}\label{punctualLightEquation}
E = L_{in} \left< \NoL \right> = \frac{I}{d^2} \left< \NoL \right>
\end{equation}$$

点光源とスポットライトの違いは、$e$がどのように計算され、特に光度$i$が光力$\phi$からどのように計算されるかです。

#### ポイントライト

ポイントライトは、figure [pointlight] に示すように、空間上の位置によってのみ定義されます。

![Figure [pointlight]: ポイントライトと表面との相互作用。減衰は距離にのみ依存します。](images/diagram_point_light.png)

ポイントライトの光度は、光の立体角にわたって光度強度を積分することで計算されます。式 $\ref{pointlightluminouspower}$ に示すように、その後光度強度を容易に求めることができます。

$$\begin{equation}\label{pointLightLuminousPower}
\Phi = \int_{\Omega} I dl = \int_{0}^{2\pi} \int_{0}^{\pi} I d\theta d\phi = 4 \pi I \\
I = \frac{\Phi}{4 \pi}
\end{equation}$$

$\ref{punctuallightequation}$ に $i$ を、および $ \ref{luminanceequation} $ に $e$ をそれぞれ代入することで、ポイントライトの輝度方程式を輝度強度の関数として表すことができます（$ \ref{pointlightluminanceequation} $を参照）。

$$\begin{equation}\label{pointLightLuminanceEquation}
L_{out} = f(v,l) \frac{\Phi}{4 \pi d^2} \left< \NoL \right>
\end{equation}$$

figure [pointlighttest] は、距離減衰が適用されたポイントライトで単純なシーンを照らした場合の効果を示しています。視覚的な説明のために、光の減衰は誇張されています。

![Figure [pointlighttest]: ポイントライトに適用された逆二乗則](images/screenshot_point_light.png)

#### スポットライト

スポットライトは、空間内の位置、方向ベクトル、および2つの円錐角 $\theta_{inner}$ と $\theta_{outer}$（figure [スポットライト]参照）によって定義されます。これらの2つの角度は、スポットライトの角度減衰を定義するために使用されます。スポットライトの光評価関数は、逆二乗則とこれらの2つの角度を考慮して、適切な輝度減衰を評価する必要があります。

![Figure [スポットライト]: スポットライトと表面の相互作用。減衰は、光源までの距離と表面とスポットライトの方向ベクトルの間の角度に依存します](images/diagram_spot_light.png)

方程式 $\ref{spotlightluminouspower}$ は、スポットライトの発光力を点光源と同様の方法で計算する方法を示しています。ここで、$\theta_{outer}$ はスポットライトの円錐の外角を示し、範囲は [0..$\pi$] です。

$$\begin{equation}\label{spotLightLuminousPower}
\Phi = \int_{\Omega} I dl = \int_{0}^{2\pi} \int_{0}^{\theta_{outer}} I d\theta d\phi = 2 \pi (1 - cos\frac{\theta_{outer}}{2})I \\
I = \frac{\Phi}{2 \pi (1 - cos\frac{\theta_{outer}}{2})}
\end{equation}$$

この形式は物理的に正確ですが、スポットライトの使用が少し難しくなります。円錐の外角を変更すると、照明レベルが変化するためです。figure [スポットライトテストフォーカス]は、外角が55度と15度のスポットライトで同じシーンを照らしたもので、円錐口径が狭くなるにつれて照明レベルが増加することがわかります。

![Figure [スポットライトテストフォーカス]: スポットライトの外角の比較、55度（左）と15度（右）](images/screenshot_spot_light_focused.png)

照明と外側の円錐との結合のため、アーティストは影響範囲を微調整することなくスポットライトのパーセプトした照明を変えることはできません。したがって、この結合を無効にするためにアーティストにパラメータを提供することは合理的です。方程式 $\ref{spotlightluminouspowerb}$ は、その目的のために輝度パワーを形成する方法を示しています。

$$\begin{equation}\label{spotLightLuminousPowerB}
\Phi = \pi I \\
I = \frac{\Phi}{\pi} \\
\end{equation}$$

この新しい形式による輝度強度の計算を行う場合、figure [スポットライトテスト]のテストシーンは、両方の円錐口径で類似した照明レベルを示します。

![Figure [スポットライトテスト]: スポットライトの外角の比較、55度（左）と15度（右）](images/screenshot_spot_light.png)

この新しい形式は、スポットの反射板を完全に光を吸収するマットな拡散マスクで置き換える場合、物理ベースと考えることもできます。

スポットライトの評価関数は2つの方法で表現できます: 

- **光吸収体を用いる場合**
  $$\begin{equation}\label{spotAbsorber}
  L_{out} = f(v,l) \frac{\Phi}{\pi d^2} \left< \NoL \right> \lambda(l)
  \end{equation}$$
- **光反射体を用いる場合**
  $$\begin{equation}\label{spotReflector}
  L_{out} = f(v,l) \frac{\Phi}{2 \pi (1 - cos\frac{\theta_{outer}}{2}) d^2} \left< \NoL \right> \lambda(l)
  \end{equation}$$

方程式 $ \ref{spotabsorber} $ および $ \ref{spotreflector} $ の中の $ \lambda(l) $ は、方程式 $ \ref{spotangleatt} $ で説明されているスポットの角度減衰要素です。

$$\begin{equation}\label{spotAngleAtt}
\lambda(l) = \frac{l \cdot spotDirection - cos\theta_{outer}}{cos\theta_{inner} - cos\theta_{outer}}
\end{equation}$$

#### 減衰関数

物理ベースの点光源の適切な評価のために、逆二乗則の減衰係数の評価は必須です。しかし、単純な数式の定式化は実装の観点からは実用的ではありません: 

1. 距離の二乗での除算は、オブジェクトが交差したり、光源に「接触」したりした場合に0で除算が発生する可能性があります。

2. 各光源の影響範囲は無限です（ $ \frac{i}{d^2} $ は漸近的で、0にはなりません）。つまり、ピクセルを正しくシェードするためには、世界中のすべての光を評価する必要があります。


最初の問題は、点光源は真に点ではなく、代わりに小さな面光源として扱うという仮定を設定することで簡単に解決できます。これにより、点光源を1 cm半径の球として扱うことができます（式$\ref{finitepunctuallight}$参照）。

$$\begin{equation}\label{finitePunctualLight}
E = \frac{I}{max(d^2, {0.01}^2)}
\end{equation}$$

2番目の問題は、各光源に影響範囲を導入することで解決できます。この解決策にはいくつかの利点があります。ツールは、光源ごとに球を描画するだけで、アーティストにどの部分が光源の影響を受けるかを素早く表示することができます。レンダリングエンジンは、この追加情報を使用して光源をより積極的にカリングすることができます。また、アーティストや開発者は、光源の影響範囲を手動で微調整することによってエンジンをサポートすることができます。

数学的には、光の照度は影響範囲で定義される限界でゼロに滑らかに達するべきです。[#Karis13b]は、逆二乗関数をウィンドウ化して、光の影響の大部分が影響を受けないように提案しています。提案されたウィンドウ化は、式$\ref{attenuationwindowing}$で説明されており、$r$は光源の影響半径です。

$$\begin{equation}\label{attenuationWindowing}
E = \frac{I}{max(d^2, {0.01}^2)} \left< 1 - \frac{d^4}{r^4} \right>^2
\end{equation}$$

listing [glslpunctuallight]は、glslで物理ベースの点光源を実装する方法を示しています。このコードの中で使用されている光の強度は、ルミナスパワーからcdに変換されたルミナンス強度 $i$ です。このスニペットは最適化されておらず、一部の計算をCPUにオフロードすることができます（例: 光の逆減衰半径の2乗、またはスポットのスケールと角度）。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float getSquareFalloffAttenuation(vec3 posToLight, float lightInvRadius) {
    float distanceSquare = dot(posToLight, posToLight);
    float factor = distanceSquare * lightInvRadius * lightInvRadius;
    float smoothFactor = max(1.0 - factor * factor, 0.0);
    return (smoothFactor * smoothFactor) / max(distanceSquare, 1e-4);
}

float getSpotAngleAttenuation(vec3 l, vec3 lightDir,
        float innerAngle, float outerAngle) {
    // スケールとオフセットの計算はCPUサイドで行うことができます
    float cosOuter = cos(outerAngle);
    float spotScale = 1.0 / max(cos(innerAngle) - cosOuter, 1e-4)
    float spotOffset = -cosOuter * spotScale

    float cd = dot(normalize(-lightDir), l);
    float attenuation = clamp(cd * spotScale + spotOffset, 0.0, 1.0);
    return attenuation * attenuation;
}

vec3 evaluatePunctualLight() {
    vec3 l = normalize(posToLight);
    float NoL = clamp(dot(n, l), 0.0, 1.0);
    vec3 posToLight = lightPosition - worldPosition;

    float attenuation;
    attenuation  = getSquareFalloffAttenuation(posToLight, lightInvRadius);
    attenuation *= getSpotAngleAttenuation(l, lightDir, innerAngle, outerAngle);

    vec3 luminance = (BSDF(v, l) * lightIntensity * attenuation * NoL) * lightColor;
    return luminance;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [glslpunctuallight]: glslで点光源を実装する]次のマークダウンを、書式を崩さずに日本語に翻訳してください。

### フォトメトリックライト

時計の合ったライトは、シーンを照らすために非常に実用的で効率的な方法ですが、アーティストには光の配布に対する十分な制御がありません。建築照明設計の分野は、以下を考慮して人間のニーズに応えるために照明システムを設計することに関わります。

- 提供される光の量
- 光の色
- 空間内での光の分布

これまでに説明した照明システムは、簡単に最初の2つのポイントを対処できますが、空間内の光の分布を定義する方法が必要です。光の分布は、特に屋内シーンや一部の屋外シーン、または道路照明などにおいて重要です。figure [lightDistributionTest]は、光の分布がアーティストによって制御されているシーンを示しています。このような分布制御は、展示物を展示する際に広く使用されています（例: 博物館、店舗、ギャラリーなど）。

![Figure [光分布テスト]: ポイントライトの分布を制御する](images/screenshot_photometric_lights.png)

フォトメトリックライトは、その光の強度分布を記述するためにフォトメトリックプロファイルを使用します。一般的には、IES（Illuminating Engineering Society）とEULUMDAT（European Lumen Data format）の2つの形式が使用されますが、ここでは前者に焦点を当てます。IESプロファイルは、Unreal Engine 4、Frostbite、Renderman、Maya、Killzoneなどの多くのツールやエンジンでサポートされています。さらに、IESライトプロファイルは、電球や照明器具メーカーによって一般的に提供されています（たとえば、Philipsは[広範なIESファイル](http://www.usa.lighting.philips.com/connect/tools_literature/photometric_data_1.wpd)をダウンロードできるようにしています）。フォトメトリックプロファイルは、光源が一部で覆われた照明器具や照明器具を測定する場合に特に有用です。照明器具は、特定の方向に放射される光を遮断するため、光の分布を形成します。

![光度プロファイルで説明できる実世界の照明器具の例](images/photo_photometric_lights.jpg)

an iesプロファイルは、測定された光源の周りの球上のさまざまな角度にわたる光度を格納します。この球面座標系は通常、[iesviewer](http://www.photometricviewer.com/)などの専用ツールを使用して可視化することができる光度測光ウェブと呼ばれます。下のfigure [xarrow]は、RenderManで使用するために[pixarによって提供された](http://renderman.pixar.com/view/dp25764) xarrow iesプロファイルの光度測光ウェブを示しています。また、この画像は、当社のツール「lightgen」によるxarrow iesプロファイルの3D空間でのレンダリングも示しています。

![Figure [xarrow]: xarrow ies プロファイルが、フォトメトリックウェブとしてレンダリングされ、3D空間のポイントライトとして表示されています](images/screenshot_xarrow.png)

ies形式は文書化が不十分であり、インターネット上で見つかるファイル間に構文の違いが見られることは珍しくありません。iesプロファイルを理解するための最良のリソースは、Ian Ashdownの「IESNA LM-63光度データファイルの解析」ドキュメントです（＃Ashdown98）。要約すると、iesプロファイルは光源の周りのさまざまな角度でカンデラでの光度を保持します。測定された水平角ごとに、異なる垂直角度での一連の光度が提供されます。ただし、測定された光源が水平方向に対称であることは非常に一般的です。上記に示されているxarrowプロファイルは良い例です: 光度は垂直角度（垂直軸）で変化しますが、水平軸では対称です。iesプロファイルの垂直角度の範囲は0度から180度であり、水平角度の範囲は0度から360度です。

フィギュア [lightensamples] には、私たちの `lightgen` ツールを使用して描画された、ピクサーが提供する RenderMan の ies プロファイルのシリーズが表示されています。

![figure [lightensamples]: lightgenでレンダリングされたIES光プロファイルのシリーズ](images/screenshot_lightgen_samples.png)

iesプロファイルは、点光源、ポイントまたはスポットに直接適用することができます。そのためには、まずiesプロファイルを処理し、フォトメトリックプロファイルをテクスチャとして生成する必要があります。パフォーマンスの観点から、生成するフォトメトリックプロファイルは、特定の垂直角度におけるすべての水平角度の平均光度レベルを表す1次元テクスチャです（つまり、各ピクセルは垂直角度を表します）。本当のフォトメトリックライトを表現するには2次元テクスチャを使用する必要がありますが、ほとんどのライトは水平面において完全またはほぼ対称であるため、この近似を受け入れることができます。テクスチャに格納される値は、iesプロファイルで定義された逆最大強度によって正規化されています。これにより、テクスチャを任意の浮動小数点形式または精度にわずかなコストをかけることで輝度8ビットテクスチャ（例: グレースケールのPNG）に保存することができます。正規化された値を格納することにより、フォトメトリックプロファイルをマスクとして扱うこともできます。

フォトメトリックプロファイルをマスクとして使用する場合、光の輝度強度は、他の点光源と同様に、アーティストによって光の輝度パワーを設定することで定義されます。アーティストによって定義された輝度強度は、IESプロファイルから計算された光の輝度強度で割られます。IESプロファイルには輝度強度が含まれていますが、それは裸の電球に対してのみ有効であり、測定された輝度値は照明器具を考慮に入れています。電球ではなく照明器具の輝度を測定するために、ユニット球のモンテカルロ積分を使用してプロファイルからの輝度を行います[^xarrowIntensity]。

フォトメトリックプロファイル
:   輝度強度はプロファイル自体から取得されます。1Dテクスチャからサンプリングされたすべての値は、単に最大輝度強度で乗算されます。便宜のために、乗数も提供されます。

フォトメトリックプロファイルは、レンダリング時に単純な減衰として適用することができます。輝度方程式 $ \ref{photometricLightEvaluation} $ は、フォトメトリックポイントライトの評価関数を記述しています。

$$\begin{equation}\label{photometricLightEvaluation}
L_{out} = f(v,l) \frac{I}{d^2} \left< \NoL \right> \Psi(l)
\end{equation}$$

項$ \Psi(l) $はフォトメトリック減衰関数です。これは光ベクトルに依存しますが、光の方向にも依存します。スポットライトは既に方向ベクトルを持っていますが、フォトメトリックポイントライトにも導入する必要があります。

フォトメトリック減衰関数は、GLSLで簡単に実装することができます。これには、点光源の実装に新しい減衰係数を追加するだけです（listing [glslPunctualLight]）。変更された実装は、listing [glslPhotometricPunctualLight]に示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float getPhotometricAttenuation(vec3 posToLight, vec3 lightDir) {
    float cosTheta = dot(-posToLight, lightDir);
    float angle = acos(cosTheta) * (1.0 / PI);
    return texture2DLodEXT(lightProfileMap, vec2(angle, 0.0), 0.0).r;
}

vec3 evaluatePunctualLight() {
    vec3 l = normalize(posToLight);
    float NoL = clamp(dot(n, l), 0.0, 1.0);
    vec3 posToLight = lightPosition - worldPosition;

    float attenuation;
    attenuation  = getSquareFalloffAttenuation(posToLight, lightInvRadius);
    attenuation *= getSpotAngleAttenuation(l, lightDirection, innerAngle, outerAngle);
    attenuation *= getPhotometricAttenuation(l, lightDirection);

    float luminance = (BSDF(v, l) * lightIntensity * attenuation * NoL) * lightColor;
    return luminance;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [glslPhotometricPunctualLight]: GLSLでフォトメトリックプロファイルからの減衰の実装]

光の輝度は、CPU側で計算されます（listing [photometricLightIntensity]）。これは、フォトメトリックプロファイルがマスクとして使用されるかどうかに依存します。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float multiplier;
// フォトメトリックプロファイルがマスクとして使用される場合
if (photometricLight.isMasked()) {
    // アーティストによって設定された目標の輝度強度
    // 統合された輝度強度は、照明器具の周りのユニット球上でのモンテカルロ積分から得られます
    multiplier = photometricLight.getDesiredIntensity() /
            photometricLight.getIntegratedIntensity();
} else {
    // 便宜のために提供される乗数、デフォルトでは1.0に設定されています
    multiplier = photometricLight.getMultiplier();
}

// 最大輝度強度（cd）はIESプロファイルから取得されます
float lightIntensity = photometricLight.getMaxIntensity() * multiplier;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [photometricLightIntensity]: CPU上でフォトメトリックライトの輝度を計算する]

[^xarrowIntensity]: XArrowプロファイルは、1,750 lmの輝度強度を宣言していますが、モンテカルロ積分によると、実際の輝度強度は350 lmしかありません。

### エリアライト [未完了]

### ライトのパラメータ化

標準的なマテリアルモデルのパラメータ化と同様に、私たちの目標は、アーティストや開発者が直感的かつ簡単に使用できるようにすることです。そのため、ライトの色（または色相）とライトの強度を分けることにしました。したがって、ライトの色は線形RGB色（またはツールのUIではsRGB）として定義されます。

ライトのパラメータの完全なリストは、table [lightParameters]に示されています。

          パラメータ        |      定義
--------------------------:|:---------------------
**タイプ**                   | 方向性、点、スポット、またはエリア
**方向**              | 方向性ライト、スポットライト、光度測定点光源、および線形および管状エリアライト（方向）
**色**                  | 放射光の色、線形RGB色として指定します。ツールではsRGB色または色温度として指定できます
**強度**              | ライトの明るさ。単位はライトのタイプによって異なります
**減衰半径**         | 影響範囲の最大距離
**内角**            | スポットライトの内側のコーンの角度（度）
**外角**            | スポットライトの外側のコーンの角度（度）
**長さ**                 | エリアライトの長さ、線形または管状ライトの作成に使用されます
**半径**                 | エリアライトの半径、球状または管状ライトの作成に使用されます
**光度プロファイル**    | 光度測定ライトプロファイルを表すテクスチャ。パンクチュアルライトにのみ適用されます
**マスクされたプロファイル**         | IESプロファイルがマスクとして使用されるかどうかを示すブール値。マスクとして使用される場合、ユーザー指定の強度と統合されたIESプロファイルの強度の比率でライトの明るさが乗算されます。マスクとして使用されない場合、ユーザー指定の強度は無視され、代わりにIESの乗数が使用されます
**光度乗数** | 光度測定ライトの明るさ乗数（IESをマスクとして使用しない場合）
[Table [lightParameters]: ライトのタイプパラメータ]

**注意**: 実装を簡素化するために、すべての光度パワーはシェーダーに送信される前に光度強度（$cd$）に変換されます。変換はライトに依存し、前のセクションで説明されています。

**注意**: ライトのタイプは他のパラメータから推測することができます（例: 点光源は長さ、半径、内角、外角がすべて0です）。

#### 色温度

ただし、現実世界の人工光はしばしば色温度によって定義されます。色温度はケルビン（K）で測定され、光源の色相に類似した色の光を放射する理想的な黒体放射体の温度です。利便性のために、ツールはアーティストが光源の色相を色温度として指定できるようにするべきです（有意義な範囲は1,000 Kから12,500 Kです）。

温度からRGB値を計算するために、figure [planckianLocus]に示されているプランクの軌跡を使用することができます。この軌跡は、白熱した黒体の色が色度空間で変化するにつれて取る経路です。

![Figure [プランクの軌跡]: CIE 1931色度図上に視覚化されたプランクの軌跡（出典: Wikipedia）](images/diagram_planckian_locus.png)

この軌跡からRGB値を計算する最も簡単な方法は、[#Krystek85]で説明されている式を使用することです。Krystekのアルゴリズム（式$\ref{krystek}$）は、CIE 1960（UCS）空間で動作し、$T$が目標の温度であり、$u$と$v$がUCSの座標である場合、以下の式を使用します。

$$\begin{equation}\label{krystek}
u(T) = \frac{0.860117757 + 1.54118254 \times 10^{-4}T + 1.28641212 \times 10^{-7}T^2}{1 + 8.42420235 \times 10^{-4}T + 7.08145163 \times 10^{-7}T^2} \\
v(T) = \frac{0.317398726 + 4.22806245
 \times 10^{-5}T + 4.20481691 \times 10^{-8}T^2}{1 - 2.89741816
 \times 10^{-5}T + 1.61456053 \times 10^{-7}T^2}
\end{equation}$$

この近似は、1,000Kから15,000Kの範囲でおおよそ $ 9 \times 10^{-5} $ の精度で正確です。CIE 1960空間から、式 $\ref{cieToxyY}$ の公式を使用してxyY空間（CIES 1931）の座標を計算することができます。

$$\begin{equation}\label{cieToxyY}
x = \frac{3u}{2u - 8v + 4} \\
y = \frac{2v}{2u - 8v + 4}
\end{equation}$$

上の式は、黒体色温度と標準光源の相関色温度に有効なものです。D系列の標準CIE光源の正確な色度座標を計算したい場合は、式$\ref{seriesDtoxyY}$を使うことができます。

$$\begin{equation}\label{seriesDtoxyY}
x = \begin{cases} 0.244063 + 0.09911 \frac{10^3}{T} + 2.9678 \frac{10^6}{T^2} - 4.6070 \frac{10^9}{T^3} & 4,000K \le T \le 7,000K \\
0.237040 + 0.24748 \frac{10^3}{T} + 1.9018 \frac{10^6}{T^2} - 2.0064 \frac{10^9}{T^3} & 7,000K \le T \le 25,000K \end{cases} \\
y = -3x^2 + 2.87 x - 0.275
\end{equation}$$

xyY空間から、CIE XYZ空間に変換することができます（式$\ref{xyYtoXYZ}$）。

$$\begin{equation}\label{xyYtoXYZ}
X = \frac{xY}{y} \\
Z = \frac{(1 - x - y)Y}{y}
\end{equation}$$

私たちの要件に合わせて、$Y = 1$ を固定します。これにより、式 $\ref{XYZtoRGB}$ に示されているように、XYZ空間から線形RGBに簡単な3x3行列で変換できます。

$$\begin{equation}\label{XYZtoRGB}
\left[ \begin{matrix} R \\ G \\ B \end{matrix} \right] = M^{-1} \left[ \begin{matrix} X \\ Y \\ Z \end{matrix} \right]
\end{equation}$$

変換行列Mは、目標のRGBカラースペースのプライマリから計算されます。式 $ \ref{XYZtoRGBValues} $ は、sRGBカラースペースの逆行列を使用した変換を示しています。

$$\begin{equation}\label{XYZtoRGBValues}
\left[ \begin{matrix} R \\ G \\ B \end{matrix} \right] = \left[ \begin{matrix} 3.2404542 & -1.5371385 & -0.4985314 \\ -0.9692660 & 1.8760108 & 0.0415560 \\ 0.0556434 & -0.2040259 & 1.0572252 \end{matrix} \right] \left[ \begin{matrix} X \\ Y \\ Z \end{matrix} \right]
\end{equation}$$

これらの操作の結果は、sRGB色空間内の線形RGBトリプレットです。結果の色相に注意するため、1.0を超える値がクランプされないように正規化ステップを適用する必要があります。これにより、結果の色が歪まないようになります。

$$\begin{equation}\label{normalizedRGB}
\hat{C}_{linear} = \frac{C_{linear}}{max(C_{linear})}
\end{equation}$$

最後にsRGBの光電変換関数(OECF、式$ \ref{OECFsRGB}$で示される)を適用して、表示可能な値を得なければなりません(この値はシェーディングのためにレンダラーに渡されても線形のままであるべきです)。

$$\begin{equation}\label{OECFsRGB}
C_{sRGB} = \begin{cases} 12.92 \times \hat{C}_{linear} & \hat{C}_{linear} \le 0.0031308 \\
1.055 \times \hat{C}_{linear}^{\frac{1}{2.4}} - 0.055 & \hat{C}_{linear} \gt 0.0031308 \end{cases}
\end{equation}$$

便宜上、figure [colorTemperatureScaleCCT]は、1,000Kから12,500Kまでの相関色温度の範囲を示しています。以下で使用されるすべての色は、CIEの$D_{65}$をホワイトポイントとして仮定しています（これはsRGBカラースペースの場合と同様です）。

![Figure [colorTemperatureScaleCCT]: Scale of correlated color temperatures](images/diagram_color_temperature_cct.png)

同様に、figure [colorTemperatureScaleCIE]は、1,000Kから12,500KまでのCIE標準光源系列Dの範囲を示しています。

![Figure [colorTemperatureScaleCIE]: CIE標準光源シリーズDのスケール](images/diagram_color_temperature_cie.png)

参考のために、figure [colorTemperatureScaleCCTClamped]は、式$\ref{normalizedRGB}$で示される正規化ステップなしに相関色温度の範囲を示しています。

![Figure [色温度スケールCCTクランプ]: 相関色温度の非正規化スケール](images/diagram_color_temperature_cct_clamped.png)

table [colorTemperatureSamples] は、さまざまな一般的な光源の相関色温度を sRGB 色見本として示しています。これらの色は、$ D_{65} $ 白色点に対する相対的な色ですので、ディスプレイの白色点によっては知覚される色相が異なる場合があります。詳細については、[太陽の色は何色ですか？] をご覧ください。


| 温度（K）     | 光源                     | 色
|-------------:|:-------------------------|---------------------------
| 1,700-1,800  | マッチの炎                | <div style="background-color: #ff7d00; width: 60px">&nbsp;</div>
| 1,850-1,930  | ろうそくの炎              | <div style="background-color: #ff8701; width: 60px">&nbsp;</div>
| 2,000-3,000  | 日の出・日没時の太陽       | <div style="background-color: #ffa64c; width: 60px">&nbsp;</div>
| 2,500-2,900  | 家庭用白熱電球            | <div style="background-color: #ffb05e; width: 60px">&nbsp;</div>
| 3,000        | 白熱電球 1K               | <div style="background-color: #ffb86f; width: 60px">&nbsp;</div>
| 3,200-3,500  | クォーツ灯                | <div style="background-color: #ffbf7b; width: 60px">&nbsp;</div>
| 3,200-3,700  | 蛍光灯                    | <div style="background-color: #ffbf7b; width: 60px">&nbsp;</div>
| 3,275        | 白熱電球 2K               | <div style="background-color: #ffc180; width: 60px">&nbsp;</div>
| 3,380        | 白熱電球 5K、10K          | <div style="background-color: #ffc486; width: 60px">&nbsp;</div>
| 5,000-5,400  | 正午の太陽                | <div style="background-color: #ffe9d7; width: 60px">&nbsp;</div>
| 5,500-6,500  | 昼光（太陽＋空）          | <div style="background-color: #fff3f1; width: 60px">&nbsp;</div>
| 5,500-6,500  | 雲やもやを通した太陽       | <div style="background-color: #fff3f1; width: 60px">&nbsp;</div>
| 6,000-7,500  | 曇り空                   | <div style="background-color: #faf6ff; width: 60px">&nbsp;</div>
| 6,500        | RGBモニターの白色点       | <div style="background-color: #fff8fe; width: 60px">&nbsp;</div>
| 7,000-8,000  | 屋外の日陰                | <div style="background-color: #ebecff; width: 60px">&nbsp;</div>
| 8,000-10,000 | 一部曇りの空              | <div style="background-color: #d6e0ff; width: 60px">&nbsp;</div>
[テーブル [colorTemperatureSamples]: 一般的な光源の正規化された相関色温度]

### 事前露出された光

物理ベースのレンダリングと物理的な光の単位は、興味深い課題を提起します。ライティングコードで生成される大きな範囲の値をどのようにして保存し、扱うのでしょうか？シェーダーで完全な精度で計算を行っていると仮定すると、ライティングパスの線形出力を適切なサイズのバッファ（`RGB16F`または同等のもの）に保存できるようにしたいと思います。これを実現する最も簡単で明快な方法は、ライティングパスの結果を書き出す前にカメラの露出（物理ベースのカメラのセクションで詳しく説明します）を適用することです。この簡単なステップは、listing [preexposedLighting]に示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
fragColor = luminance * camera.exposure;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [preexposedLighting]: ライティングパスの出力は、半精度のバッファに収まるように事前に露出されます]

この解決策は、保存の問題を解決しますが、中間の計算を単精度の浮動小数点数で行う必要があります。私たちは、ライティングの作業のすべて（または少なくともほとんど）を半精度の浮動小数点数で行えるようにしたいと考えています。これは、特にモバイルデバイスでは、パフォーマンスと電力消費を大幅に改善できるからです。しかし、半精度の浮動小数点数は、この種の作業には不向きです。なぜなら、一般的な照度や輝度の値（例えば太陽の場合）は、その範囲を超えてしまうからです。解決策は、ライティングパスの結果ではなく、光自体を事前に露出させることです。これは、ライトの定数バッファを更新するのが安価であれば、CPUで効率的に行うことができます。また、GPUでも行うことができます。listing [preexposedLights]に示します。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// 入力はhighp/単精度でなければなりません。
// 範囲（強度）と精度（露出）の両方についてです。
// 出力はmediump/半精度です
float computePreExposedIntensity(highp float intensity, highp float exposure) {
    return intensity * exposure;
}

Light getPointLight(uint index) {
    Light light;
    uint lightIndex = // ライトインデックスを取得する;

    // 強度はhighp/単精度でなければなりません
    highp vec4 colorIntensity  = lightsUniforms.lights[lightIndex][1];

    // ライトを事前に露出させる
    light.colorIntensity.w = computePreExposedIntensity(
            colorIntensity.w, frameUniforms.exposure);

    return light;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [preexposedLights]: 光を事前に露出させることで、シェーディングパイプライン全体で半精度の浮動小数点数を使用できます]

実際には、以下の光を事前に露出させます: 
- 点光源（点とスポット）: GPUで
- 方向光: CPUで
- IBL: CPUで
- 材料の発光: GPUで

## 画像ベースのライティング

現実世界では、光は直接的に光源から来るか、環境中の物体から反射して間接的に来るなど、あらゆる方向から来ます。この過程で部分的に吸収されます。ある物体の周囲の環境全体を光源と見なすことができます。特にキューブマップなどの画像は、このような「環境光」をエンコードする素晴らしい方法です。これを画像ベースのライティング（IBL）または間接ライティングと呼びます。

![Figure [iblBall]: ここで表示されているオブジェクトは、画像エンコードされた環境光のみで照らされています。この技術を使って適用できる微妙なライティング効果に注目してください。](images/screenshot_ball_ibl.png)

画像ベースのライティングには制限があります。明らかに環境画像はどこかで取得されなければならず、以下で見るようにライティングに使用する前に前処理する必要があります。通常、環境画像はリアルワールドでオフラインで取得されるか、エンジンによってオフラインまたは実行時に生成されます。どちらにせよ、ローカルまたは遠隔プローブが使用されます。

これらのプローブは、遠隔またはローカルな環境を取得するために使用できます。この文書では、光が無限に遠くから来ると仮定される遠隔環境プローブに焦点を当てています（つまり、物体の表面のすべての点が同じ環境マップを使用します）。

全体の環境は、物体の表面の特定の点に光をもたらします；これを**輻射度** ($E$) と呼びます。物体から反射する光は輻射度 ($L_{out}$) と呼ばれます。入射光はBRDFの拡散部分と鏡面部分に一貫して適用されなければなりません。

画像ベースのライト（IBL）の輻射度と材料モデル（BRDF）$f(\Theta)$[^ibl1]の相互作用から得られる輻射度 $L_{out}$ は次のように計算されます: 

$$\begin{equation}
L_{out}(n, v, \Theta) = \int_\Omega  f(l, v, \Theta) L_{\bot}(l) \left< \NoL \right> dl 
\end{equation}$$ 

ここで、ここでは**マクロ**レベルの表面の振る舞いを見ていることに注意してください（ミクロレベルの方程式と混同しないでください）。そのため、これは $\vec n$ と $\vec v$ にのみ依存しています。基本的には、BRDFをIBLでエンコードされたあらゆる方向から来る「ポイントライト」に適用しています。

### IBLの種類 ###

現代のレンダリングエンジンで使われるIBLには、以下の4つの一般的な種類があります。

- **遠方光源プローブ**は、パララックスを無視できる「無限遠」での照明情報をキャプチャするために使われます。遠方プローブには、空や遠くの風景や建物などが含まれます。エンジンによってキャプチャされるか、高動的レンジ画像（HDRI）としてカメラから取得されます。

- **局所光源プローブ**は、特定の視点から世界のある領域をキャプチャするために使われます。キャプチャは、周囲の幾何学に応じて、立方体や球体に投影されます。局所プローブは遠方プローブよりも正確で、特に素材に局所的な反射を加えるのに便利です。

- **平面反射**は、シーンを平面で反転させてレンダリングすることで反射をキャプチャするために使われます。この技術は、建物の床や道路や水などの平らな表面にのみ適用できます。

- **スクリーンスペース反射**は、レンダリングされたシーン（前のフレームなどを使う）に基づいて、深度バッファでレイマーチングを行うことで反射をキャプチャするために使われます。SSRは素晴らしい結果を与えますが、非常に高価です。

さらに、静的なIBLと動的なIBLを区別する必要があります。完全に動的な昼夜サイクルを実装するには、例えば遠方光源プローブを動的に再計算する必要があります[^iblTypes1]。平面反射とスクリーンスペース反射は本質的に動的です。

### IBLユニット ###

前述の直接照明のセクションで説明したように、すべてのライトは物理単位を使用しなければなりません。そのため、IBLではルミナンス単位$\frac{cd}{m^2}$を使用します。これは、すべての直接照明の方程式の出力単位でもあります。ルミナンス単位は、エンジンによってキャプチャされたライトプローブ（動的または静的にオフラインで）に対しては簡単に使用できます。

しかし、ハイダイナミックレンジ画像はもう少し繊細に扱う必要があります。カメラは測定されたルミナンスではなく、元のシーンのルミナンスとは_関連_するだけのデバイス依存の値を記録します。そのため、アーティストには、元の絶対ルミナンスを回復するか、少なくとも近似することができる乗数を提供する必要があります。

IBLのためにHDRIのルミナンスを正しく再構築するには、アーティストは環境の写真を撮るだけでなく、以下のような追加情報を記録する必要があります: 

- **色校正**: グレーカードや[MacBeth ColorChecker]を使用する

- **カメラ設定**: 絞り、シャッター、ISO

- **ルミナンスサンプル**: スポット/ルミナンスメーターを使用する

[TODO] 測定して一般的なルミナンス値（晴れた空、室内など）をリストする

### Processing light probes ###

We saw previously that the radiance of an IBL is computed by integrating over the surface's hemisphere. Since this would obviously be too expensive to do in real-time, we must first pre-process our light probes to convert them into a format better suited for real-time interactions.

The sections below will discuss the techniques used to accelerate the evaluation of light probes:

- **Specular reflectance**: pre-filtered importance sampling and split-sum approximation

- **Diffuse reflectance**: irradiance map and spherical harmonics

### Distant light probes ###

#### 拡散BRDFの積分 ####

Using the Lambertian BRDF[^iblDiffuse1], we get the radiance:

$$
\begin{align*}
   f_d(\sigma) &= \frac{\sigma}{\pi} \\
L_d(n, \sigma) &= \int_{\Omega} f_d(\sigma) L_{\bot}(l) \left< \NoL \right> dl \\
               &= \frac{\sigma}{\pi} \int_{\Omega} L_{\bot}(l) \left< \NoL \right> dl \\
               &= \frac{\sigma}{\pi} E_d(n) \quad \text{with the irradiance} \; 
        E_d(n) = \int_{\Omega} L_{\bot}(l) \left< \NoL \right> dl
\end{align*}
$$

Or in the discrete domain:

$$ E_d(n) \equiv \sum_{\forall \, i \in image} L_{\bot}(s_i) \left< n \cdot s_i \right> \Omega_s $$

$\Omega_s$ is the solid-angle[^iblDiffuse2] associated to sample $i$.

The irradiance integral $\Ed$ can be trivially, albeit slowly[^iblDiffuse3], precomputed and stored into a cubemap for efficient access at runtime. Typically, _image_ is a cubemap or an equirectangular image. The term $ \frac{\sigma}{\pi} $ is independent of the IBL and is added at runtime to obtain the _radiance_.

![Figure [iblOriginal]: 画像ベースの環境](images/ibl/ibl_river_roughness_m0.png style="max-width:100%;")

![Figure [iblIrradiance]: Lambertian BRDFを用いた画像ベースの入射光マップ](images/ibl/ibl_irradiance.png style="max-width:100%;")


[^ibl1]: $\Theta$は、マテリアルモデル$f$のパラメータを表し、つまり「粗さ」、「アルベド」などを指します。

[^iblTypes1^]: これは静的プローブのブレンドや時間にわたるワークロードの分散によって行うことができます

[^iblDiffuse1]: Lambertian BRDFは$\vec l$、$\vec v$、または$\theta$に依存しないため、$L_d(n,v,\theta) \equiv L_d(n,\sigma)$が成り立ちます。

[^iblDiffuse2]: キューブマップの場合、$\Omega_s$は$\frac{2\pi}{6 \cdot width \cdot height}$で近似できます。

[^iblDiffuse3]: $O(12\,n^2\,m^2)$、ここで$n$と$m$はそれぞれ環境の寸法と事前計算されたキューブマップの寸法です。


ただし、照射度は球面調和関数（SH、詳細は球面調和関数セクションに記載）に分解して非常に近似することもでき、ランタイムで安価に計算することもできます。モバイル上でのテクスチャフェッチは避け、テクスチャユニットを解放するのが通常のベストプラクティスです。キューブマップに保存されていても、SH分解に続くレンダリングによって積分を事前に計算する方が桁違いに高速です。

SH decomposition is similar in concept to a Fourier transform, it expresses the signal over an orthonormal base in the frequency domain. The properties that interests us most are:

- $\cosTheta$ をエンコードするのには非常に少数の係数が必要です

- 円対称を持つカーネルによる畳み込みは非常にコストがかかりませんし、SH空間では積になります

実際には、$\cosTheta$に対しては4または9の係数（つまり2または3のバンド）だけで十分であり、$\Lt$に対してもそれ以上は必要ありません。

![Figure [iblSH3]: 3バンド（9係数）](images/ibl/ibl_irradiance_sh3.png style="max-width:100%;")

![Figure [iblSH2]: 2本のバンド (4つの係数)](images/ibl/ibl_irradiance_sh2.png style="max-width:100%;")


実際には、$\Lt$を$\cosTheta$と事前畳み込みし、これらの係数を基底スケーリングファクター$K_l^m$で事前にスケーリングして、シェーダー内で再構築コードができるだけ簡単になるようにしています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 irradianceSH(vec3 n) {
    // uniform vec3 sphericalHarmonics[9]
    // We can use only the first 2 bands for better performance
    return
          sphericalHarmonics[0]
        + sphericalHarmonics[1] * (n.y)
        + sphericalHarmonics[2] * (n.z)
        + sphericalHarmonics[3] * (n.x)
        + sphericalHarmonics[4] * (n.y * n.x)
        + sphericalHarmonics[5] * (n.y * n.z)
        + sphericalHarmonics[6] * (3.0 * n.z * n.z - 1.0)
        + sphericalHarmonics[7] * (n.z * n.x)
        + sphericalHarmonics[8] * (n.x * n.x - n.y * n.y);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [irradianceSH]: 事前スケーリングされたSHから放射輝度を再構築するためのGLSLコード]

2つのバンドがある場合、上記の計算は単一の$4 \times 4$行列とベクトルの乗算になります。

$K_l^m$による事前スケーリングのため、SH係数は色と考えることができます。特に、`sphericalHarmonics[0]`は平均入射輝度に直接対応します。


#### Specular BRDF integration ####

鏡面BRDFとは、表面の反射特性を表す関数で、入射光と視線の方向によって変化します。IBL（Image Based Lighting）とは、環境マップを用いて表面に光を当てる技術です。IBLの放射輝度と鏡面BRDFの相互作用によって生じる放射輝度$\Lout$は、以下の式で表されます: 

$$\begin{equation}\label{specularBRDFIntegration}
\Lout(n, v, \Theta) = \int_\Omega f(l, v, \Theta) \Lt(l) \left< \NoL \right> \partial l 
\end{equation}$$ 

ここで、$f$は鏡面BRDF、$\Lt$は環境マップの放射輝度、$\Omega$は半球、$n$は法線、$v$は視線、$l$は入射光、$\Theta$は表面のパラメータ（粗さや色など）を表します。$\left< \NoL \right>$は、法線と入射光の内積の正の値を表します。この式は、$\Lt$に対して$f(l, v, \Theta) \left< \NoL \right>$を畳み込み（コンボリューション）したものと見ることができます。つまり、環境はBRDFをカーネルとして*フィルタリング*されます。粗さが高いほど、鏡面反射は*ぼやけた*見え方になります。

$f$の式を代入すると、次のようになります: 

$$\begin{equation}
\Lout(n,v,\Theta) = \int_\Omega D(l, v, \alpha) F(l, v, f_0, f_{90}) V(l, v, \alpha) \left< \NoL \right> \Lt(l) \partial l
\end{equation}$$ 

ここで、$D$はノーマル分布関数（NDF）、$F$はフレネル関数、$V$は幾何学的減衰関数（GAF）を表します。$\alpha$は表面の粗さ、$f_0$と$f_{90}$はフレネル係数を表します。この式は、積分の中に$v$、$\alpha$、$f_0$、$f_{90}$が含まれているため、評価が非常に高価で、モバイルでのリアルタイムには適していません（事前フィルタリングされた重要度サンプリングを使っても）。

この回答は、以下のウェブサイトの情報をもとに作成しました[^1^][1] [^2^][2] [^3^][3] [^4^][4] [^5^][5]。

##### BRDF積分の単純化 #####

$\Lout$積分の閉じた形式の解または簡単な計算方法がないため、代わりに簡略化された方程式$\hat{I}$を使用します。この方程式では、$v = n$と仮定し、つまり視線方向$v$は常に表面法線$n$と等しいと仮定します。明らかに、この仮定は、視線に依存する効果すべて、例えば視点に近い反射での増加したぼやけ（ストレッチリフレクションなど）を無効にします。

このような単純化は、白い炉などの一定の環境にも深刻な影響を与える可能性があります。なぜなら、その結果の定数項（つまり、DC項）の大きさに影響を与えるからです。しかし、適切に選択すれば、簡略化された積分にスケールファクター$K$を用いることで、平均放射輝度を正確に保つことができます。

- $I$ は私たちの元の積分であり、つまり次のように表されます: $I(g) = \int_\Omega g(l) \left< \NoL \right> \partial l$
- $\hat{I}$ は、$v = n$ となる簡略化された積分です。
- $K$ は、$\hat{I}$ によって平均放射輝度が変わらないようにするスケールファクターです。
- $\tilde{I}$ は私たちの最終的な $I$ の近似であり、$\tilde{I} = \hat{I} \times K$ です。

$I$が積分であるため、積算はそれに対して分配できます。すなわち、$I(g()f()) = I(g())I(f())$ となります。


それを持って、

$$\begin{equation}
I( f(\Theta) \Lt ) \approx \tilde{I}( f(\Theta) \Lt )                       \\
\tilde{I}( f(\Theta) \Lt ) = K \times \hat{I}( f(\Theta) \Lt )              \\
K = \frac{I(f(\Theta))}{\hat{I}(f(\Theta))}
\end{equation}$$ 


上の方程式から、$\Lt$が定数のとき$\tilde{I}$は$I$と等価であることがわかります。
そして、正しい結果を得ます:

$$\begin{align*}
\tilde{I}(f(\Theta)\Lt^{constant}) &= \Lt^{constant} \hat{I}(f(\Theta)) \frac{I(f(\Theta))}{\hat{I}(f(\Theta))} \\
                                   &= \Lt^{constant} I(f(\Theta))                                               \\
                                   &= I(f(\Theta)\Lt^{constant})
\end{align*}$$


同様に、$v = n$の場合にも、結果が正しいことを示すことができます。その場合、$I = \hat{I}$となります。

$$\begin{align*}
\tilde{I}(f(\Theta)\Lt) &= I(f(\Theta)\Lt) \frac{I(f(\Theta))}{I(f(\Theta))}    \\
                        &= I(f(\Theta)\Lt)
\end{align*}$$

最後に、スケールファクター$K$が平均放射輝度（$\bar{\Lt}$）の要件を満たすことを示すために、$\Lt = \bar{\Lt} + (\Lt - \bar{\Lt}) = \bar{\Lt} + \Delta\Lt$を$\tilde{I}$に代入します。

$$\begin{align*}
\tilde{I}(f(\Theta)\Lt) &= \tilde{I}\left[f\left(\Theta\right) \left(\bar{\Lt} + \Delta\Lt\right)\right] \\
                        &= K \times \hat{I}\left[f\left(\Theta\right) \left(\bar{\Lt} + \Delta\Lt\right)\right] \\
                        &= K \times \left[\hat{I}\left(f\left(\Theta\right)\bar{\Lt}\right) + \hat{I}\left(f\left(\Theta\right)\Delta\Lt\right)\right] \\ 
                        &= K \times \hat{I}\left(f\left(\Theta\right)\bar{\Lt}\right) + K \times \hat{I}\left(f\left(\Theta\right) \Delta\Lt\right) \\
                        &= \tilde{I}\left(f\left(\Theta\right)\bar{\Lt}\right) + \tilde{I}\left(f\left(\Theta\right) \Delta\Lt\right) \\
                        &= I\left(f\left(\Theta\right)\bar{\Lt}\right) + \tilde{I}\left(f\left(\Theta\right) \Delta\Lt\right)
\end{align*}$$

上記の結果から、平均放射照度が正しく計算されていることが示されています。すなわち、$I(f(\Theta)\bar{\Lt})$。

この近似方法を考える一つの方法は、放射輝度$\Lt$を平均$\bar{\Lt}$と平均からの差分$\Delta\Lt$の2つの部分に分割し、平均部分の正確な積分を計算し、その後に差分部分の簡略化された積分を加えるというものです。

$$\begin{equation}
approximation(\Lt) = correct(\bar{\Lt}) + simplified(\Lt - \bar{\Lt})
\end{equation}$$ 



さて、各項を見てみましょう:

$$\begin{equation}\label{iblPartialEquations}
\hat{I}(f(n, \alpha) \Lt) = \int_\Omega f(l, n, \alpha) \Lt(l) \left< \NoL \right> \partial l   \\
\hat{I}(f(n, \alpha))     = \int_\Omega f(l, n, \alpha)        \left< \NoL \right> \partial l   \\
I(f(n, v, \alpha))        = \int_\Omega f(l, n, v, \alpha)     \left< \NoL \right> \partial l
\end{equation}$$


これらの3つの方程式は、以下で説明されているように、簡単に事前計算してルックアップテーブルに保存することができます。


##### 離散ドメイン #####

離散領域では、\ref{iblPartialEquations}の方程式は次のようになります:

$$\begin{equation}
\hat{I}(f(n, \alpha) \Lt) \equiv \frac{1}{N}\sum_{\forall \, i \in image} f(l_i, n, \alpha) \Lt(l_i) \left<\NoL\right>  \\
\hat{I}(f(n, \alpha))     \equiv \frac{1}{N}\sum_{\forall \, i \in image} f(l_i, n, \alpha)          \left<\NoL\right>  \\
I(f(n, v, \alpha))        \equiv \frac{1}{N}\sum_{\forall \, i \in image} f(l_i, n, v, \alpha)       \left<\NoL\right>
\end{equation}$$

However, in practice we're using _importance sampling_ which needs to take the $pdf$ of the distribution
into account and adds a term $\frac{4\left<\VoH\right>}{D(h_i, \alpha)\left<\NoH\right>}$.
See Importance Sampling For The IBL section:

$$\begin{equation}\label{iblImportanceSampling}
\hat{I}(f(n, \alpha) \Lt) \equiv \frac{4}{N}\sum_i^N f(l_i, n, \alpha)    \frac{\left<\VoH\right>}{D(h_i, \alpha)\left<\NoH\right>} \Lt(l_i) \left<\NoL\right>  \\
\hat{I}(f(n, \alpha))     \equiv \frac{4}{N}\sum_i^N f(l_i, n, \alpha)    \frac{\left<\VoH\right>}{D(h_i, \alpha)\left<\NoH\right>}          \left<\NoL\right>  \\
I(f(n, v, \alpha))        \equiv \frac{4}{N}\sum_i^N f(l_i, n, v, \alpha) \frac{\left<\VoH\right>}{D(h_i, \alpha)\left<\NoH\right>}          \left<\NoL\right>
\end{equation}$$


Recalling that for $\hat{I}$, we assume that $v = n$, equations \ref{iblImportanceSampling},
simplifies to:

$$\begin{equation}
\hat{I}(f(n, \alpha) \Lt) \equiv \frac{4}{N}\sum_i^N \frac{f(l_i, n,    \alpha)}{D(h_i, \alpha)} \Lt(l_i) \left<\NoL\right>  \\
\hat{I}(f(n, \alpha))     \equiv \frac{4}{N}\sum_i^N \frac{f(l_i, n,    \alpha)}{D(h_i, \alpha)}          \left<\NoL\right>  \\
I(f(n, v, \alpha))        \equiv \frac{4}{N}\sum_i^N \frac{f(l_i, n, v, \alpha)}{D(h_i, \alpha)} \frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right>
\end{equation}$$

すると、最初の2つの式は次のように結合されます: $LD(n, \alpha) = \frac{\hat{I}(f(n, \alpha) \Lt)}{\hat{I}(f(n, \alpha))}$

$$\begin{equation}\label{iblLD}
LD(n, \alpha)       \equiv \frac{\sum_i^N \frac{f(l_i, n, \alpha)}{D(h_i, \alpha)} \Lt(l_i) \left<\NoL\right>}{\sum_i^N \frac{f(l_i, n, \alpha)}{D(h_i, \alpha)}\left<\NoL\right>}
\end{equation}$$
$$\begin{equation}\label{iblDFV}
I(f(n, v, \alpha))  \equiv \frac{4}{N}\sum_i^N \frac{f(l_i, n, v, \alpha)}{D(h_i, \alpha)} \frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right>
\end{equation}$$

この時点では、残りの両方の方程式をオフラインでほぼ計算できるということに注意してください。唯一の難しさは、これらの積分を事前に計算する際に、$f_0$と$f_{90}$がわからないということです。以下で見るように、方程式\ref{iblDFV}では、これらの項を実行時に組み込むことができます。しかし、方程式\ref{iblLD}に対してはこれができないため、$f_0 = f_{90} = 1$（つまり、フレネル項は常に1に評価されると仮定する）必要があります。

実際には、BRDFの可視性項に対処する必要もありますが、それを保持することは、正確な結果と比較してわずかに劣った結果につながることがあります。そのため、我々は$V = 1$と設定します。

式\ref{iblLD}と式\ref{iblDFV}に$f$を代入しましょう:

$$\begin{equation}
f(l_i, n, \alpha) = D(h_i, \alpha)F(f_0, f_{90}, \left<\VoH\right>)V(l_i, v, \alpha)
\end{equation}$$

最初の単純化は、BRDF内の項$D(h_i, \alpha)$が分母（重要サンプリングによるPDFから来たもの）とキャンセルされ、FとVはその値が1であると仮定されるため、消えます。

$$\begin{equation}
LD(n, \alpha)       \equiv \frac{\sum_i^N V(l_i, v, \alpha)\left<\NoL\right>\Lt(l_i) }{\sum_i^N \left<\NoL\right>}
\end{equation}$$
$$\begin{equation}\label{iblFV}
I(f(n, v, \alpha))  \equiv \frac{4}{N}\sum_i^N \color{green}{F(f_0, f_{90}, \left<\VoH\right>)} V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right>
\end{equation}$$

次のように、フレネル項を式\ref{iblFV}に代入しましょう:

$$\begin{equation}
F(f_0, f_{90}, \left<\VoH\right>) = f_0 (1 - F_c(\left<\VoH\right>)) + f_{90} F_c(\left<\VoH\right>) \\
F_c(\left<\VoH\right>) = (1 - \left<\VoH\right>)^5
\end{equation}$$


$$\begin{equation}
I(f(n, v, \alpha))  \equiv \frac{4}{N}\sum_i^N \left[\color{green}{f_0 (1 - F_c(\left<\VoH\right>)) + f_{90} F_c(\left<\VoH\right>)}\right] V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
\end{equation}$$

$$
\begin{align*}
I(f(n, v, \alpha))  \equiv & \color{green}{f_0   } \frac{4}{N}\sum_i^N  \color{green}{(1 - F_c(\left<\VoH\right>))} V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
                    +      & \color{green}{f_{90}} \frac{4}{N}\sum_i^N  \color{green}{     F_c(\left<\VoH\right>) } V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right>
\end{align*}
$$


最後に、オフラインで計算可能な方程式（つまり、ランタイムパラメータ $f_0$ と $f_{90}$ に依存しない部分）を抽出します。

$$\begin{equation}\label{iblAllEquations}
DFG_1(\alpha, \left<\NoV\right>) = \frac{4}{N}\sum_i^N  \color{green}{(1 - F_c(\left<\VoH\right>))} V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
DFG_2(\alpha, \left<\NoV\right>) = \frac{4}{N}\sum_i^N  \color{green}{     F_c(\left<\VoH\right>) } V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
I(f(n, v, \alpha))  \equiv   \color{green}{f_0} \color{red}{DFG_1(\alpha, \left<\NoV\right>)} + \color{green}{f_{90}} \color{red}{DFG_2(\alpha, \left<\NoV\right>)}
\end{equation}$$


$DFG_1$と$DFG_2$は$\NoV$にのみ依存していることに注意してください。つまり、法線$n$と視線方向$v$の間の角度です。これは、積分が$n$に関して対称であるためです。
積分する際には、$\NoV$を満たす限り、任意の$v$を選択できます（例:$\VoH$を計算するとき）。


すべてを元に戻すと:

$$
\begin{align*}
\Lout(n,v,\alpha,f_0,f_{90})     &\simeq \big[ f_0 \color{red}{DFG_1(\NoV, \alpha)} + f_{90} \color{red}{DFG_2(\NoV, \alpha)} \big] \times LD(n, \alpha) \\
DFG_1(\alpha, \left<\NoV\right>) &=      \frac{4}{N}\sum_i^N  \color{green}{(1 - F_c(\left<\VoH\right>))} V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
DFG_2(\alpha, \left<\NoV\right>) &=      \frac{4}{N}\sum_i^N  \color{green}{     F_c(\left<\VoH\right>) } V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
LD(n, \alpha)                    &=      \frac{\sum_i^N V(l_i, n, \alpha)\left<\NoL\right>\Lt(l_i) }{\sum_i^N \left<\NoL\right>}
\end{align*}     
$$

#### $DFG_1$ と $DFG_2$ 項の視覚化 ####

$DFG_1$ と $DFG_2$ は、$(\NoV, \alpha)$ でインデックスされた通常の2Dテクスチャ内で事前に計算するか、
表面の解析的な近似を使用してランタイムで計算できます。 付録にサンプルコードを参照してください。
事前に計算されたテクスチャはtable [textureDFG]に表示されています。
画像ベースの照明のためのLの事前計算のC++実装は、セクション[Precomputing L for image-based lighting]にあります。


$DFG_1$                  | $DFG_2$                  | ${ DFG_1, DFG_2, 0 }$
-------------------------|--------------------------|----------------------
![](images/ibl/dfg1.png) | ![](images/ibl/dfg2.png) | ![](images/ibl/dfg.png)
[Table [textureDFG]: Y軸: $\alpha$。X軸: $cos \theta$]

$DFG_1$ と $DFG_2$ は便利に $[0, 1]$ の範囲内にありますが、8ビットのテクスチャは十分な精度を持たず、問題を引き起こします。
残念ながら、モバイルデバイスでは、16ビットまたは浮動小数点テクスチャは一般的ではなく、サンプラーの数に制約があります。
テクスチャを使用するシェーダーコードの魅力的な単純さにもかかわらず、解析的な近似を使用することが良いかもしれません。ただし、2つの項のみを保存する必要があるため、OpenGL ES 3.0のRG16Fテクスチャ形式は適しています。

このような解析的な近似は、[#Karis14]に記載されており、それ自体が[#Lazarov13]に基づいています。
[#Narkowicz14]も興味深い近似です。これら2つの近似は、セクション[Pre-integration for multiscattering]で提示されたエネルギー補償項とは互換性がありません。
table [textureApproxDFG]はこれらの近似の視覚的表現を示しています。

$DFG_1$                         | $DFG_2$                         | ${ DFG_1, DFG_2, 0 }$
--------------------------------|---------------------------------|----------------------
![](images/ibl/dfg1_approx.png) | ![](images/ibl/dfg2_approx.png) | ![](images/ibl/dfg_approx.png)
[Table [textureApproxDFG]: Y軸: $\alpha$。X軸: $cos \theta$]

#### $LD$項の視覚化 ####

$LD$は環境の畳み込みで、$\alpha$パラメータにのみ依存する関数によって行われます（これは粗さに関連しており、[粗さのリマッピングとクランプ]セクションを参照してください）。
$LD$は、LOD（Level of Detail）が増加するに従って粗さが増加した状態で事前フィルタリングされた環境を受け取るミップマップキューブマップに便利に格納できます。これは、この畳み込みが強力な低域通過フィルターであるため、うまく機能します。各ミップマップレベルをうまく活用するには、$\alpha$をリマップする必要があります。$\gamma = 2$を使用すると、うまく機能し、便利です。

$$
\begin{align*}
    \alpha       &= perceptualRoughness^2                        \\
    lod_{\alpha} &= \alpha^{\frac{1}{2}} = perceptualRoughness   \\
\end{align*}
$$

以下は例です:


![$\alpha=0.0$](images/ibl/ibl_river_roughness_m0.png style="max-width:100%;")
![$\alpha=0.2$](images/ibl/ibl_river_roughness_m1.png style="max-width:100%;")
![$\alpha=0.4$](images/ibl/ibl_river_roughness_m2.png style="max-width:100%;")
![$0.6$](images/ibl/ibl_river_roughness_m3.png style="max-width:100%;")
![$0.8$](images/ibl/ibl_river_roughness_m4.png style="max-width:100%;")

#### 間接的な鏡面反射と間接的な拡散反射の成分の可視化 ####

Figure [iblVisualized]は、間接照明が誘電体と導体とどのように相互作用するかを示しています。説明のために直接照明は除去されました。

![Figure [iblVisualized]: 間接的な拡散反射と鏡面反射の分解](images/ibl/ibl_visualization.jpg)


#### IBL評価の実装 ####

listing [iblEvaluation]は、前節で説明したさまざまなテクスチャを使用してIBLを評価するGLSLの実装を示しています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 ibl(vec3 n, vec3 v, vec3 diffuseColor, vec3 f0, vec3 f90,
        float perceptualRoughness) {
    vec3 r = reflect(n);
    vec3 Ld = textureCube(irradianceEnvMap, r) * diffuseColor;
    float lod = computeLODFromRoughness(perceptualRoughness);
    vec3 Lld = textureCube(prefilteredEnvMap, r, lod);
    vec2 Ldfg = textureLod(dfgLut, vec2(dot(n, v), perceptualRoughness), 0.0).xy;
    vec3 Lr =  (f0 * Ldfg.x + f90 * Ldfg.y) * Lld;
    return Ld + Lr;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [iblEvaluation]: 画像ベースの照明評価のGLSL実装]

しかし、球面調和関数を放射照度キューブマップの代わりに使用し、$DFG$ LUTの解析的近似を使用することで、テクスチャのルックアップをいくつか省くことができます。listing [optimizedIblEvaluation]に示します。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 irradianceSH(vec3 n) {
    // uniform vec3 sphericalHarmonics[9]
    // We can use only the first 2 bands for better performance
    return
          sphericalHarmonics[0]
        + sphericalHarmonics[1] * (n.y)
        + sphericalHarmonics[2] * (n.z)
        + sphericalHarmonics[3] * (n.x)
        + sphericalHarmonics[4] * (n.y * n.x)
        + sphericalHarmonics[5] * (n.y * n.z)
        + sphericalHarmonics[6] * (3.0 * n.z * n.z - 1.0)
        + sphericalHarmonics[7] * (n.z * n.x)
        + sphericalHarmonics[8] * (n.x * n.x - n.y * n.y);
}

// NOTE: this is the DFG LUT implementation of the function above
vec2 prefilteredDFG_LUT(float coord, float NoV) {
  // coord = sqrt(roughness), which is the mapping used by the
  // IBL prefiltering code when computing the mipmaps
  return textureLod(dfgLut, vec2(NoV, coord), 0.0).rg;
}

vec3 evaluateSpecularIBL(vec3 r, float perceptualRoughness) {
  // This assumes a 256x256 cubemap, with 9 mip levels
  float lod = 8.0 * perceptualRoughness;
  // decodeEnvironmentMap() either decodes RGBM or is a no-op if the
  // cubemap is stored in a float texture
  return decodeEnvironmentMap(textureCubeLodEXT(environmentMap, r, lod));
}

vec3 evaluateIBL(vec3 n, vec3 v, vec3 diffuseColor, vec3 f0, vec3 f90, float perceptualRoughness) {
  float NoV = max(dot(n, v), 0.0);
  vec3 r = reflect(-v, n);

  // Specular indirect
  vec3 indirectSpecular = evaluateSpecularIBL(r, perceptualRoughness);
  vec2 env = prefilteredDFG_LUT(perceptualRoughness, NoV);
  vec3 specularColor = f0 * env.x + f90 * env.y;

  // Diffuse indirect
  // We multiply by the Lambertian BRDF to compute radiance from irradiance
  // With the Disney BRDF we would have to remove the Fresnel term that
  // depends on NoL (it would be rolled into the SH). The Lambertian BRDF
  // can be baked directly in the SH to save a multiplication here
  vec3 indirectDiffuse = max(irradianceSH(n), 0.0) * Fd_Lambert();

  // Indirect contribution
  return diffuseColor * indirectDiffuse + indirectSpecular * specularColor;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [optimizedIblEvaluation]: GLSL implementation of image based lighting evaluation]


#### マルチスキャッタリングの事前統合 ####

[鏡面反射におけるエネルギー損失]のセクションでは、BRDFにおける単一の散乱イベントのみを考慮したことによるエネルギー損失を補償するために、2つ目のスケールされた鏡面ローブを使用する方法について説明しました。このエネルギー補償ローブは、以下で定義される$r$に依存する項でスケールされます。

$$\begin{equation}
r = \int_{\Omega} D(l,v) V(l,v) \left< \NoL \right> \partial l
\end{equation}$$

または、重要度サンプリングで評価した場合（IBLセクションの重要度サンプリングを参照）:

$$\begin{equation}
r \equiv  \frac{4}{N}\sum_i^N  V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right>
\end{equation}$$

この等式は、式$\ref{iblAllEquations}$で見られる$DFG_1$や$DFG_2$と非常に似ています。実際、フレネル項を除いたものです。

さらに、$f_{90} = 1$という仮定をすることで、$DFG_1$や$DFG_2$および$\Lout$の再構成を次のように書き直すことができます:

$$
\begin{align*}
\Lout(n,v,\alpha,f_0)                           &\simeq \big[ (1 - f_0) \color{red}{DFG_1^{multiscatter}(\NoV, \alpha)} + f_0 \color{red}{DFG_2^{multiscatter}(\NoV, \alpha)} \big] \times LD(n, \alpha) \\
DFG_1^{multiscatter}(\alpha, \left<\NoV\right>) &=      \frac{4}{N}\sum_i^N  \color{green}{F_c(\left<\VoH\right>)} V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
DFG_2^{multiscatter}(\alpha, \left<\NoV\right>) &=      \frac{4}{N}\sum_i^N                                        V(l_i, v, \alpha)\frac{\left<\VoH\right>}{\left<\NoH\right>} \left<\NoL\right> \\
LD(n, \alpha)                                   &=      \frac{\sum_i^N V(l_i, n, \alpha)\left<\NoL\right>\Lt(l_i) }{\sum_i^N V(l_i, n, \alpha)\left<\NoL\right>}
\end{align*}     
$$

これらの新しい$DFG$項は、[イメージベースライティングのためのLの事前計算]セクションで示された実装で使用されているものに置き換えるだけです。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float Fc = pow(1 - VoH, 5.0f);
r.x += Gv * Fc;
r.y += Gv;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [multiscatterIBLPreintegration]: マルチスキャッタリングのための$L_{DFG}$項のC++実装]

再構成を行うためには、[multiscatterIBLEvaluation]のリストをわずかに変更する必要があります。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec2 dfg = textureLod(dfgLut, vec2(dot(n, v), perceptualRoughness), 0.0).xy;
// (1 - f0) * dfg.x + f0 * dfg.y
vec3 specularColor = mix(dfg.xxx, dfg.yyy, f0);
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [multiscatterIBLEvaluation]: マルチスキャッタリングLUTを使用したイメージベースライティングの評価のGLSL実装]


#### まとめ ####

遠方の画像ベースの光源の鏡面反射の寄与を計算するために、いくつかの近似と妥協をする必要がありました。

    - $v = n$、IBLの非定数部分を積分する際に最も大きな誤差をもたらす仮定です。これにより、視点に対する粗さの異方性が完全に失われます。

    - IBLの非定数部分に対する粗さの寄与は量子化され、これらのレベル間で三線形フィルタリングが用いられて補間されます。これは、低粗さ（例えば、9 LODsのキューブマップでは0.0625程度）で最も目立ちます。

    - 事前に積分された環境をミップマップレベルに格納するため、テクスチャの縮小にミップマップレベルを使用することができません。これは、高周波領域や環境、低粗さや遠方や小さな物体において、エイリアシングやモアレのようなアーティファクトを引き起こす可能性があります。これは、キャッシュアクセスパターンが悪化することにより、パフォーマンスにも影響を与える可能性があります。

    - IBLの非定数部分に対してはフレネルがありません。

    - IBLの非定数部分に対しては可視性 = 1です。

    - Schlickのフレネル

    - 多重散乱の場合は$f_{90} = 1$です。


![Figure [iblPrefilterVsImportanceSampling]:
重要度サンプリングによる参照（上）と事前フィルタリングされたIBL（中）の比較。](images/ibl/ibl_prefilter_vs_reference.png)

![Figure [iblStretchyReflectionLoss]:
$v = n$と仮定することによる反射の誤差（下）--「伸び縮みする反射」の喪失。](images/ibl/ibl_stretchy_reflections_error.png)

![Figure [iblRoughnessInLods0]:
粗さをキューブマップのLODに格納することによる誤差（粗さ = 0.0625、つまりレベル間で正確にサンプリングする場合）。
ぼかしの代わりに、二つのぼかしの間で「クロスフェード」が見られることに注意してください。](images/ibl/ibl_trilinear_0.png)

![Figure [iblRoughnessInLods1]:
粗さをキューブマップのLODに格納することによる誤差（粗さ = 0.125、つまり正確にレベル1をサンプリングする場合）。
粗さがLODに近くなると、キューブマップの三線形フィルタリングによる誤差は減少します。傾斜角での$v = n$による誤差に注意してください。](images/ibl/ibl_trilinear_1.png)

![Figure [iblMoirePattern]:
色付きの縦縞の環境（スカイボックスは非表示）を使用して、$\alpha = 0$の金属球にテクスチャの縮小によるモアレパターン。](images/ibl/ibl_no_mipmaping.png)


### クリアコート ###

IBLをサンプリングするとき、クリアコート層は2番目の鏡面反射ローブとして計算されます。この鏡面反射ローブは、半球面上で合理的に積分できないので、視線方向に沿って向けられます。listing [clearCoatIBL]は、この近似を実践で示しています。また、エネルギー保存のステップも示しています。この2番目の鏡面反射ローブは、メインの鏡面反射ローブとまったく同じ方法で計算され、同じDFG近似を使用することに注意が必要です。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// クリアコート層に独自の法線マップがない場合、clearCoat_NoV == shading_NoV
float Fc = F_Schlick(0.04, 1.0, clearCoat_NoV) * clearCoat;
// エネルギー補償のためのベース層の減衰
iblDiffuse  *= 1.0 - Fc;
iblSpecular *= sq(1.0 - Fc);
iblSpecular += specularIBL(r, clearCoatPerceptualRoughness) * Fc;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [clearCoatIBL]: 画像ベースの照明におけるクリアコート鏡面反射ローブのGLSL実装]

### 異方性 ###

[#McAuley15] では、[#Revie12] に基づく "bent reflection vector" と呼ばれる手法について説明しています。この手法は異方性ライティングの粗い近似であり、代替手法として重点サンプリングを使用することもできます。この近似は計算コストが十分に安く、figure [anisotropicIBL1] およびfigure [anisotropicIBL2] で示されているように、良好な結果を提供します。

![Figure [anisotropicIBL1]: 曲面粗さ 0.3 (左)、曲面粗さ 0.0 (右) の異方性間接鏡面反射 (両方: 異方性 1.0)](images/screenshot_anisotropic_ibl1.jpg)

![Figure [anisotropicIBL2]: 曲面粗さ、金属性などを変えた異方性反射](images/screenshot_anisotropic_ibl2.jpg)

この手法の実装は簡単であり、listing [bentReflectionVector] で示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 anisotropicTangent = cross(bitangent, v);
vec3 anisotropicNormal = cross(anisotropicTangent, bitangent);
vec3 bentNormal = normalize(mix(n, anisotropicNormal, anisotropy));
vec3 r = reflect(-v, bentNormal);
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [bentReflectionVector]: bent reflection vector の GLSL 実装]

この手法は、listing [bentReflectionVectorDirection] で示されているように、負の `anisotropy` 値を受け入れることでより有用になります。異方性が負の場合、ハイライトは接線の方向ではなく、かわりに接線とは逆の方向になります。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 anisotropicDirection = anisotropy >= 0.0 ? bitangent : tangent;
vec3 anisotropicTangent = cross(anisotropicDirection, v);
vec3 anisotropicNormal = cross(anisotropicTangent, anisotropicDirection);
vec3 bentNormal = normalize(mix(n, anisotropicNormal, anisotropy));
vec3 r = reflect(-v, bentNormal);
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [bentReflectionVectorDirection]: bent reflection vector の GLSL 実装]

figure [anisotropicDirection] は、この変更された実装を実践で示しています。

![Figure [anisotropicDirection]: 正の値 (左) および負の値 (右) を使用した異方性方向の制御](images/screenshot_anisotropy_direction.png)

### サブサーフェス ###

[TODO] サブサーフェスとIBLについて説明する

### 布 ###

布のマテリアルモデルのIBL実装は、他のマテリアルモデルよりも複雑です。主な違いは、異なるNDF（「チャーリー」対高さ相関スミスGGX）を使用することに起因します。このセクションで説明したように、IBLを計算する際には、BRDFのDFG項を計算するために分割和近似を使用します。このDFG項は別のBRDF用に設計されており、布のBRDFには使用できません。私たちは布のBRDFをフレネル項が不要なように設計したので、DFG LUTの3番目のチャンネルに単一のDG項を生成することができます。結果はfigure [dfgClothLUT]に示されています。

DG項は、[#Estevez17]で推奨されているように、一様サンプリングを使用して生成します。一様サンプリングでは、$pdf$は単に$\frac{1}{2\pi}$であり、ヤコビアン$\frac{1}{4\left< \VoH \right>}$を使用する必要があります。

![Figure [dfgClothLUT]: 3番目のチャンネルに布のBRDFのDG項を符号化したDFG LUT](images/ibl/dfg_cloth.png)

画像ベースの照明の実装の残りの部分は、通常のライトの実装と同じ手順に従います。これには、オプションのサブサーフェイス散乱項とそのラップ拡散成分が含まれます。クリアコートのIBL実装と同様に、半球面上で積分することはできず、ラップ拡散成分を計算するために、視線方向を支配的な光方向として使用します。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float diffuse = Fd_Lambert() * ambientOcclusion;
#if defined(SHADING_MODEL_CLOTH)
#if defined(MATERIAL_HAS_SUBSURFACE_COLOR)
diffuse *= saturate((NoV + 0.5) / 2.25);
#endif
#endif

vec3 indirectDiffuse = irradianceIBL(n) * diffuse;
#if defined(SHADING_MODEL_CLOTH) && defined(MATERIAL_HAS_SUBSURFACE_COLOR)
indirectDiffuse *= saturate(subsurfaceColor + NoV);
#endif

vec3 ibl = diffuseColor * indirectDiffuse + indirectSpecular * specularColor;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [clothApprox]: クロスNDFのDFG近似のためのGLSL実装]

重要なことは、これがIBL問題の一部しか解決していないということです。以前に説明した事前フィルタリングされた環境マップは、標準のシェーディングモデルのBRDFと異なるクロスBRDFで畳み込まれます。正確な結果を得るためには、理論上はエンジンで使用される各BRDFに対して1つのIBLを提供すべきです。しかし、私たちのユースケースではIBLを2つ提供することは実用的ではないため、既存のIBLに依存することにしました。

## 静的照明

[TODO] 球面調和関数または球面ガウス光マップ、輻射度ボリューム、PRTなど…

## Transparency and translucency lighting

Transparent and translucent materials are important to add realism and correctness to scenes. Filament must therefore provide lighting models for both types of materials to allow artists to properly recreate realistic scenes. Translucency can also be used effectively in a number of non-realistic settings.

### 透明度

透明な表面を適切に照らすためには、まず、素材の不透明度がどのように適用されるかを理解する必要があります。窓を観察すると、拡散反射は透明であることがわかります。一方、鏡面反射が明るいほど、窓は不透明に見えます。この効果はfigure [cameraTransparency]で見ることができます。シーンはガラス面に適切に反射されていますが、太陽の鏡面ハイライトは十分に明るく、不透明に見えます。

![Figure [cameraTransparency]: 照らされた表面の透明度が重要な役割を果たす複雑なオブジェクトの例](images/screenshot_camera_transparency.jpg)

![Figure [litCar]: 照らされた表面の透明度が重要な役割を果たす複雑なオブジェクトの例](images/screenshot_car.jpg)

不透明度を適切に実装するために、事前乗算アルファ形式を使用します。目的の不透明度を $ \alpha_{opacity} $ 、拡散色を $ \sigma $ （線形、事前乗算されていない）とすると、フラグメントの有効な不透明度を計算することができます。

$$\begin{align*}
color &= \sigma * \alpha_{opacity} \\
opacity &= \alpha_{opacity}
\end{align*}$$

物理的な解釈は、ソースカラーのRGB成分がピクセルから放出される光の量を定義し、アルファ成分がそのピクセルによって遮られる背後の光の量を定義するというものです。したがって、次のブレンド関数を使用する必要があります。

$$\begin{align*}
Blend_{src} &= 1 \\
Blend_{dst} &= 1 - src_{\alpha}
\end{align*}$$

これらの式のGLSLでの実装は、listing [surfaceTransparency]に示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// baseColorはすでに事前乗算されている
vec4 shadeSurface(vec4 baseColor) {
    float alpha = baseColor.a;

    vec3 diffuseColor = evaluateDiffuseLighting();
    vec3 specularColor = evaluateSpecularLighting();    

    return vec4(diffuseColor + specularColor, alpha);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [surfaceTransparency]: GLSLでの照らされた表面の透明度の実装]

### 半透明

半透明な素材は、次の2つのカテゴリーに分けられます。
- 表面の半透明
- 体積の半透明

体積の半透明は、雲や煙などの粒子システムを照らすのに便利です。表面の半透明は、ワックス、大理石、肌など、透過散乱を持つ素材を模倣するのに使えます。

[TODO] 表面の半透明 (BRDF+BTDF, BSSRDF)

![Figure [半透明]: 前面から照らされた半透明な物体 (左) と背面から照らされた半透明な物体 (右)。近似的な BTDF と BSSRDF を使用。モデル: スタンフォード大学コンピュータグラフィックス研究所の Lucy](images/screenshot_translucency.png)

## 遮蔽

遮蔽は、さまざまなスケールで影を再現するための重要な暗転要因です。

小規模
:   マイクロ遮蔽はしわ、亀裂、くぼみなどを処理するために使用されます。

中規模
:   マクロ遮蔽は、オブジェクトのジオメトリによる遮蔽や法線マップに焼き込まれたジオメトリ（レンガなど）による遮蔽を処理するために使用されます。

大規模
:   オブジェクト同士の接触から生じる遮蔽、またはオブジェクト自体のジオメトリによる遮蔽です。

現在、私たちはマイクロ遮蔽を無視していますが、これはしばしば「くぼみマップ」の形でツールやエンジンで公開されています。Sébastien Lagardeは、Frostbiteでマイクロ遮蔽がどのように処理されるかについて興味深い議論を提供しています。拡散マイクロ遮蔽は、拡散マップに事前に焼き込まれ、鏡面反射マイクロ遮蔽は、反射テクスチャに事前に焼き込まれます。
私たちのシステムでは、マイクロ遮蔽は単純にベースカラーマップに焼き込むことができます。ただし、鏡面光はマイクロ遮蔽に影響を受けないことを知っておく必要があります。

中規模の環境遮蔽は、環境遮蔽マップに事前に焼き込まれ、マテリアルパラメータとして公開されることがあります。これは以前のマテリアルパラメータ化セクションで見られたように行われます。

大規模の環境遮蔽は、しばしばSSAO（スクリーンスペース環境遮蔽）、HBAO（ホライズンベース環境遮蔽）などのスクリーンスペース技術を使用して計算されます。これらの技術は、カメラが表面に十分に近い場合には中規模の環境遮蔽にも寄与することができます。

**注意**: 中規模と大規模の遮蔽を両方使用する際に過度な暗転を防ぐために、Lagardeは$min({AO}_{medium}, {AO}_{large})$を使用することを推奨しています。

### 拡散散乱

Morgan McGuire は物理ベースのレンダリングの文脈で環境光遮蔽を形式化しました [#McGuire10]。 McGuire は、我々の場合に球面調和でエンコードされた環境照明関数 $ L_a $ を定義しました。また、方向 $l$ における表面からの視線が遮蔽されていない場合に $V(l)=1$ となる可視性関数 $V$ も定義しました。  

これらの関数を使って、レンダリング方程式の環境項は、式 $\ref{diffuseAO}$ で示されるように表現できます。

$$\begin{equation}\label{diffuseAO}
L(l,v) = \int_{\Omega} f(l,v) L_a(l) V(l) \left< \NoL \right> dl
\end{equation}$$

この式は、可視性項を照明関数から分離して近似することで、式 $\ref{diffuseAOApprox}$ で示されるように表現できます。

$$\begin{equation}\label{diffuseAOApprox}
L(l,v) \approx \left( \pi \int_{\Omega} f(l,v) L_a(l) dl \right) \left( \frac{1}{\pi} \int_{\Omega} V(l) \left< \NoL \right> dl \right)
\end{equation}$$

この近似は、遠方光 $ L_a $ が一定であり、$f$ がランバート項であるときにのみ正確です。しかし、McGuire は、両方の関数が球のほとんど全体で比較的滑らかである場合には、この近似が妥当であると述べています。これは、遠方光プローブ（IBL）の場合に該当します。

この近似の左項は、IBL の事前計算された拡散成分です。右項は、点の分数的なアクセス可能性を示す 0 から 1 のスカラー係数です。その逆は、拡散環境遮蔽項であり、式 $\ref{diffuseAOTerm}$ で示されています。

$$\begin{equation}\label{diffuseAOTerm}
{AO} = 1 - \frac{1}{\pi} \int_{\Omega} V(l) \left< \NoL \right> dl
\end{equation}$$

事前計算された拡散項を使用しているため、実行時に影を落とす点の正確なアクセス可能性を計算することはできません。この事前計算された項における情報の欠如を補うために、シェーディングされた点の特定の表面材料に固有の環境遮蔽係数を適用することで、入射光を部分的に再構築します。

実際には、焼き付けられた環境遮蔽は、他のテクスチャ（たとえばベースカラーや法線）よりも低い解像度で保存されるグレースケールのテクスチャとして格納されます。当該マテリアルモデルの環境遮蔽プロパティは、マクロレベルの拡散環境遮蔽を再現することを意図しています。この近似は物理的に正確ではないものの、品質とパフォーマンスのトレードオフとして受け入れられるものです。

figure [aoComparison] は、拡散環境遮蔽がない場合とある場合の2つの異なるマテリアルを示しています。拡散環境遮蔽がないと、両方のマテリアルが平坦すぎるように見えます。

![Figure [aoComparison]: 拡散環境遮蔽がない場合（左）とある場合（右）のマテリアルの比較](images/screenshot_ao.jpg)

GLSL シェーダでの焼き付けられた拡散環境遮蔽の適用は簡単です。以下はlisting [bakedDiffuseAO] で示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// 拡散間接
vec3 indirectDiffuse = max(irradianceSH(n), 0.0) * Fd_Lambert();
// 環境遮蔽
indirectDiffuse *= texture2D(aoMap, outUV).r;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [bakedDiffuseAO]: GLSL での焼き付けられた拡散環境遮蔽の実装]

環境遮蔽項が間接照明にのみ適用される点に注意してください。

### 鏡面遮蔽

鏡面微遮蔽は、拡散色から導かれる $\fNormal$ に基づいています。この導出は、実世界の素材には2%未満の反射率が存在しないという知識に基づいています。そのため、0-2%の範囲の値は、フレネル項を滑らかに消滅させるために使用される、事前に焼き込まれた鏡面遮蔽として扱うことができます。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float f90 = clamp(dot(f0, 50.0 * 0.33), 0.0, 1.0);
// 安価な輝度の近似
float f90 = clamp(50.0 * f0.g, 0.0, 1.0);
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [specularMicroOcclusion]: GLSLにおける事前に焼き込まれた鏡面遮蔽]

先に述べた遮蔽の導出はランバート面を仮定し、間接的な拡散照明に対してのみ有効です。表面のアクセス可能性に関する情報の欠如は、間接的な鏡面照明の再構築にとって特に有害です。これは通常、光の漏れとして現れます。

Sébastien Lagardeは、拡散遮蔽項から鏡面遮蔽項を導出するための経験的なアプローチを提案しています（[#Lagarde14]）。その結果は物理的な根拠はありませんが、視覚的に魅力的な結果を生み出します。彼の式の目標は、粗い表面に対しては拡散遮蔽項を変更せずに返し、滑らかな表面に対しては、listing [specularOcclusion] で実装された式によって法線方向での遮蔽の影響を減少させ、狭い角度での影響を増加させることです。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float computeSpecularAO(float NoV, float ao, float roughness) {
    return clamp(pow(NoV + ao, exp2(-16.0 * roughness - 1.0)) - 1.0 + ao, 0.0, 1.0);
}

// 鏡面間接
vec3 indirectSpecular = evaluateSpecularIBL(r, perceptualRoughness);
// 環境遮蔽
float ao = texture2D(aoMap, outUV).r;
indirectSpecular *= computeSpecularAO(NoV, ao, roughness);
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [specularOcclusion]: GLSLにおけるLagardeの鏡面遮蔽係数の実装]

鏡面遮蔽係数が間接照明にのみ適用されることに注意してください。




#### 水平面鏡面遮蔽

法線マップを使用する表面の鏡面IBL寄与を計算する際に、反射ベクトルが表面に向かっている場合があります。この反射ベクトルをそのままシェーディングに使用すると、光が当たっていないはずの場所に光が当たってしまうことがあります（不透明な表面を仮定しています）。これは、Jeff Russellによって説明された簡単な技術を使って簡単に最小化できる別の光漏れの発生です[#Russell15]。

重要なアイデアは、表面の裏側から来る光を遮蔽することです。反射ベクトルと表面の法線の内積が負であれば、反射ベクトルが表面に向かっていることを示すので、これは簡単に実現できます。listing [horizonOcclusion]に示す私たちの実装は、Russellのものに似ていますが、アーティストが制御できる水平面フェードファクターはありません。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// 鏡面間接光
vec3 indirectSpecular = evaluateSpecularIBL(r, perceptualRoughness);

// フェード付き水平面遮蔽、直接鏡面光にも計算するべき
float horizon = min(1.0 + dot(r, n), 1.0);
indirectSpecular *= horizon * horizon;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [horizonOcclusion]: GLSLでの水平面鏡面遮蔽の実装]

水平面鏡面遮蔽フェードは安価ですが、必要に応じてパフォーマンスを向上させるために省略することもできます。

## ノーマルマッピング

ノーマルマップの一般的な用途には、ハイポリメッシュをローポリメッシュで置き換える（ベースマップを使用）ことと、表面のディテールを追加する（ディテールマップを使用）ことがあります。

たとえば、タフテッドレザーで覆われた家具をレンダリングしたいとします。タフテッドパターンを正確に表現するためにジオメトリをモデリングすると、三角形があまりにも多く必要になるため、代わりにハイポリメッシュをノーマルマップに焼き付けます。このベースマップを簡略化されたメッシュ（この場合は四角形）に適用すると、figure [ノーマルマップ]の結果が得られます。この効果を作成するために使用されたベースマップは、figure [ベースノーマルマップ]に表示されています。

![Figure [ノーマルマップ]: ノーマルマッピングを使用しないローポリメッシュ（左）と使用した場合（右）](images/screenshot_normal_mapping.jpg)

![Figure [ベースノーマルマップ]: ベースマップとして使用されるノーマルマップ](images/screenshot_normal_map.jpg)

ここで問題となるのは、このベースマップを第二のノーマルマップと組み合わせたい場合です。たとえば、figure [ディテールノーマルマップ]に示されているディテールマップを使用して、レザーにクラックを追加したいとします。

![Figure [ディテールノーマルマップ]: ディテールマップとして使用されるノーマルマップ](images/screenshot_normal_map_detail.jpg)

ノーマルマップの性質（接空間に保存されたXYZ成分）から、線形やオーバーレイのような単純なブレンディング手法はうまく機能しないことが明らかです。ここでは、数学的に正確な手法と、リアルタイムのシェーディングに適した近似手法の2つの高度な技術を使用します。

次のように日本語に翻訳しました:

コリン・バレ・ブリゾワとスティーブン・ヒルは、[#Hill12]で、詳細マップの基底をベースマップの法線に回転させるという、数学的に正しい解決法である*Reoriented Normal Mapping*を提案しています。この技術は、回転を適用するための最短弧クォータニオンに依存しており、接空間の性質のおかげで大幅に簡略化されます。

[#Hill12]で説明されている簡略化に従って、GLSLでの実装は、listing [reorientedNormalMapping]のようになります。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 t = texture(baseMap,   uv).xyz * vec3( 2.0,  2.0, 2.0) + vec3(-1.0, -1.0,  0.0);
vec3 u = texture(detailMap, uv).xyz * vec3(-2.0, -2.0, 2.0) + vec3( 1.0,  1.0, -1.0);
vec3 r = normalize(t * dot(t, u) - u * t.z);
return r;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [reorientedNormalMapping]: GLSLでのreoriented normal mappingの実装]

この実装は、法線がソーステクスチャで非圧縮で[0..1]の範囲で格納されていることを前提としています。

正規化ステップは厳密には必要ではなく、ランタイムでこの技術を使用する場合は省略できます。その場合、`r`の計算は`t * dot(t, u) / t.z - u`となります。

この技術は、以下で説明する技術よりも若干高価なので、主にオフラインで使用します。そこで、2つの法線マップを組み合わせるための簡単なオフラインツールを提供します。figure [blendedNormalMaps]は、前述のベースマップと詳細マップを使用したツールの出力を示しています。

![Figure [blendedNormalMaps]: ブレンドされた法線マップと詳細マップ（左）と、拡散マップと組み合わせたときのレンダリング結果（右）](images/screenshot_normal_map_blended.jpg)

### UDNブレンディング

UDNブレンディングと呼ばれる技法は、[#Hill12]に記述されている部分微分ブレンディング技法の一種です。この技法の主な利点は、必要なシェーダー命令の数が少ないことです（listing [udnBlending]を参照）。平坦な領域での詳細度の低下を招きますが、ブレンディングを実行時に行わなければならない場合には、UDNブレンディングは興味深いものです。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 t = texture(baseMap,   uv).xyz * 2.0 - 1.0;
vec3 u = texture(detailMap, uv).xyz * 2.0 - 1.0;
vec3 r = normalize(t.xy + u.xy, t.z);
return r;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [udnBlending]: GLSLでのUDNブレンディングの実装]

この結果は、Reoriented Normal Mappingに視覚的に近いものですが、データを注意深く比較すると、UDNが実際には正確さに劣ることがわかります。figure [blendedNormalMapsUDN]は、前の例と同じソースデータを用いて、UDNブレンディング手法の結果を示しています。

![Figure [blendedNormalMapsUDN]: UDNブレンディング技法を用いたノーマルマップとディテールマップのブレンド](images/screenshot_normal_map_blended_udn.jpg)


# ボリューメトリックエフェクト

## 指数的高さの霧

![Figure [exponentialHeightFog1]: 指数的高さの霧による方向性の散乱の例](images/screenshot_fog1.jpg)

![Figure [exponentialHeightFog2]: 指数的高さの霧による方向性の散乱の例](images/screenshot_fog2.jpg)

# アンチエイリアシング

[TODO] MSAA、ジオメトリAA（法線と粗さ）、シェーダーアンチエイリアシング（オブジェクト空間のシェーディング？）

# 画像処理パイプライン

このドキュメントのライティングセクションでは、物理ベースの方法で光がシーン内の表面と相互作用する方法について説明しています。実用的な結果を得るためには、ライティング方程式によって計算されたシーンの輝度を、表示可能なピクセル値に変換するために必要な変換を考慮する必要があります。

今回使用する一連の変換は、以下の画像処理パイプラインを形成します:

*************************************************************************************
* .-------------.      .--------------.      .---------------.                      *
* |             |      |              |      |  ホ ワ イ ト    |                      *
* |  シ ー ン 輝 度  +----->|  正 規 化 輝 度  +----->|  バ ラ ン ス  |                      *
* |             |      |    (HDR)     |      |               |                      *
* '-------------'      '--------------'      '-------+-------'                      *
*                                                    |                              *
*                                                    v                              *
*                                            .---------------.                      *
*                                            |               |                      *
*                                            | カラーグレーディング|                      *
*                                            |               |                      *
*                                            '-------+-------'                      *
*                                                    |                              *
*                                                    v                              *
*                                            .---------------.                      *
*                                            |               |                      *
*                                            | トーンマッピング |                      *
*                                            |               |                      *
*                                            '-------+-------'                      *
*                                                    |                              *
*                                                    v                              *
*                                            .---------------.      .-------------. *
*                                            |               |      |    ピクセル   | *
*                                            |     OETF      +----->|    値       | *
*                                            |               |      |    (LDR)    | *
*                                            '---------------'      '-------------' *
*************************************************************************************

**注意**: *OETF* ステップは、対象のカラースペースの光電変換関数の適用です。明瞭さのため、この図にはビネットやブルームなどのポストプロセスのステップは含まれていません。これらの効果については別途議論されます。

[TODO] カラースペース（ACES、sRGB、Rec. 709、Rec. 2020など）、ガンマ/リニアなど。

## 物理ベースのカメラ

画像変換プロセスの最初のステップは、物理ベースのカメラを使用して、シーンの出力輝度を適切に露出させることです。

### 露出設定

ライティングパイプラインでは光度単位を使っているので、カメラに到達する光は、$cd.m^{-2}$で表される輝度$L$というエネルギーです。カメラセンサーに入射する光は、星光の$10^{-5}cd.m^{-2}$から太陽の$10^{9}cd.m^{-2}$まで、非常に広い範囲の値をとります。このような広い範囲の値を操作したり記録したりすることはできないので、再マッピングする必要があります。

この範囲の再マッピングは、カメラでセンサーに一定時間露光することで行われます。センサーの限られた範囲を最大限に活用するために、シーンの光の範囲は「ミドルグレイ」と呼ばれる、黒と白の中間の値を中心にします。露出は、手動または自動で、以下の3つの設定を操作することで行われます。

- 絞り
- シャッタースピード
- 感度（ゲインとも呼ばれる）

絞り
:   $N$と表記され、fストップƒで表されるこの設定は、カメラシステムの絞りがどれだけ開いているか、または閉じているかを制御します。fストップはレンズの焦点距離と入射瞳の直径の比を示すので、高い値（ƒ/16）は小さな絞りを、小さい値（ƒ/1.4）は大きな絞りを示します。露出に加えて、絞りの設定は被写界深度を制御します。

シャッタースピード
:   $t$と表記され、秒$s$で表されるこの設定は、絞りが開いている時間を制御します（電子的または機械的なセンサーシャッターのタイミングも制御します）。露出に加えて、シャッタースピードはモーションブラーを制御します。

感度
:   $S$と表記され、ISOで表されるこの設定は、センサーに到達する光の量子化の仕方を制御します。単位のために、この設定は単に「ISO」や「ISO設定」と呼ばれることがよくあります。露出に加えて、感度の設定はノイズの量を制御します。


### 露出値

私たちの方程式でこれら3つの設定を参照するのは扱いづらいため、代わりに露出三角形を露出値としてまとめます。露出値はEVと表記され、その定義は以下の通りです[^reciprocity]。

露出値は、2を底とする対数尺度で表され、1つの露出値の差を1ストップと呼びます。1つの正のストップ（+1 EV）は輝度を2倍にし、1つの負のストップ（-1 EV）は輝度を半分にします。

式 $ \ref{ev} $ は[露出値の形式的な定義](https://en.wikipedia.org/wiki/Exposure_value)を示しています。

$$\begin{equation}\label{ev}
EV = log_2(\frac{N^2}{t})
\end{equation}$$

この定義は、感度に依存せず、絞りとシャッタースピードのみによるものです。露出値は慣例的にISO 100に対して定義されており、 $ EV_{100} $ と表記されます。この慣例に従うため、 $ EV_{100} $ を感度の関数として表す必要があります。

1ストップごとに輝度が2倍増減するという基準に基づいて、感度における露出値 $ EV_{S} $ を形式的に定義できます（式$\ref{evS}$）。

$$\begin{equation}\label{evS}
{EV}_S = EV_{100} + log_2(\frac{S}{100})
\end{equation}$$

また、$ EV_{100} $ を3つのカメラ設定の関数として計算することは簡単であり、その式は以下の通りです（式$\ref{ev100}$）。

$$\begin{equation}\label{ev100}
{EV}_{100} = EV_{S} - log_2(\frac{S}{100}) = log_2(\frac{N^2}{t}) - log_2(\frac{S}{100})
\end{equation}$$

なお、撮影者などの操作者は、絞り、シャッタースピード、感度の組み合わせを変えることで、同じ露出（したがって同じEV）を得ることができます。これにより、プロセスにおいて芸術的な制御が可能となります（被写界深度vs.モーションブラーvs.粒状感など）。

[^reciprocity]: 私たちはデジタルセンサーを想定しているため、相互感応性の欠如を考慮する必要はありません。

#### 露出値と輝度

カメラは、スポットメーターと同様に、シーンの平均輝度を測定し、EVに変換して自動露出を行うことができる。また、少なくともユーザーに露出のガイダンスを提供することができる。

シーン輝度$L$と、デバイスごとの校正定数$K$（式 $ \ref{evK} $ ）を与えると、EVを関数として定義することができる。

$$\begin{equation}\label{evC}
EV = log_2(\frac{E \times S}{C})
\end{equation}$$

この定数$K$は、反射光メーター定数と呼ばれ、メーカーによって異なる。一般的には、12.5と14の2つの値が見られる。12.5はキヤノン、ニコン、セコニックが使用しており、14はペンタックスとミノルタが使用している。キヤノンとニコンのカメラの普及率が高く、また私たち自身がセコニックの露出計を使用していることから、$ K = 12.5 $を使用することにする。

$ EV_{100} $ で作業したいのであれば、式 $ \ref{evK} $ の$K$と$S$を代入して、式 $ \ref{ev100L} $ を得ることができる。

$$\begin{equation}\label{ev100L}
EV = log_2(L \frac{100}{12.5})
\end{equation}$$

この関係を用いれば、フレームの平均輝度を測定することで、自動露出をエンジンで実装することが可能である。これを簡単に実現する方法の一つは、輝度バッファを1ピクセルにダウンサンプリングして、残った値を読み取ることである。この手法は残念ながら安定性が低く、極端な値の影響を受けやすい。多くのゲームでは、輝度ヒストグラムを用いて極端な値を除去するという別のアプローチを採用している。

検証やテストの目的であれば、EVから輝度を計算することができる。

$$\begin{equation}
L = 2^{EV_{100}} \times \frac{12.5}{100} = 2^{EV_{100} - 3}
\end{equation}$$

#### 露出量と照度

デバイスごとの校正定数$C$を与えると、照度$E$の関数としてEVを定義することができます:

$$\begin{equation}\label{evC}
EV = log_2(\frac{E \times S}{C})
\end{equation}$$

定数$C$は入射光メーター定数で、メーカーやセンサーの種類によって異なります。一般的なセンサーの種類には、平面型と半球型があります。平面型のセンサーでは、一般的な値は250です。半球型のセンサーでは、ミノルタが使用する320と、セコニックが使用する340という2つの一般的な値があります。

$ EV_{100} $ で作業したいのであれば、$S$を $ \ref{evC} $ に代入して、式 $ \ref{ev100C} $ を得ることができます。

$$\begin{equation}\label{ev100C}
EV = log_2(E \frac{100}{C})
\end{equation}$$

照度は、与えられたEVから計算することができます。 $ C = 250 $ の平面型センサーでは、式 $ \ref{eFlatSensor} $ を得ます。

$$\begin{equation}\label{eFlatSensor}
E = 2^{EV_{100}} \times 2.5
\end{equation}$$

$ C = 340 $ の半球型センサーでは、式 $ \ref{eHemisphereSensor} $ を得ます。

$$\begin{equation}\label{eHemisphereSensor}
E = 2^{EV_{100}} \times 3.4
\end{equation}$$

#### 露出補正

露出値はカメラの設定の組み合わせを示すものですが、写真家は光の強さを表すためによく使います。このため、カメラには露出補正という機能があり、画像を過露出または減露出にすることができます。この設定は芸術的なコントロールだけでなく、適切な露出を得るためにも使えます（例えば、雪は18%の中間灰色として露出されます）。

露出補正$EC$を適用するには、式$ \ref{ec} $に示すように、露出値にオフセットを加えるだけです。

$$\begin{equation}\label{ec}
EV_{100}' = EV_{100} - EC
\end{equation}$$

この式では、最終的な露出を調整するために、fストップで$EC$を使っているので、マイナス記号を使っています。EVを増やすと、レンズの絞りを絞る（またはシャッタースピードを減らす、感度を減らす）のと同じです。EVが高いと、画像は暗くなります。

### 露光

シーンの輝度を正規化された輝度に変換するためには、[光度露出](https://en.wikipedia.org/wiki/Exposure_value#Camera_settings_vs._photometric_exposure)（または光度露光）を使用しなければなりません。これはカメラセンサーに到達するシーンの輝度の量を示します。光度露出は、ルクス秒で表され、$H$と表される式 $\ref{photometricExposure}$ で与えられます。

$$\begin{equation}\label{photometricExposure}
H = \frac{q \cdot t}{N^2} L
\end{equation}$$

ここで、$L$はシーンの輝度、$t$はシャッタースピード、$N$は絞り、$q$はレンズおよびビネッティングの減衰を表します（通常、$ q = 0.65 $）[^lensAttenuation]。この定義にはセンサーの感度は考慮されていません。それを考慮するには、光度露出と感度を関連付ける3つの方法のうちの1つを使用する必要があります: 飽和速度、ノイズベース速度、および標準出力感度。

我々は飽和速度に関連する光度露出を選択し、これによりクリップやブルームが発生しない最大露光である $ H_{sat} $ が得られます（式 $\ref{hSat}$）。

$$\begin{equation}\label{hSat}
H_{sat} = \frac{78}{S_{sat}}
\end{equation}$$

我々は式 $\ref{hSat}$ と式 $\ref{photometricExposure}$ を組み合わせて、露光設定 $S$、$N$、および $t$ に従ってセンサーを飽和させる最大輝度 $L_{max}$ を計算する式 $\ref{lmax}$ を得ます。

$$\begin{equation}\label{lmax}
L_{max} = \frac{N^2}{q \cdot t} \frac{78}{S}
\end{equation}$$

この最大輝度は、式 $\ref{normalizedLuminance}$ で示されるように、入射輝度 $L$ を正規化するために使用できます。

$$\begin{equation}\label{normalizedLuminance}
L' = L \frac{1}{L_{max}}
\end{equation}$$

$ L_{max} $ は、式 $\ref{ev}$、$ S = 100 $、および $ q = 0.65 $ を使用して簡略化できます:

$$\begin{align*}
L_{max} &= \frac{N^2}{t} \frac{78}{q \cdot S} \\
L_{max} &= 2^{EV_{100}} \frac{78}{q \cdot S} \\
L_{max} &= 2^{EV_{100}} \times 1.2
\end{align*}$$

[Listing [fragmentExposure]] は、フラグメントシェーダーで計算されたピクセルカラーに露光項を直接適用する方法を示しています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// 露光設定からカメラのEV100を計算
// 絞りはF値
// シャッタースピードは秒単位
// 感度はISO
float exposureSettings(float aperture, float shutterSpeed, float sensitivity) {
    return log2((aperture * aperture) / shutterSpeed * 100.0 / sensitivity);
}

// カメラのEV100から露光の正規化ファクターを計算
float exposure(float ev100) {
    return 1.0 / (pow(2.0, ev100) * 1.2);
}

float ev100 = exposureSettings(aperture, shutterSpeed, sensitivity);
float exposure = exposure(ev100);

vec4 color = evaluateLighting();
color.rgb *= exposure;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [fragmentExposure]]: GLSLでの露光の実装

実際には、CPUで露光ファクターを事前に計算して、シェーダー命令を節約することができます。

[^lensAttenuation]: Wikipediaの「Film Speed, Measurements and calculations」を参照（https://en.wikipedia.org/wiki/Film_speed）

### 自動露出

上記のプロセスは、アーティストがカメラ露出設定を手動で行うことに依存しています。しかし、カメラの動きやダイナミックな効果がシーンの輝度に大きな影響を与えることがあるため、実際にはこれは手間がかかることがあります。与えられた輝度から露出値を計算する方法がわかっているため（詳細は「露出値と輝度」のセクションを参照）、カメラをスポットメーターに変換することができます。そのためには、シーンの輝度を測定する必要があります。

シーンの輝度を測定するためには、一般的に以下の2つの技術が使用されます。

- **輝度のダウンサンプリング**: 前のフレームを縮小して1x1の対数輝度バッファを取得し、CPUで読むことができるようにする（これはコンピュートシェーダーを使用しても達成できる）。その結果は、シーンの平均対数輝度です。最初のダウンサンプリングではまず各ピクセルの輝度を抽出する必要があります。この技術は不安定であり、その出力は時間の経過とともに滑らかにする必要があります。
- **輝度ヒストグラムの使用**: 平均対数輝度を見つけるための技術。この技術は前のものよりも利点があり、極端な値を無視することができ、より安定した結果を提供します。

これらの方法のいずれも、アルベドによる乗算後に平均輝度を見つけます。これは完全に正確ではないですが、代替案は表面のアルベドによる乗算前の各ピクセルの輝度を含む輝度バッファを保持することです。これは計算的にもメモリ的にも高コストです。

また、これらの2つの技術は、各ピクセルが最終的な露出に同じ影響（または重み）を持つ平均測光システムに制限します。カメラは通常、3つの測光モードを提供しています。

スポット測光
:   画像の中心部の小さな円のみが最終的な露出に寄与する測光モード。その円は通常、全画像サイズの1〜5%です。

中央重点測光
:   画面の中央にあるシーンの輝度値により大きな影響を与えます。

マルチゾーンまたはマトリックス測光
:   各メーカーによって異なる測光モード。このモードの目標は、シーンの最も重要な部分の露出を優先することです。これは通常、画像をグリッドに分割し、各セルを分類することによって達成されます（フォーカス情報、最小/最大輝度などを使用）。高度な実装では、シーンを既知のデータセットと比較して適切な露出を達成しようとします（逆光の夕日、曇り空の雪の日など）。

#### スポット測光

シーンの輝度を計算する際に使用する各輝度値の重み$w$は、次の式$\ref{spotMetering}$で与えられます。

$$\begin{equation}\label{spotMetering}
w(x,y) = \begin{cases} 1 & \left| p_{x,y} - s_{x,y} \right| \le s_r \\ 0 & \left| p_{x,y} - s_{x,y} \right| \gt s_r \end{cases}
\end{equation}$$

ここで、$p$はピクセルの位置、$s$はスポットの中心、$s_r$はスポットの半径です。

#### 中央重点測光

$$\begin{equation}\label{centerMetering}
w(x,y) = smooth(\left| p_{x,y} - c \right| \times \frac{2}{width} )
\end{equation}$$

ここで、$c$は画像の中心点で、$ smooth() $はGLSLの`smoothstep()`のような平滑化関数です。

#### 適応

計測結果を滑らかにするために、Pattanaikらによって[Pattanaik00]で説明された指数フィードバックループを用いた式$ \ref{adaptation} $を使用できます。

$$\begin{equation}\label{adaptation}
L_{avg} = L_{avg} + (L - L_{avg}) \times (1 - e^{-\Delta t \cdot \tau})
\end{equation}$$

ここで、$ \Delta t $は前のフレームからの時間差であり、$\tau$は適応率を制御する定数です。

### ブルーム

EVスケールはほぼ知覚的に直線的なため、露出値は光の単位としてもよく使用されます。これは、アーティストが露出補正を単位として光の強度や放射面の強度を指定できることを意味します。放射される光の強度は、露出設定に対して相対的であるべきです。露出補正を光の単位として使用することはできるが、できる限り避けるべきであり、カメラの設定に独立して（たとえばゲーム内のライトセーバーのように）放射面の周囲にブルーム効果を強制（またはキャンセル）するのに役立つことがあります。

![Figure : センサー上の飽和したフォトサイトがシーンの明るい部分にブルーミング効果を作り出す](images/screenshot_bloom.jpg)

$c$をブルームの色、$ EV_{100} $を現在の露出値とすると、式 $ \ref{bloomEV} $ に示すようにブルーム値の輝度を簡単に計算できます。

$$\begin{equation}\label{bloomEV}
EV_{bloom} = EV_{100} + EC \\
L_{bloom} = c \times 2^{EV_{bloom} - 3}
\end{equation}$$

式 $ \ref{bloomEV} $ は、放射ブルームを実装するためのフラグメントシェーダーで使用できます。これは、listing [fragmentEmissive] に示されているように、放射ブルームの実装です。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec4 surfaceShading() {
    vec4 color = evaluateLights();
    // rgb = color, w = exposure compensation
    vec4 emissive = getEmissive();
    color.rgb += emissive.rgb * pow(2.0, ev100 + emissive.w - 3.0);
    color.rgb *= exposure;
    return color;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [fragmentEmissive]: GLSLでの放射ブルームの実装]

## 光学後処理

### 色収差

[TODO]

![Figure [fringing]: 色収差の例: 左の耳や下のあごに注目してください。](images/screenshot_fringing.jpg)

### レンズフレア

[TODO] ノート: レンズフレアを生成する物理ベースのアプローチとして、レンズの光学アセンブリを通過する光線を追跡する方法がありますが、私たちは画像ベースのアプローチを採用します。このアプローチは費用がかからず、無料発光体の遮蔽や無制限の光源サポートなど、いくつかの利点があります。

## フィルムのポストプロセス

[TODO] シーンリファードデータ（トーンマッピング前のリニアスペース）に可能な限りポストプロセスを行う

最終画像に対するアーティストの制御を強化するために、カラーコレクションツールを提供することが重要です。これらのツールは、Adobe PhotoshopやAdobe After Effectsなどの写真やビデオの処理アプリケーションに見つかります。

### コントラスト

### 曲線

### レベル

### カラーグレーディング

## 光の経路

エンジンが使用する光の経路、またはレンダリング方法は、パフォーマンスに大きな影響を与える可能性があり、シーンで使用できるライトの数に強い制限を課す場合があります。3Dエンジンが使用するレンダリング方法には、伝統的にはフォワードレンダリングとディファードレンダリングの2種類があります。

私たちの目標は、以下の制約に従うレンダリング方法を使用することです。

- 帯域幅の要件が低い
- ピクセルごとに複数の動的ライトをサポートする

さらに、以下の機能を簡単にサポートできると望ましいです。

- MSAA
- 透明度
- 複数のマテリアルモデル

ディファードレンダリングは、多くの現代の3Dレンダリングエンジンで、数十、数百、あるいは数千もの光源を簡単にサポートできる（その他の利点もある）方法として使用されています。この方法は、残念ながら帯域幅の面で非常に高価です。私たちのデフォルトのPBRマテリアルモデルでは、Gバッファはピクセルあたり160から192ビットを使用することになり、これは直接的にかなり高い帯域幅の要件につながります。

一方、フォワードレンダリング方法は、歴史的に複数のライトを扱うのが苦手でした。一般的な実装方法は、シーンを可視ライトごとに複数回レンダリングし、結果をブレンド（加算）するというものです。別の技術は、シーン内の各オブジェクトに最大のライト数を割り当てるというものです。しかし、これはオブジェクトが世界の広い範囲を占める場合（建物、道路など）には非現実的です。

タイルドシェーディングは、フォワードレンダリングとディファードレンダリングの両方の方法に適用できます。この方法の考え方は、画面をタイルのグリッドに分割し、各タイルに対して、そのタイル内のピクセルに影響を与えるライトのリストを見つけるというものです。これには、オーバードロー（ディファードレンダリングの場合）や大きなオブジェクトのシェーディング計算（フォワードレンダリングの場合）を減らすという利点があります。しかし、この技術は深度の不連続性の問題に苦しみ、余分な作業が多く発生する可能性があります。

figure [sponza]に表示されているシーンは、クラスタードフォワードレンダリングを使用してレンダリングされました。

![Figure [sponza]: 数十の動的ライトとMSAAを使用したクラスタードフォワードレンダリング](images/screenshot_sponza.jpg)

figure [sponzaTiles]は、同じシーンをタイルに分割したものです（この場合、1280x720のレンダリングターゲットに80x80pxのタイルを使用しています）。

![Figure [sponzaTiles]: タイルドシェーディング（16x9タイル）](images/screenshot_sponza_tiles.jpg)

### クラスタード・フォワード・レンダリング

私たちは、クラスタード・シェーディングという別の方法を、そのフォワード・バリアントで探求することにしました。クラスタード・シェーディングは、タイルド・レンダリングのアイデアを拡張し、3次元の軸に沿ってセグメンテーションを行います。"クラスタリング"はビュー空間で行われ、フラスタムを3次元のグリッドに分割します。

まず、figure [sponzaSlices]に示すように、フラスタムを深度軸に沿ってスライスします。

![Figure [sponzaSlices]: Depth slicing (16 slices)](images/screenshot_sponza_slices.jpg)

そして、深度スライスとスクリーンタイルを組み合わせて、フラスタムを"ボクセル化"します。私たちは各クラスターをフロクセルと呼びます。これは、それらが何を表しているかを明確にするためです（フラスタム空間のボクセル）。"フロクセル化"パスの結果は、figure [froxel1]とfigure [froxel2]に示されています。

![Figure [froxel1]: Frustum voxelization (5x3 tiles, 8 depth slices)](images/screenshot_sponza_froxels1.jpg)

![Figure [froxel2]: Frustum voxelization (5x3 tiles, 8 depth slices)](images/screenshot_sponza_froxels2.jpg)

フレームをレンダリングする前に、シーン内の各ライトは、交差するフロクセルに割り当てられます。ライトの割り当てパスの結果は、各フロクセルに対するライトのリストになります。レンダリングパス中に、フラグメントが属するフロクセルのIDを計算し、そのフラグメントに影響を与える可能性のあるライトのリストを得ることができます。

深度スライスは線形ではなく、指数関数的です。典型的なシーンでは、近接面に近いピクセルの方が、遠方面に近いピクセルよりも多くなります。フロクセルの指数関数的なグリッドは、より重要な部分でのライトの割り当てを改善するでしょう。

figure [froxelDistribution]は、指数関数的なスライスで、各深度スライスがどのくらいのワールド空間の単位を使っているかを示しています。

![Figure [froxelDistribution]: Near: 0.1m, Far: 100m, 16 slices](images/diagram_froxels1.png)

単純な指数関数的なボクセル化は残念ながら十分ではありません。上のグラフィックは、スライスに沿ったワールド空間の分布をはっきりと示していますが、近接面に近いところで何が起こっているかを示していません。同じ分布をより小さな範囲（0.1mから7m）で調べると、figure [froxelDistributionClose]に示すように、興味深い問題が現れます。

![Figure [froxelDistributionClose]: Depth distribution in the 0.1-7m range](images/diagram_froxels2.png)

このグラフィックは、単純な指数関数的な分布では、近接面に非常に近いところでスライスの半分を使ってしまうことを示しています。この特定のケースでは、最初の5メートルで16のスライスのうち8つを使っています。ダイナミックなワールドライトは、ポイントライト（球）やスポットライト（円錐）であることが多いので、近接面に近いところでの細かい解像度は完全に不要です。

私たちの解決策は、シーンや近接面と遠方面に応じて、最初のフロクセルのサイズを手動で調整することです。そうすることで、残りのフロクセルをフラスタム全体により良く分配することができます。figure [froxelDistributionExp]は、例えば、0.1mから5mの間に特別なフロクセルを使った場合に何が起こるかを示しています。

![Figure [froxelDistributionExp]: Near: 0.1, Far: 100m, 16 slices, Special froxel: 0.1-5m](images/diagram_froxels3.png)

この新しい分布ははるかに効率的で、フラスタム全体でのライトの割り当てを改善することができます。

### 実装に関する注意事項

ライティングの割り当ては、GPUまたはCPUのどちらかで行うことができます。

#### GPUライト割り当て

この実装にはOpenGL ES 3.1とコンピュートシェーダーのサポートが必要です。ライトはシェーダーストレージバッファオブジェクト（SSBO）に格納され、各ライトを対応するフロクセルに割り当てるコンピュートシェーダーに渡されます。

フラスタムボクセル化は、射影行列が変化しない限り、最初のコンピュートシェーダーによって一度だけ実行できます。そして、ライトの割り当ては、別のコンピュートシェーダーによって毎フレーム実行できます。

コンピュートシェーダーのスレッドモデルは、このタスクに特に適しています。フロクセルの数と同じだけのワークグループを呼び出します（X、Y、Zのワークグループカウントをフロクセルグリッドの解像度に直接マップできます）。各ワークグループはスレッド化され、割り当てるためにすべてのライトを走査します。

交差テストは、単純な球/フラスタムまたは円錐/フラスタムのテストを含みます。

GPU実装（ポイントライトのみ）のソースコードは、付録にあります。

#### CPUライト割り当て

OpenGL ES 3.1以外のデバイスでは、ライト割り当てはCPUで効率的に行うことができます。アルゴリズムはGPU実装とは異なります。各フロクセルに対してすべてのライトを反復処理するのではなく、エンジンは各ライトをフロクセルとして「ラスタライズ」します。例えば、ポイントライトの中心と半径が与えられれば、それが交差するフロクセルのリストを簡単に計算することができます。

この手法には、GPUバリアントよりもより厳密なカリングを提供するという利点もあります。CPU実装では、パックされたライトのリストも容易に生成することができます。

#### シェーディング

フロクセルごとのライトのリストは、SSBO（OpenGL ES 3.1）またはテクスチャとしてフラグメントシェーダーに渡すことができます。

#### デプスからフロクセルへ

近平面を $n$、遠平面を $f$、最大デプススライス数を $m$、そして範囲 [0..1] の線形デプス値 $z$ が与えられた場合、式 $\ref{zToCluster}$ を使用して、特定の位置のクラスターのインデックスを計算できます。

$$\begin{equation}\label{zToCluster}
zToCluster(z,n,f,m)=floor \left( max \left( log2(z) \frac{m}{-log2(\frac{n}{f})} + m, 0 \right) \right)
\end{equation}$$

しかしながら、この式は以前に述べた解像度の問題を抱えています。これを修正するために、初めのフロクセルの範囲を定義する特別な近平面値 $sn$ を導入することで修正できます（初めのフロクセルは範囲 [n..sn]、残りのフロクセルは [sn..f] を占有します）。

$$\begin{equation}\label{zToClusterFix}
zToCluster(z,n,sn,f,m)=floor \left( max \left( log2(z) \frac{m-1}{-log2(\frac{sn}{f})} + m, 0 \right) \right)
\end{equation}$$

式 $\ref{linearZ}$ は、`gl_FragCoord.z` から線形デプス値を計算するのに使用できます（標準のOpenGLプロジェクション行列を仮定）。

$$\begin{equation}\label{linearZ}
linearZ(z)=\frac{n}{f+z(n-f)}
\end{equation}$$

この式は、式 $\ref{linearZFix}$ で示されるように、2つの項 $c0$ と $c1$ を事前に計算することで簡略化できます。

$$\begin{equation}\label{linearZFix}
c1 = \frac{f}{n} \\
c0 = 1 - c1 \\
linearZ(z)=\frac{1}{z \cdot c0 + c1}
\end{equation}$$

この簡略化は重要です。なぜならば、線形 z 値を $\ref{zToClusterFix}$ の `log2` に渡すからです。対数の下では除算が否定になるため、代わりに $-log2(z \cdot c0 + c1)$ を使用することで除算を回避できます。

以上をすべて組み合わせると、特定のフラグメントのフロクセルインデックスを比較的簡単に実装することができます。listing [fragCoordToFroxel] に示されているように。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// フロクセルごとの最大ライト数
#define MAX_LIGHT_COUNT 16

// フロクセルの解像度 x, 解像度 y, x方向の数, y方向の数
uniform uvec4 froxels;

// zパラメータ c0, c1, インデックススケール, インデックスバイアス
uniform vec4 zParams;

uint getDepthSlice() {
    return uint(max(0.0, log2(zParams.x * gl_FragCoord.z + zParams.y) *
            zParams.z + zParams.w));
}

uint getFroxelOffset(uint depthSlice) {
    uvec2 froxelCoord = uvec2(gl_FragCoord.xy) / froxels.xy;
    froxelCoord.y = (froxels.w - 1u) - froxelCoord.y;

    uint index = froxelCoord.x + froxelCoord.y * froxels.z +
            depthSlice * froxels.z * froxels.w;
    return index * MAX_FROXEL_LIGHT_COUNT;
}

uint slice = getDepthSlice();
uint offset = getFroxelOffset(slice);

// ライティングを計算する...
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [fragCoordToFroxel]: フラグメントの画面座標からフロクセルインデックスを計算するGLSL実装]

効率的にインデックス評価を行うために、いくつかのユニフォームを事前に計算する必要があります。これらのユニフォームを事前に計算するためのコードは、listing [froxelIndexPrecomputation]に示されています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
froxels[0] = TILE_RESOLUTION_IN_PX;
froxels[1] = TILE_RESOLUTION_IN_PX;
froxels[2] = numberOfTilesInX;
froxels[3] = numberOfTilesInY;

zParams[0] = 1.0f - Z_FAR / Z_NEAR;
zParams[1] = Z_FAR / Z_NEAR;
zParams[2] = (MAX_DEPTH_SLICES - 1) / log2(Z_SPECIAL_NEAR / Z_FAR);
zParams[3] = MAX_DEPTH_SLICES;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [froxelIndexPrecomputation]]

#### フロクセルから深度へ

フロクセルのインデックス $i$、特別な近接平面 $sn$、遠方平面 $f$、最大の深度スライス数 $m$ が与えられたとき、式 $\ref{clusterToZ}$ は、与えられたフロクセルの最小深度を計算します。

$$\begin{equation}\label{clusterToZ}
clusterToZ(i \ge 1,sn,f,m)=2^{(i-m) \frac{-log2(\frac{sn}{f})}{m-1}}
\end{equation}$$

$i=0$ のとき、z 値は 0 です。この式の結果は [0..1] の範囲にあり、ワールド単位での距離を得るには $f$ を掛ける必要があります。

コンピュートシェーダの実装では、`pow` の代わりに `exp2` を使うべきです。除算は事前に計算してユニフォームとして渡すことができます。

## 検証

私たちの照明システムは複雑なので、実装を検証することが重要です。私たちは、参照レンダリング、光測定、データ可視化など、いくつかの方法で検証します。

[TODO] 光測定による検証について説明する（レンダーターゲットからEVを読み取り、光度計やカメラなどで測定した値と比較するなど）。

### シーン参照の可視化

シーンの照明を検証する簡単で便利な方法は、シェーダーを変更して、関連するデータに直感的なマッピングを提供する色を出力することです。これは、偽の色を出力するカスタムデバッグトーンマッピングオペレーターを使用することで簡単に行うことができます。

#### 輝度ストップ

発光材料やIBLを使うと、反射ハイライトがその明るさの源よりも明るくなるシーンを簡単に作ることができます。このような問題はトーンマッピングや量子化の後では観察しにくいですが、シーンリファード空間ではかなり明らかです。figure [luminanceViz]は、listing [tonemapLuminanceViz]で説明されているカスタムオペレーターを使って、シーンの露出した輝度を表示する方法を示しています。

![Figure [luminanceViz]: ストップで輝度を色分けして視覚化する: シアンはミドルグレイ、青は1ストップ暗い、緑は1ストップ明るい、など。](images/screenshot_luminance_debug.png)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec3 Tonemap_DisplayRange(const vec3 x) {
    // 配列の5番目の色（シアン）はミドルグレイ（18%）を表す
    // ミドルグレイより上下のストップごとに色が変わる
    float v = log2(luminance(x) / 0.18);
    v = clamp(v + 5.0, 0.0, 15.0);
    int index = int(floor(v));
    return mix(debugColors[index], debugColors[min(15, index + 1)], fract(v));
}

const vec3 debugColors[16] = vec3[](
     vec3(0.0, 0.0, 0.0),         // black
     vec3(0.0, 0.0, 0.1647),      // darkest blue
     vec3(0.0, 0.0, 0.3647),      // darker blue
     vec3(0.0, 0.0, 0.6647),      // dark blue
     vec3(0.0, 0.0, 0.9647),      // blue
     vec3(0.0, 0.9255, 0.9255),   // cyan
     vec3(0.0, 0.5647, 0.0),      // dark green
     vec3(0.0, 0.7843, 0.0),      // green
     vec3(1.0, 1.0, 0.0),         // yellow
     vec3(0.90588, 0.75294, 0.0), // yellow-orange
     vec3(1.0, 0.5647, 0.0),      // orange
     vec3(1.0, 0.0, 0.0),         // bright red
     vec3(0.8392, 0.0, 0.0),      // red
     vec3(1.0, 0.0, 1.0),         // magenta
     vec3(0.6, 0.3333, 0.7882),   // purple
     vec3(1.0, 1.0, 1.0)          // white
);
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [tonemapLuminanceViz]: GLSL implementation of a custom debug tone-mapping operator for luminance visualization]

### 参考レンダリング

私たちの実装を参考レンダリングと比較するために、Mitsubaという商用グレードのオープンソースの物理ベースのオフラインパストレーサーを使います。Mitsubaは、さまざまなインテグレーター、サンプラー、マテリアルモデルを提供しており、リアルタイムレンダラーと公平な比較ができるはずです。このパストレーサーは、私たちのシーン記述から自動的に生成できるはずの、シンプルなXMLシーン記述フォーマットにも依存しています。

figure [mitsubaReference]とfigure [filamentReference]は、それぞれMitsubaとFilamentでレンダリングされた、完全に滑らかな誘電体の球体のシンプルなシーンを示しています。

![Figure [mitsubaReference]: 2013年製の12コアMacProで1分42秒で2048x1440でレンダリングされました](images/screenshot_ref_mitsuba.jpg)

![Figure [filamentReference]: Nexus 9デバイス(Tegra K1 GPU)でMSAA 4xで2048x1440で60 fpsでレンダリングされました](images/screenshot_ref_filament.jpg)

両方のシーンで使用したパラメータは以下の通りです。

**Filament**

- マテリアル
  - ベースカラー: sRGB 0.81, 0, 0
  - メタリック: 0
  - ラフネス: 0
  - 反射率: 0.5
- 間接光: IBL
  - office.exrからcmgenで生成された256x256のキューブマップ
  - マルチプライヤー: 35,000
- 直接光: 方向光
  - 線形色: 1.0, 0.96, 0.95
  - 強度: 120,000ルクス
- 露出
  - 絞り: f/16
  - シャッタースピード: 1/125秒
  - ISO: 100

**Mitsuba**

- BSDF: roughplastic
  - 分布: GGX
  - アルファ: 0
  - 拡散反射率: sRGB 0.81, 0, 0
- エミッター: 環境マップ
  - ソース: office.exr
  - スケール: 35,000
- エミッター: 方向
  - 照度: 線形RGB 120,000 115,200 114,000
- フィルム: LDR
  - 露出: -15.23, log2(filamentExposure)から計算
- インテグレーター: パス
- サンプラー: ldsampler
  - サンプル数: 256

Mitsubaのシーン全体は付録として見つけることができます。両方のシーンは同じ解像度(2048x1440)でレンダリングされました。

#### 比較

両方のレンダリングのわずかな違いは、Filamentが使用しているさまざまな近似に由来しています。RGBM 256x256 反射プローブ、RGBM 1024x1024 背景マップ、ランバート拡散、分割和近似、DFG項の解析的近似などです。

figure [referenceComparison]は、両方のエンジンが生成した画像の輝度勾配を示しています。比較はLDR画像に対して行われました。

![Figure [referenceComparison]: Mitsuba（左）とFilament（右）の輝度勾配](images/screenshot_ref_comparison.png)

最も大きな違いは、ほぼ平行な角度で見られます。これは、Filamentがランバート拡散項を使用していることによると考えられます。ディズニー拡散項とそのほぼ平行な反射は、FilamentをMitsubaに近づけるでしょう。

## 座標系

### 世界座標系

Filamentは、Y軸が上向きで右手系の座標系を使用しています。

![Figure [coordinates]: 赤 +X, 緑 +Y, 青 +Z (Marmoset Toolbagでレンダリングされたもの)](images/screenshot_coordinates.jpg)


### カメラ座標系

Filamentのカメラは、ローカルの-Z軸方向を見ています。つまり、カメラを世界に配置する際に何も変換を適用しない場合、カメラは世界の-Z軸方向を向いています。


### キューブマップの座標系

Filamentで使用されるすべてのキューブマップは、OpenGLの面の配置に従っています。その配置は、figure [cubemapCoordinates]に示されています。

![Figure [cubemapCoordinates]: OpenGLの面の配置に従ったキューブマップの水平クロス表現](images/screenshot_cubemap_coordinates.png)

なお、環境の背景や反射プローブはミラーリングされていることに注意してください（[ミラーリング]セクションを参照）。


#### ミラーリング

反射のレンダリングを簡単にするために、IBLキューブマップはX軸に沿ってミラーリングされて保存されます。これは、`cmgen`ツールのデフォルトの動作です。これは、環境背景として使用されるIBLキューブマップは、実行時に再びミラーリングする必要があることを意味します。スカイボックスの場合、テクスチャ付きの裏面を使用すると、簡単に実現できます。Filamentは、デフォルトでこれを行います。


#### 等角環境マップ

等角環境マップを水平/垂直のクロスキューブマップに変換するには、+Z面を元の直交環境マップの中心に配置します。


#### 環境マップとスカイボックスのワールド空間での方向

FilamentでスカイボックスやIBLを指定するとき、指定されたキューブマップは、その-Z面がワールドの+Z軸を向くように方向付けされます（これは、Filamentがミラーされたキューブマップを想定しているためです。[ミラーリング]の節を参照してください）。しかし、環境やスカイボックスは事前にミラーされていることが期待されるので、その-Z（背面）面はワールドの-Z軸を向いています（これは、カメラがデフォルトでその方向を向いているためです。[カメラ座標系]の節を参照してください）。


# 付録

## 鏡面色

The specular color of a metallic surface, or $\fNormal$, can be computed directly from measured spectral data. Online databases such as [Refractive Index](https://refractiveindex.info/?shelf=3d&book=metals&page=brass) provide tables of complex IOR measured at different wavelengths for various materials.

Earlier in this document, we presented equation $\ref{fresnelEquation}$ to compute the Fresnel reflectance at normal incidence for a dielectric surface given its IOR. The same equation can be rewritten for conductors by using complex numbers to represent the surface's IOR:

$$\begin{equation}
c_{ior} = n_{ior} + ik
\end{equation}$$

Equation $\ref{fresnelComplexIOR}$ presents the resulting Fresnel formula, where $c^*$ is the conjugate of the complex number $c$:

$$\begin{equation}\label{fresnelComplexIOR}
\fNormal(c_{ior}) = \frac{(c_{ior} - 1)(c_{ior}^* - 1)}{(c_{ior} + 1)(c_{ior}^* + 1)}
\end{equation}$$

To compute the specular color of a material we need to evaluate the complex Fresnel equation at each spectral sample of complex IOR over the visible spectrum. For each spectral sample, we obtain a spectral reflectance sample. To find the RGB color at normal incidence, we must multiply each sample by the CIE XYZ CMFs (color matching functions) and the spectral power distribution of the desired illuminant. We choose the standard illuminant D65 because we want to compute a color in the sRGB color space.

すべてのサンプルを合計（積分）して正規化することで、XYZ色空間で$\fNormal$を得ます。そこから、単純な色空間変換によって、線形sRGB色や、光電変換関数（OETF、一般に「ガンマ」曲線として知られる）を適用した非線形sRGB色を得ます。金などの一部の材料では、最終的なsRGB色がガモット外になる可能性があることに注意してください。私たちは、安価なガモットリマッピングの形として、単純な正規化ステップを使用していますが、より広いガモットを持つ色空間（例えばBT.2020）で値を計算することを検討するのも興味深いでしょう。

目的の結果を得るために、私たちはICE 1931 2度CMF（360nmから830nmまで1nm間隔で）([出典])と、CIE標準光源D65の相対分光放射分布（300nmから830nmまで5nm間隔で）([出典])を使用しました。

私たちの実装は、listing [specularColorImpl]に示されていますが、簡潔にするために実際のデータは省略しています。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
// CIE 1931 2度視野の色マッチング関数（CMFs）、360nmから830nmまで、
// 1nm間隔で
//
// データソース:
//     http://cvrl.ioo.ucl.ac.uk/cmfs.htm
//     http://cvrl.ioo.ucl.ac.uk/database/text/cmfs/ciexyz31.htm
const size_t CIE_XYZ_START = 360;
const size_t CIE_XYZ_COUNT = 471;
const float3 CIE_XYZ[CIE_XYZ_COUNT] = { ... };

// CIE Standard Illuminant D65 relative spectral power distribution,
// from 300nm to 830, at 5nm intervals
//
// Data source:
//     https://en.wikipedia.org/wiki/Illuminant_D65
//     https://cielab.xyz/pdf/CIE_sel_colorimetric_tables.xls
const size_t CIE_D65_INTERVAL = 5;
const size_t CIE_D65_START = 300;
const size_t CIE_D65_END = 830;
const size_t CIE_D65_COUNT = 107;
const float CIE_D65[CIE_D65_COUNT] = { ... };

struct Sample {
    float w = 0.0f; // wavelength
    std::complex<float> ior; // complex IOR, n + ik
};

static float illuminantD65(float w) {
    auto i0 = size_t((w - CIE_D65_START) / CIE_D65_INTERVAL);
    uint2 indexBounds{i0, std::min(i0 + 1, CIE_D65_END)};

    float2 wavelengthBounds = CIE_D65_START + float2{indexBounds} * CIE_D65_INTERVAL;
    float t = (w - wavelengthBounds.x) / (wavelengthBounds.y - wavelengthBounds.x);
    return lerp(CIE_D65[indexBounds.x], CIE_D65[indexBounds.y], t);
}

// For std::lower_bound
bool operator<(const Sample& lhs, const Sample& rhs) {
    return lhs.w < rhs.w;
}

// The wavelength w must be between 360nm and 830nm
static std::complex<float> findSample(const std::vector<Sample>& samples, float w) {
    auto i1 = std::lower_bound(
	        samples.begin(), samples.end(), Sample{w, 0.0f + 0.0if});
    auto i0 = i1 - 1;

    // Interpolate the complex IORs
    float t = (w - i0->w) / (i1->w - i0->w);
    float n = lerp(i0->ior.real(), i1->ior.real(), t);
    float k = lerp(i0->ior.imag(), i1->ior.imag(), t);
    return { n, k };
}

static float fresnel(const std::complex<float>& sample) {
    return (((sample - (1.0f + 0if)) * (std::conj(sample) - (1.0f + 0if))) /
            ((sample + (1.0f + 0if)) * (std::conj(sample) + (1.0f + 0if)))).real();
}

static float3 XYZ_to_sRGB(const float3& v) {
    const mat3f XYZ_sRGB{
             3.2404542f, -0.9692660f,  0.0556434f,
            -1.5371385f,  1.8760108f, -0.2040259f,
            -0.4985314f,  0.0415560f,  1.0572252f
    };
    return XYZ_sRGB * v;
}

// Outputs a linear sRGB color
static float3 computeColor(const std::vector<Sample>& samples) {
    float3 xyz{0.0f};
    float y = 0.0f;

    for (size_t i = 0; i < CIE_XYZ_COUNT; i++) {
        // Current wavelength
        float w = CIE_XYZ_START + i;

        // Find most appropriate CIE XYZ sample for the wavelength
        auto sample = findSample(samples, w);
        // Compute Fresnel reflectance at normal incidence
        float f0 = fresnel(sample);

        // We need to multiply by the spectral power distribution of the illuminant
        float d65 = illuminantD65(w);

        xyz += f0 * CIE_XYZ[i] * d65;
        y += CIE_XYZ[i].y * d65;
    }

    // Normalize so that 100% reflectance at every wavelength yields Y=1
    xyz /= y;

    float3 linear = XYZ_to_sRGB(xyz);

    // Normalize out-of-gamut values
    if (any(greaterThan(linear, float3{1.0f}))) linear *= 1.0f / max(linear);

    return linear;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [specularColorImpl]: スペクトルデータから金属表面のベースカラーを計算するC++実装]

このトピックに関して貴重な助言をくれたナティ・ホフマンに感謝します。

## IBLのための重要度サンプリング

離散領域では、式$\ref{iblSampling}$で定義されるように、積分はサンプリングで近似できます。

$$\begin{equation}\label{iblSampling}
\Lout(n,v,\Theta) \equiv \frac{1}{N} \sum_{i}^{N} f(l_{i}^{uniform},v,\Theta) L_{\perp}(l_i) \left< n \cdot l_i^{uniform} \right>
\end{equation}$$

残念ながら、この積分を評価するには多くのサンプルが必要です。一般的に用いられる技術は、より「重要な」サンプルをより頻繁に選択することで、これを呼びます_重要度サンプリング_。
私たちの場合は、微小面法線の分布、$D_{ggx}$を、重要なサンプルの分布として使用します。

重要度サンプリングを用いた$ \Lout(n,v,\Theta) $の評価は、式$\ref{annexIblImportanceSampling}$に示されています。

$$\begin{equation}\label{annexIblImportanceSampling}
\Lout(n,v,\Theta) \equiv \frac{1}{N} \sum_{i}^{N} \frac{f(l_{i},v,\Theta)}{p(l_i,v,\Theta)} L_{\perp}(l_i) \left< n \cdot l_i \right>
\end{equation}$$

式$\ref{annexIblImportanceSampling}$で、$p$は_重要な方向サンプル_ $l_i$の確率密度関数（PDF）です。これらのサンプルは$h_i$、$v$、$\alpha$に依存します。
PDFの定義は式$\ref{iblPDF}$に示されています。

$h_i$は、私たちが選んだ分布によって与えられます。詳細は[重要な方向の選択]を参照してください。

_重要な方向サンプル_ $l_i$は、$h_i$の周りで$v$の反射として計算されるため、
**同じ** PDFを持ちません$h_i$。変換された分布のPDFは次のように与えられます。

$$\begin{equation}
p(T_r(x)) = p(x) |J(T_r)|^{-1}
\end{equation}$$

ここで、$|J(T_r)|$は変換のヤコビアンの行列式です。私たちの場合は、
$h_i$から$l_i$への変換を考えており、そのヤコビアンの行列式は\ref{iblPDF}で与えられます。

$$\begin{equation}\label{iblPDF}
p(l,v,\Theta) = D(h,\alpha) \left< \NoH \right> |J_{h \rightarrow l}|^{-1} \\
|J_{h \rightarrow l}| = 4 \left< \VoH \right>
\end{equation}$$

### 重要な方向の選択

詳細は[BRDFのサンプリングのための重要な方向の選択]の節を参照してください。一様分布$(\zeta_{\phi},\zeta_{\theta})$が与えられたとき、重要な方向$l$は式$\ref{importantDirection}$で定義されます。

$$\begin{equation}\label{importantDirection}
\phi = 2 \pi \zeta_{\phi} \\
\theta = cos^{-1} \sqrt{\frac{1 - \zeta_{\theta}}{(\alpha^2 - 1)\zeta_{\theta}+1}} \\
l = \{ cos \phi sin \theta, sin \phi sin \theta, cos \theta \}
\end{equation}$$

通常、$ (\zeta_{\phi},\zeta_{\theta}) $は、[ハマースレイ系列]の節で説明されているハマースレイの一様分布アルゴリズムを用いて選択されます。

### 重要度サンプリングの事前フィルタリング

重要度サンプリングは、重要な方向を生成するためにPDFのみを考慮し、IBLの実際の内容には無関係です。後者がサンプルの少ない領域に高周波数を含んでいる場合、積分は正確になりません。これは、_事前フィルタリングされた重要度サンプリング_と呼ばれる技術を使うことで、ある程度緩和することができます。さらに、この技術を使うと、はるかに少ないサンプルで積分を収束させることができます。

事前フィルタリングされた重要度サンプリングは、環境の画像をいくつか使用し、低域通過フィルタリングをかけていきます。これは、ミップマップとボックスフィルタを使って非常に効率的に実装することができます。LODは、サンプルの重要度に基づいて選択されます。つまり、低確率のサンプルは、高いLODインデックス（よりフィルタリングされたもの）を使用します。

この技術は、[#Krivanek08]で詳しく説明されています。

キューブマップのLODは、次のように決定されます: 

$$\begin{align*}
lod &= log_4 \left( K\frac{\Omega_s}{\Omega_p} \right) \\
K &= 4.0 \\
\Omega_s &= \frac{1}{N \cdot p(l_i)} \\
\Omega_p &\approx \frac{4\pi}{6 \cdot width \cdot height}
\end{align*}$$

ここで、$K$は経験的に決定された定数、$p$はBRDFのPDF、$ \Omega_{s} $はサンプルに関連付けられた立体角、$\Omega_p$はキューブマップのテクセルに関連付けられた立体角です。

キューブマップのサンプリングは、シームレスな三線形フィルタリングを使って行われます。キューブマップを正しくサンプリングするために、OpenGLのシームレスサンプリング機能や、シームを回避/減らす他の技術を使うことが非常に重要です。

table [importanceSamplingViz]は、figure [importanceSamplingRef]に重要度サンプリングと事前フィルタリングされた重要度サンプリングを適用したときの比較を示しています。

![Figure [importanceSamplingRef]: 重要度サンプリングの画像参照](images/image_is_original.png)


 サンプル数 |      重要度サンプリング      |    事前フィルタリングされた重要度サンプリング
 ---------|-------------------------------|---------------------------------------
 4096   | ![](images/image_is_4096.png) | &nbsp;
 1024   | ![](images/image_is_1024.png) | ![](images/image_fis_1024.png)
 32     | ![](images/image_is_32.png)   | ![](images/image_fis_32.png)
[Table [importanceSamplingViz]: Importance sampling vs pre-filtered importance sampling with $\alpha = 0.4$]

The reference renderer used in the comparison below performs no approximation. In particular, it does not assume $v = n$ and does not perform the split sum approximation.  The pre-filtered renderer uses all the techniques discussed in this section: pre-filtered cubemaps, the analytic formulation of the DFG term, and of course the split sum approximation.

Left: reference renderer, right: pre-filtered importance sampling.

![](images/image_is_ref_1.png) ![](images/image_filtered_1.png)
![](images/image_is_ref_2.png) ![](images/image_filtered_2.png)
![](images/image_is_ref_3.png) ![](images/image_filtered_3.png)
![](images/image_is_ref_4.png) ![](images/image_filtered_4.png)

## BRDFのサンプリングにおける重要な方向の選択

簡単のために、BRDFの $ D $ 項をPDFとして使用しますが、PDFは半球上の積分が1になるように正規化する必要があります:

$$\begin{equation}
\int_{\Omega}p(m)dm = 1 \\
\int_{\Omega}D(m)(n \cdot m)dm = 1 \\
\int_{\phi=0}^{2\pi}\int_{\theta=0}^{\frac{\pi}{2}}D(\theta,\phi) cos \theta sin \theta d\theta d\phi = 1 \\
\end{equation}$$

したがって、BRDFのPDFは式$\ref{importantPDF}$のように表現できます:

$$\begin{equation}\label{importantPDF}
p(\theta,\phi) = \frac{\alpha^2}{\pi(cos^2\theta (\alpha^2-1) + 1)^2} cos\theta sin\theta
\end{equation}$$

$sin\theta$の項は、球面上の積分における微小立体角$sin\theta d\phi d\theta$に由来します。$\theta$と$\phi$を独立にサンプリングします:

$$\begin{align*}
p(\theta) &= \int_0^{2\pi} p(\theta,\phi) d\phi = \frac{2\alpha^2}{(cos^2\theta (\alpha^2-1) + 1)^2} cos\theta sin\theta \\
p(\phi) &= \frac{p(\theta,\phi)}{p(\phi)} = \frac{1}{2\pi}
\end{align*}$$

$p(\phi)$の式は、法線の分布が等方的である場合に成り立ちます。

次に、各変数の累積分布関数（CDF）を計算します:

$$\begin{align*}
P(s_{\phi}) &= \int_{0}^{s_{\phi}} p(\phi) d\phi = \frac{s_{\phi}}{2\pi} \\
P(s_{\theta}) &= \int_{0}^{s_{\theta}} p(\theta) d\theta = 2 \alpha^2 \left( \frac{1}{(2\alpha^4-4\alpha^2+2) cos(s_{\theta})^2 + 2\alpha^2 - 2} - \frac{1}{2\alpha^4-2\alpha^2} \right)
\end{align*}$$

$ P(s_{\phi}) $ と $ P(s_{\theta}) $ をそれぞれ乱数 $ \zeta_{\phi} $ と $ \zeta_{\theta} $ に設定し、$ s_{\phi} $ と $ s_{\theta} $ を求めます:

$$\begin{align*}
P(s_{\phi}) &= \zeta_{\phi} \rightarrow s_{\phi} = 2\pi\zeta_{\phi} \\
P(s_{\theta}) &= \zeta_{\theta} \rightarrow s_{\theta} = cos^{-1} \sqrt{\frac{1-\zeta_{\theta}}{(\alpha^2-1)\zeta_{\theta}+1}}
\end{align*}$$

したがって、一様分布 $ (\zeta_{\phi},\zeta_{\theta}) $ が与えられたとき、重要な方向$l$は次のように定義されます:

$$\begin{align*}
\phi &= 2\pi\zeta_{\phi} \\
\theta &= cos^{-1} \sqrt{\frac{1-\zeta_{\theta}}{(\alpha^2-1)\zeta_{\theta}+1}} \\
l &= \{ cos\phi sin\theta,sin\phi sin\theta,cos\theta \}
\end{align*}$$

## Hammersley数列

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
vec2f hammersley(uint i, float numSamples) {
    uint bits = i;
    bits = (bits << 16) | (bits >> 16);
    bits = ((bits & 0x55555555) << 1) | ((bits & 0xAAAAAAAA) >> 1);
    bits = ((bits & 0x33333333) << 2) | ((bits & 0xCCCCCCCC) >> 2);
    bits = ((bits & 0x0F0F0F0F) << 4) | ((bits & 0xF0F0F0F0) >> 4);
    bits = ((bits & 0x00FF00FF) << 8) | ((bits & 0xFF00FF00) >> 8);
    return vec2f(i / numSamples, bits / exp2(32));
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Hammersley数列を生成するC++実装]

## Precomputing L for image-based lighting

The term $ L_{DFG} $ is only dependent on $ \NoV $. Below, the normal is arbitrarily set to $ n=\left[0, 0, 1\right] $ and $v$ is chosen to satisfy $ \NoV $. The vector $ h_i $ is the $ D_{GGX}(\alpha) $ important direction sample $i$.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
float GDFG(float NoV, float NoL, float a) {
    float a2 = a * a;
    float GGXL = NoV * sqrt((-NoL * a2 + NoL) * NoL + a2);
    float GGXV = NoL * sqrt((-NoV * a2 + NoV) * NoV + a2);
    return (2 * NoL) / (GGXV + GGXL);
}

float2 DFG(float NoV, float a) {
    float3 V;
    V.x = sqrt(1.0f - NoV*NoV);
    V.y = 0.0f;
    V.z = NoV;

    float2 r = 0.0f;
    for (uint i = 0; i < sampleCount; i++) {
        float2 Xi = hammersley(i, sampleCount);
        float3 H = importanceSampleGGX(Xi, a, N);
        float3 L = 2.0f * dot(V, H) * H - V;

        float VoH = saturate(dot(V, H));
        float NoL = saturate(L.z);
        float NoH = saturate(H.z);

        if (NoL > 0.0f) {
            float G = GDFG(NoV, NoL, a);
            float Gv = G * VoH / NoH;
            float Fc = pow(1 - VoH, 5.0f);
            r.x += Gv * (1 - Fc);
            r.y += Gv * Fc;
        }
    }
    return r * (1.0f / sampleCount);
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[C++ implementation of the $ L_{DFG} $ term]

## 球面調和関数

          記号             |           定義
:---------------------------:|:---------------------------:
$K^m_l$                      | 正規化係数
$P^m_l(x)$                   | Legendre(ルジャンドル)多項式
$y^m_l$                      | 球面調和基底、またはSH基底
$L^m_l$                      | 単位球上で定義された$L(s)$関数のSH係数
[Table [shSymbols]: Spherical harmonics symbols definitions]

### 基底関数

単位球面上の点の球パラメタリゼーション:

$$\begin{equation}
\{ x, y, z \} = \{ cos \phi sin \theta, sin \phi sin \theta, cos \theta \}
\end{equation}$$

The complex spherical harmonics bases are given by:

$$\begin{equation}
Y^m_l(\theta, \phi) = K^m_l e^{im\theta} P^{|m|}_l(cos \theta), l \in N, -l <= m <= l
\end{equation}$$

しかし、必要とするのは実数の基底だけです:

$$\begin{align*}
y^{m > 0}_l &= \sqrt{2} K^m_l cos(m \phi) P^m_l(cos \theta) \\
y^{m < 0}_l &= \sqrt{2} K^m_l sin(|m| \phi) P^{|m|}_l(cos \theta) \\
y^0_l &= K^0_l P^0_l(cos \theta)
\end{align*}$$

正規化係数は次のように与えられます:

$$\begin{equation}
K^m_l = \sqrt{\frac{(2l + 1)(l - |m|)!}{4 \pi (l + |m|)!}}
\end{equation}$$

関連するLegendre(ルジャンドル)多項式$P^{|m|}_l$は以下の再帰式から計算できる:

$$\begin{equation}\label{shRecursions}
P^0_0(x) = 1 \\
P^0_1(x) = x \\
P^l_l(x) = (-1)^l (2l - 1)!! (1 - x^2)^{\frac{l}{2}} \\
P^m_l(x) = \frac{((2l - 1) x P^m_{l - 1} - (l + m - 1) P^m_{l - 2})}{l - m} \\
\end{equation}$$

$y^{|m|}_l$を計算するには、まず$P^{|m|}_l(z)$を計算する必要があります。これは、式$\ref{shRecursions}$の再帰を使って比較的簡単に行うことができます。3番目の再帰は、table [basisFunctions]で「斜めに移動」するのに使えます。つまり、$y^0_0$、$y^1_1$、$y^2_2$などを計算します。そして、4番目の再帰は、垂直に移動するのに使えます。

  帯指数 |  基底関数 $-l <= m <= l$
:-------:|:-----------------------------:|
$l = 0$  | $y^0_0$
$l = 1$  | $y^{-1}_1$ $y^0_1$ $y^1_1$
$l = 2$  | $y^{-2}_2$ $y^{-1}_2$ $y^0_2$ $y^1_2$ $y^2_2$
[Table [basisFunctions]: 帯ごとの基底関数]

三角関数の項は再帰的に計算するのも比較的簡単です:

$$\begin{align*}
C_m &\equiv cos(m \phi)sin(\theta)^m \\
S_m &\equiv sin(m \phi)sin(\theta)^m \\
\{ x, y, z \} &= \{ cos \phi sin \theta, sin \phi sin \theta, cos \theta \}
\end{align*}$$

角度和の三角恒等式を使う:

$$\begin{align*}
cos(m \phi + \phi) &= cos(m \phi) cos(\phi) - sin(m \phi) sin(\phi) \Leftrightarrow C_{m + 1} = x C_m - y S_m \\
sin(m \phi + \phi) &= sin(m \phi) cos(\phi) + cos(m \phi) sin(\phi) \Leftrightarrow S_{m + 1} = x S_m - y C_m
\end{align*}$$


[nonNormalizedSHBasis]のリストは、非正規化されたSH基底$\frac{y^m_l(s)}{\sqrt{2} K^m_l}$を計算するC++コードを示しています:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
static inline size_t SHindex(ssize_t m, size_t l) {
    return l * (l + 1) + m;
}

void computeShBasis(
        double* const SHb,
        size_t numBands,
        const vec3& s)
{
    // handle m=0 separately, since it produces only one coefficient
    double Pml_2 = 0;
    double Pml_1 = 1;
    SHb[0] =  Pml_1;
    for (ssize_t l = 1; l < numBands; l++) {
        double Pml = ((2 * l - 1) * Pml_1 * s.z - (l - 1) * Pml_2) / l;
        Pml_2 = Pml_1;
        Pml_1 = Pml;
        SHb[SHindex(0, l)] = Pml;
    }
    double Pmm = 1;
    for (ssize_t m = 1; m < numBands ; m++) {
        Pmm = (1 - 2 * m) * Pmm;
        double Pml_2 = Pmm;
        double Pml_1 = (2 * m + 1)*Pmm*s.z;
        // l == m
        SHb[SHindex(-m, m)] = Pml_2;
        SHb[SHindex( m, m)] = Pml_2;
        if (m + 1 < numBands) {
            // l == m+1
            SHb[SHindex(-m, m + 1)] = Pml_1;
            SHb[SHindex( m, m + 1)] = Pml_1;
            for (ssize_t l = m + 2; l < numBands; l++) {
                double Pml = ((2 * l - 1) * Pml_1 * s.z - (l + m - 1) * Pml_2)
                        / (l - m);
                Pml_2 = Pml_1;
                Pml_1 = Pml;
                SHb[SHindex(-m, l)] = Pml;
                SHb[SHindex( m, l)] = Pml;
            }
        }
    }
    double Cm = s.x;
    double Sm = s.y;
    for (ssize_t m = 1; m <= numBands ; m++) {
        for (ssize_t l = m; l < numBands ; l++) {
            SHb[SHindex(-m, l)] *= Sm;
            SHb[SHindex( m, l)] *= Cm;
        }
        double Cm1 = Cm * s.x - Sm * s.y;
        double Sm1 = Sm * s.x + Cm * s.y;
        Cm = Cm1;
        Sm = Sm1;
    }
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [nonNormalizedSHBasis]: 非正規化SH規定を計算するC++実装]

Normalized SH basis functions $y^m_l(s)$ for the first 3 bands:

   Band  |               $m = -2$               |                $m = -1$               |                      $m = 0$                        |                $m = 1$                |                    $m = 2$                    |
:-------:|:------------------------------------:|:-------------------------------------:|:---------------------------------------------------:|:-------------------------------------:|:---------------------------------------------:|
$l = 0$  |                                      |                                       | $\frac{1}{2}\sqrt{\frac{1}{\pi}}$                   |                                       |                                               |
$l = 1$  |                                      | $-\frac{1}{2}\sqrt{\frac{3}{\pi}}y$   | $\frac{1}{2}\sqrt{\frac{3}{\pi}}z$                  | $-\frac{1}{2}\sqrt{\frac{3}{\pi}}x$   |                                               |
$l = 2$  | $\frac{1}{2}\sqrt{\frac{15}{\pi}}xy$ | $-\frac{1}{2}\sqrt{\frac{15}{\pi}}yz$ | $\frac{1}{4}\sqrt{\frac{5}{\pi}}(2z^2 - x^2 - y^2)$ | $-\frac{1}{2}\sqrt{\frac{15}{\pi}}xz$ | $\frac{1}{4}\sqrt{\frac{15}{\pi}}(x^2 - y^2)$ |
[Table [basisFunctions]: Normalized basis functions per band]

### Decomposition and reconstruction

A function $L(s)$ defined on a sphere is projected to the SH basis as follows:

$$\begin{equation}
L^m_l = \int_\Omega L(s) y^m_l(s) ds \\
L^m_l = \int_{\theta = 0}^{\pi} \int_{\phi = 0}^{2\pi} L(\theta, \phi) y^m_l(\theta, \phi) sin \theta d\theta d\phi
\end{equation}$$

Note that each $L^m_l$ is a vector of 3 values, one for each RGB color channel.

The inverse transformation, or reconstruction, or rendering, from the SH coefficients is given by:

$$\begin{equation}
\hat{L}(s) = \sum_l \sum_{m = -l}^l L^m_l y^m_l(s)
\end{equation}$$

### Decomposition of $\left< cos \theta \right>$

Since $\left< cos \theta \right>$ does not depend on $\phi$ (azimuthal independence), the integral simplifies to:

$$\begin{align*}
C^0_l &= 2\pi \int_0^{\pi} \left< cos \theta \right> y^0_l(\theta) sin \theta d\theta \\
C^0_l &= 2\pi K^m_l \int_0^{\frac{\pi}{2}} P^0_l(cos \theta) cos \theta sin \theta d\theta \\
C^m_l &= 0, m != 0
\end{align*}$$

In [#Ramamoorthi01] an analytical solution to the integral is described:

$$\begin{align*}
C_1 &= \sqrt{\frac{\pi}{3}} \\
C_{odd} &= 0 \\
C_{l, even} &= 2\pi \sqrt{\frac{2l + 1}{4\pi}} \frac{(-1)^{\frac{l}{2} - 1}}{(l + 2)(l - 1)} \frac{l!}{2^l (\frac{l!}{2})^2}
\end{align*}$$

The first few coefficients are:

$$\begin{align*}
C_0 &= +0.88623 \\
C_1 &= +1.02333 \\
C_2 &= +0.49542 \\
C_3 &= +0.00000 \\
C_4 &= -0.11078
\end{align*}$$

Very few coefficients are needed to reasonably approximate $\left< cos \theta \right>$, as shown in figure [shCosThetaApprox].

![Figure [shCosThetaApprox]: Approximation of $cos \theta$ with SH coefficients](images/chart_sh_cos_thera_approx.png)

### 畳み込み

Convolutions by a kernel $h$ that has a circular symmetry can be applied directly and easily in SH space:

$$\begin{equation}
(h * f)^m_l = \sqrt{\frac{4\pi}{2l + 1}} h^0_l(s) f^m_l(s)
\end{equation}$$

Conveniently, $\sqrt{\frac{4\pi}{2l + 1}} = \frac{1}{K^0_l}$, so in practice we pre-multiply $C_l$ by $\frac{1}{K^0_l}$ and we get a simpler expression:

$$\begin{equation}
\hat{C}_{l, even} = 2\pi \frac{(-1)^{\frac{l}{2} - 1}}{(l + 2)(l - 1)} \frac{l!}{2^l (\frac{l!}{2})^2} \\
\hat{C}_1 = \frac{2\pi}{3}
\end{equation}$$

Here is the C++ code to compute $\hat{C}_l$:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
static double factorial(size_t n, size_t d = 1);

// < cos(theta) > SH coefficients pre-multiplied by 1 / K(0,l)
double computeTruncatedCosSh(size_t l) {
    if (l == 0) {
        return M_PI;
    } else if (l == 1) {
        return 2 * M_PI / 3;
    } else if (l & 1) {
        return 0;
    }
    const size_t l_2 = l / 2;
    double A0 = ((l_2 & 1) ? 1.0 : -1.0) / ((l + 2) * (l - 1));
    double A1 = factorial(l, l_2) / (factorial(l_2) * (1 << l));
    return 2 * M_PI * A0 * A1;
}

// returns n! / d!
double factorial(size_t n, size_t d ) {
   d = std::max(size_t(1), d);
   n = std::max(size_t(1), n);
   double r = 1.0;
   if (n == d) {
       // intentionally left blank
   } else if (n > d) {
       for ( ; n>d ; n--) {
           r *= n;
       }
   } else {
       for ( ; d>n ; d--) {
           r *= d;
       }
       r = 1.0 / r;
   }
   return r;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

## Sample validation scene for Mitsuba

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
&lt;scene version="0.5.0"&gt;
    &lt;integrator type="path"/&gt;

    &lt;shape type="serialized" id="sphere_mesh"&gt;
        &lt;string name="filename" value="plastic_sphere.serialized"/&gt;
        &lt;integer name="shapeIndex" value="0"/&gt;

        &lt;bsdf type="roughplastic"&gt;
            &lt;string name="distribution" value="ggx"/&gt;
            &lt;float name="alpha" value="0.0"/&gt;
            &lt;srgb name="diffuseReflectance" value="0.81, 0.0, 0.0"/&gt;
        &lt;/bsdf&gt;
    &lt;/shape&gt;

    &lt;emitter type="envmap"&gt;
        &lt;string name="filename" value="../../environments/office/office.exr"/&gt;
        &lt;float name="scale" value="35000.0" /&gt;
        &lt;boolean name="cache" value="false" /&gt;
    &lt;/emitter&gt;

    &lt;emitter type="directional"&gt;
        &lt;vector name="direction" x="-1" y="-1" z="1" /&gt;
        &lt;rgb name="irradiance" value="120000.0, 115200.0, 114000.0" /&gt;
    &lt;/emitter&gt;

    &lt;sensor type="perspective"&gt;
        &lt;float name="farClip" value="12.0"/&gt;
        &lt;float name="focusDistance" value="4.1"/&gt;
        &lt;float name="fov" value="45"/&gt;
        &lt;string name="fovAxis" value="y"/&gt;
        &lt;float name="nearClip" value="0.01"/&gt;
        &lt;transform name="toWorld"&gt;

            &lt;lookat target="0, 0, 0" origin="0, 0, -3.1" up="0, 1, 0"/&gt;
        &lt;/transform&gt;

        &lt;sampler type="ldsampler"&gt;
            &lt;integer name="sampleCount" value="256"/&gt;
        &lt;/sampler&gt;

        &lt;film type="ldrfilm"&gt;
            &lt;integer name="height" value="1440"/&gt;
            &lt;integer name="width" value="2048"/&gt;
            &lt;float name="exposure" value="-15.23" /&gt;
            &lt;rfilter type="gaussian"/&gt;
        &lt;/film&gt;
    &lt;/sensor&gt;
&lt;/scene&gt;
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

## Light assignment with froxels

Assigning lights to froxels can be implemented on the GPU using two compute shaders. The first one, shown in listing [froxelGeneration], creates the froxels data (4 planes + a min Z and max Z per froxel) in an SSBO and needs to be run only once. The shader requires the following uniforms:

Projection&nbsp;matrix
:   The projection matrix used to render the scene (view space to clip space transformation).

Inverse&nbsp;projection&nbsp;matrix
:   The inverse of the projection matrix used to render the scene (clip space to view space transformation).

Depth&nbsp;parameters
:   $-log2(\frac{z_{lighnear}}{z_{far}}) \frac{1}{maxSlices-1}$, maximum number of depth slices, Z near and Z far.

Clip&nbsp;space&nbsp;size
:   $\frac{F_x \times F_r}{w} \times 2$, with $F_x$ the number of tiles on the X axis, $F_r$ the resolution in pixels of a tile and w the width in pixels of the render target.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#version 310 es

precision highp float;
precision highp int;


#define FROXEL_RESOLUTION 80u

layout(local_size_x = 1, local_size_y = 1, local_size_z = 1) in;

layout(location = 0) uniform mat4 projectionMatrix;
layout(location = 1) uniform mat4 projectionInverseMatrix;
layout(location = 2) uniform vec4 depthParams; // index scale, index bias, near, far
layout(location = 3) uniform float clipSpaceSize;

struct Froxel {
    // NOTE: the planes should be stored in vec4[4] but the
    // Adreno shader compiler has a bug that causes the data
    // to not be read properly inside the loop
    vec4 plane0;
    vec4 plane1;
    vec4 plane2;
    vec4 plane3;
    vec2 minMaxZ;
};

layout(binding = 0, std140) writeonly restrict buffer FroxelBuffer {
    Froxel data[];
} froxels;

shared vec4 corners[4];
shared vec2 minMaxZ;

vec4 projectionToView(vec4 p) {
    p = projectionInverseMatrix * p;
    return p / p.w;
}

vec4 createPlane(vec4 b, vec4 c) {
    // standard plane equation, with a at (0, 0, 0)
    return vec4(normalize(cross(c.xyz, b.xyz)), 1.0);
}

void main() {
    uint index = gl_WorkGroupID.x + gl_WorkGroupID.y * gl_NumWorkGroups.x +
            gl_WorkGroupID.z * gl_NumWorkGroups.x * gl_NumWorkGroups.y;

    if (gl_LocalInvocationIndex == 0u) {
        // first tile the screen and build the frustum for the current tile
        vec2 renderTargetSize = vec2(FROXEL_RESOLUTION * gl_NumWorkGroups.xy);
        vec2 frustumMin = vec2(FROXEL_RESOLUTION * gl_WorkGroupID.xy);
        vec2 frustumMax = vec2(FROXEL_RESOLUTION * (gl_WorkGroupID.xy + 1u));

        corners[0] = vec4(
            frustumMin.x / renderTargetSize.x * clipSpaceSize - 1.0,
            (renderTargetSize.y - frustumMin.y) / renderTargetSize.y
			    * clipSpaceSize - 1.0,
            1.0,
            1.0
        );
        corners[1] = vec4(
            frustumMax.x / renderTargetSize.x * clipSpaceSize - 1.0,
            (renderTargetSize.y - frustumMin.y) / renderTargetSize.y
			    * clipSpaceSize - 1.0,
            1.0,
            1.0
        );
        corners[2] = vec4(
            frustumMax.x / renderTargetSize.x * clipSpaceSize - 1.0,
            (renderTargetSize.y - frustumMax.y) / renderTargetSize.y
			    * clipSpaceSize - 1.0,
            1.0,
            1.0
        );
        corners[3] = vec4(
            frustumMin.x / renderTargetSize.x * clipSpaceSize - 1.0,
            (renderTargetSize.y - frustumMax.y) / renderTargetSize.y
			    * clipSpaceSize - 1.0,
            1.0,
            1.0
        );

        uint froxelSlice = gl_WorkGroupID.z;
        minMaxZ = vec2(0.0, 0.0);
        if (froxelSlice > 0u) {
            minMaxZ.x = exp2((float(froxelSlice) - depthParams.y) * depthParams.x)
                    * depthParams.w;
        }
        minMaxZ.y = exp2((float(froxelSlice + 1u) - depthParams.y) * depthParams.x)
                * depthParams.w;
    }

    if (gl_LocalInvocationIndex == 0u) {
        vec4 frustum[4];
        frustum[0] = projectionToView(corners[0]);
        frustum[1] = projectionToView(corners[1]);
        frustum[2] = projectionToView(corners[2]);
        frustum[3] = projectionToView(corners[3]);

        froxels.data[index].plane0 = createPlane(frustum[0], frustum[1]);
        froxels.data[index].plane1 = createPlane(frustum[1], frustum[2]);
        froxels.data[index].plane2 = createPlane(frustum[2], frustum[3]);
        froxels.data[index].plane3 = createPlane(frustum[3], frustum[0]);
        froxels.data[index].minMaxZ = minMaxZ;
    }
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [froxelGeneration]: froxelsデータ生成のGLSL実装（コンピュートシェーダー）]

二つ目のコンピュートシェーダーは、listing [froxelEvaluation]に示されており、フレームごと（カメラやライトが変更された場合）に実行され、すべてのライトをそれぞれのfroxelに割り当てます。このシェーダーは、いくつかのユニフォーム（ポイント/スポットライトの数とビューマトリクス）と4つのSSBOだけに依存しています:

ライトインデックスバッファ
:   各froxelに対して、該当するfroxelに影響を与える各ライトのインデックス。ポイントライトのインデックスが最初に書き込まれ、スペースが十分に残っていれば、スポットライトのインデックスも書き込まれます。値0x7fffffffuのセンチネルは、ポイントとスポットライトを分け、またはfroxelのライトリストの終わりを示します。各froxelは最大数のライト（ポイント+スポット）を持ちます。

ポイントライトバッファ
:   シーンのポイントライトを記述する構造体の配列。

スポットライトバッファ
:   シーンのスポットライトを記述する構造体の配列。

Froxelsバッファ
:   前のコンピュートシェーダーによって作成された平面によって表現されるfroxelsのリスト。

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#version 310 es
precision highp float;
precision highp int;

#define LIGHT_BUFFER_SENTINEL 0x7fffffffu
#define MAX_FROXEL_LIGHT_COUNT 32u

#define THREADS_PER_FROXEL_X 8u
#define THREADS_PER_FROXEL_Y 8u
#define THREADS_PER_FROXEL_Z 1u
#define THREADS_PER_FROXEL (THREADS_PER_FROXEL_X * \
        THREADS_PER_FROXEL_Y * THREADS_PER_FROXEL_Z)

layout(local_size_x = THREADS_PER_FROXEL_X,
       local_size_y = THREADS_PER_FROXEL_Y,
       local_size_z = THREADS_PER_FROXEL_Z) in;

// x = point lights, y = spot lights
layout(location = 0) uniform uvec2 totalLightCount;
layout(location = 1) uniform mat4 viewMatrix;

layout(binding = 0, packed) writeonly restrict buffer LightIndexBuffer {
    uint index[];
} lightIndexBuffer;

struct PointLight {
    vec4 positionFalloff; // x, y, z, falloff
    vec4 colorIntensity;  // r, g, b, intensity
    vec4 directionIES;    // dir x, dir y, dir z, IES profile index
};

layout(binding = 1, std140) readonly restrict buffer PointLightBuffer {
    PointLight lights[];
} pointLights;

struct SpotLight {
    vec4 positionFalloff; // x, y, z, falloff
    vec4 colorIntensity;  // r, g, b, intensity
    vec4 directionIES;    // dir x, dir y, dir z, IES profile index
    vec4 angle;           // angle scale, angle offset, unused, unused
};

layout(binding = 2, std140) readonly restrict buffer SpotLightBuffer {
    SpotLight lights[];
} spotLights;

struct Froxel {
    // NOTE: the planes should be stored in vec4[4] but the
    // Adreno shader compiler has a bug that causes the data
    // to not be read properly inside the loop
    vec4 plane0;
    vec4 plane1;
    vec4 plane2;
    vec4 plane3;
    vec2 minMaxZ;
};

layout(binding = 3, std140) readonly restrict buffer FroxelBuffer {
    Froxel data[];
} froxels;

shared uint groupLightCounter;
shared uint groupLightIndexBuffer[MAX_FROXEL_LIGHT_COUNT];

float signedDistanceFromPlane(vec4 p, vec4 plane) {
    // plane.w == 0.0, simplify computation
    return dot(plane.xyz, p.xyz);
}

void synchronize() {
    memoryBarrierShared();
    barrier();
}

void main() {
    if (gl_LocalInvocationIndex == 0u) {
        groupLightCounter = 0u;
    }
    memoryBarrierShared();

    uint froxelIndex = gl_WorkGroupID.x + gl_WorkGroupID.y * gl_NumWorkGroups.x +
            gl_WorkGroupID.z * gl_NumWorkGroups.x * gl_NumWorkGroups.y;
    Froxel current = froxels.data[froxelIndex];

    uint offset = gl_LocalInvocationID.x +
	        gl_LocalInvocationID.y * THREADS_PER_FROXEL_X;
    for (uint i = 0u; i < totalLightCount.x &&
		    groupLightCounter < MAX_FROXEL_LIGHT_COUNT &&
            offset + i < totalLightCount.x; i += THREADS_PER_FROXEL) {

        uint currentLight = offset + i;

        vec4 center = pointLights.lights[currentLight].positionFalloff;
        center.xyz = (viewMatrix * vec4(center.xyz, 1.0)).xyz;
        float r = inversesqrt(center.w);

        if (-center.z + r > current.minMaxZ.x &&
                -center.z - r <= current.minMaxZ.y) {
            if (signedDistanceFromPlane(center, current.plane0) < r &&
                signedDistanceFromPlane(center, current.plane1) < r &&
                signedDistanceFromPlane(center, current.plane2) < r &&
                signedDistanceFromPlane(center, current.plane3) < r) {

                uint index = atomicAdd(groupLightCounter, 1u);
                groupLightIndexBuffer[index] = currentLight;
            }
        }
    }

    synchronize();

    uint pointLightCount = groupLightCounter;
    offset = froxelIndex * MAX_FROXEL_LIGHT_COUNT;

    for (uint i = gl_LocalInvocationIndex; i < pointLightCount;
            i += THREADS_PER_FROXEL) {
        lightIndexBuffer.index[offset + i] = groupLightIndexBuffer[i];
    }

    if (gl_LocalInvocationIndex == 0u) {
        if (pointLightCount < MAX_FROXEL_LIGHT_COUNT) {
            lightIndexBuffer.index[offset + pointLightCount] = LIGHT_BUFFER_SENTINEL;
        }
    }
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[Listing [froxelEvaluation]: GLSL implementation of assigning lights to froxels (compute shader)]

# 改訂履歴

略

# 参考文献

[#Ashdown98]: Ian Ashdown. 1998. Parsing the IESNA LM-63 photometric data file. http://lumen.iee.put.poznan.pl/kw/iesna.txt

[#Ashikhmin00]: Michael Ashikhmin, Simon Premoze and Peter Shirley. A Microfacet-based BRDF Generator. *SIGGRAPH '00 Proceedings*, 65-74.

[#Ashikhmin07]: Michael Ashikhmin and Simon Premoze. 2007. Distribution-based BRDFs.

[#Burley12]: Brent Burley. 2012. Physically Based Shading at Disney. *Physically Based Shading in Film and Game Production, ACM SIGGRAPH 2012 Courses*.

[#Estevez17]: Alejandro Conty Estevez and Christopher Kulla. 2017. Production Friendly Microfacet Sheen BRDF. *ACM SIGGRAPH 2017*.

[#Hammon17]: Earl Hammon. 217. PBR Diffuse Lighting for GGX+Smith Microsurfaces. *GDC 2017*.

[#Heitz14]: Eric Heitz. 2014. Understanding the Masking-Shadowing Function
in Microfacet-Based BRDFs. *Journal of Computer Graphics Techniques*, 3 (2).

[#Heitz16]: Eric Heitz et al. 2016. Multiple-Scattering Microfacet BSDFs with the Smith Model. *ACM SIGGRAPH 2016*.

[#Hill12]: Colin Barré-Brisebois and Stephen Hill. 2012. Blending in Detail. http://blog.selfshadow.com/publications/blending-in-detail/

[#Karis13a]: Brian Karis. 2013. Specular BRDF Reference. http://graphicrants.blogspot.com/2013/08/specular-brdf-reference.html

[#Karis13b]: Brian Karis, 2013. Real Shading in Unreal Engine 4. https://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_notes_v2.pdf

[#Karis14]: Brian Karis. 2014. Physically Based Shading on Mobile. https://www.unrealengine.com/blog/physically-based-shading-on-mobile

[#Kelemen01]: Csaba Kelemen et al. 2001. A Microfacet Based Coupled Specular-Matte BRDF Model with Importance Sampling. *Eurographics Short Presentations*.

[#Krystek85]: M. Krystek. 1985. An algorithm to calculate correlated color temperature. *Color Research & Application*, 10 (1), 38–40.

[#Krivanek08]: Jaroslave Krivànek and Mark Colbert. 2008. Real-time Shading with Filtered Importance Sampling. *Eurographics Symposium on Rendering 2008*, Volume 27, Number 4.

[#Kulla17]: Christopher Kulla and Alejandro Conty. 2017. Revisiting Physically Based Shading at Imageworks. *ACM SIGGRAPH 2017*

[#Lagarde14]: Sébastien Lagarde and Charles de Rousiers. 2014. Moving Frostbite to PBR. *Physically Based Shading in Theory and Practice, ACM SIGGRAPH 2014 Courses*.

[#Lagarde18]: Sébastien Lagarde and Evgenii Golubev. 2018. The road toward unified rendering with Unity’s high definition rendering pipeline. *Advances in Real-Time Rendering in Games, ACM SIGGRAPH 2018 Courses*.

[#Lazarov13]: Dimitar Lazarov. 2013. Physically-Based Shading in Call of Duty: Black Ops. *Physically Based Shading in Theory and Practice, ACM SIGGRAPH 2013 Courses*.

[#McAuley15]: Stephen McAuley. 2015. Rendering the World of Far Cry 4. *GDC 2015*.

[#McGuire10]: Morgan McGuire. 2010. Ambient Occlusion Volumes. *High Performance Graphics*.

[#Narkowicz14]: Krzysztof Narkowicz. 2014. Analytical DFG Term for IBL. https://knarkowicz.wordpress.com/2014/12/27/analytical-dfg-term-for-ibl

[#Neubelt13]: David Neubelt and Matt Pettineo. 2013. Crafting a Next-Gen Material Pipeline for The Order: 1886. *Physically Based Shading in Theory and Practice, ACM SIGGRAPH 2013 Courses*.

[#Oren94]: Michael Oren and Shree K. Nayar. 1994. Generalization of lambert's reflectance model. *SIGGRAPH*, 239–246. ACM.

[#Pattanaik00]: Sumanta Pattanaik00 et al. 2000. Time-Dependent Visual Adaptation
For Fast Realistic Image Display. *SIGGRAPH '00 Proceedings of the 27th annual conference on Computer graphics and interactive techniques*, 47-54.

[#Ramamoorthi01]: Ravi Ramamoorthi and Pat Hanrahan. 2001. On the relationship between radiance and irradiance: determining the illumination from images of a convex Lambertian object. *Journal of the Optical Society of America*, Volume 18, Number 10, October 2001.

[#Revie12]: Donald Revie. 2012.  Implementing Fur in Deferred Shading. *GPU Pro 2*, Chapter 2.

[#Russell15]: Jeff Russell. 2015. Horizon Occlusion for Normal Mapped Reflections. http://marmosetco.tumblr.com/post/81245981087

[#Schlick94]: Christophe Schlick. 1994. An Inexpensive BRDF Model for Physically-Based Rendering. *Computer Graphics Forum*, 13 (3), 233–246.

[#Walter07]: Bruce Walter et al. 2007. Microfacet Models for Refraction through Rough Surfaces. *Proceedings of the Eurographics Symposium on Rendering*.

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="../third_party/markdeep/markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
